{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to AI Month","text":"<p>Welcome to ACM's archive page for AI Month. After an extremely exciting month, we conclude on a high. We hope you enjoyed it just as much as us. The documentation used for the events have been linked above for your reference. See you at our future events!!!</p>"},{"location":"#featured","title":"Featured","text":"<p>Felicitating the active Particpants of the ML Essentials Sessions</p> <p>Winners of Kaggle Konquest 2024 are on our instagram! Congratulations to all the winners! \ud83c\udf89</p>"},{"location":"ML%20Essentials%20-%20NLP/","title":"Natural Language Processing","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install tensorflow-hub\n!pip install tensorflow-datasets\n</pre> !pip install tensorflow-hub !pip install tensorflow-datasets <pre>Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\nRequirement already satisfied: numpy&gt;=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (1.25.2)\nRequirement already satisfied: protobuf&gt;=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (3.20.3)\nRequirement already satisfied: tf-keras&gt;=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.15.0)\nRequirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.4)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.4.0)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.7)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\nRequirement already satisfied: etils[enp,epath,etree]&gt;=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.7.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.25.2)\nRequirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\nRequirement already satisfied: protobuf&gt;=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (3.20.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\nRequirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.31.0)\nRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.4.0)\nRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.66.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.1)\nRequirement already satisfied: array-record&gt;=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.5.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow-datasets) (2023.6.0)\nRequirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow-datasets) (6.1.2)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow-datasets) (4.10.0)\nRequirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow-datasets) (3.17.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow-datasets) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow-datasets) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow-datasets) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow-datasets) (2024.2.2)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise-&gt;tensorflow-datasets) (1.16.0)\nRequirement already satisfied: googleapis-common-protos&lt;2,&gt;=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata-&gt;tensorflow-datasets) (1.62.0)\n</pre> In\u00a0[\u00a0]: Copied! <pre>import os\nimport numpy as np\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\n</pre> import os import numpy as np  import tensorflow as tf import tensorflow_hub as hub import tensorflow_datasets as tfds In\u00a0[\u00a0]: Copied! <pre>tfds.list_builders()\n</pre> tfds.list_builders() Out[\u00a0]: <pre>['abstract_reasoning',\n 'accentdb',\n 'aeslc',\n 'aflw2k3d',\n 'ag_news_subset',\n 'ai2_arc',\n 'ai2_arc_with_ir',\n 'amazon_us_reviews',\n 'anli',\n 'answer_equivalence',\n 'arc',\n 'asqa',\n 'asset',\n 'assin2',\n 'asu_table_top_converted_externally_to_rlds',\n 'austin_buds_dataset_converted_externally_to_rlds',\n 'austin_sailor_dataset_converted_externally_to_rlds',\n 'austin_sirius_dataset_converted_externally_to_rlds',\n 'bair_robot_pushing_small',\n 'bc_z',\n 'bccd',\n 'beans',\n 'bee_dataset',\n 'beir',\n 'berkeley_autolab_ur5',\n 'berkeley_cable_routing',\n 'berkeley_fanuc_manipulation',\n 'berkeley_gnm_cory_hall',\n 'berkeley_gnm_recon',\n 'berkeley_gnm_sac_son',\n 'berkeley_mvp_converted_externally_to_rlds',\n 'berkeley_rpt_converted_externally_to_rlds',\n 'big_patent',\n 'bigearthnet',\n 'billsum',\n 'binarized_mnist',\n 'binary_alpha_digits',\n 'ble_wind_field',\n 'blimp',\n 'booksum',\n 'bool_q',\n 'bot_adversarial_dialogue',\n 'bridge',\n 'bucc',\n 'c4',\n 'c4_wsrs',\n 'caltech101',\n 'caltech_birds2010',\n 'caltech_birds2011',\n 'cardiotox',\n 'cars196',\n 'cassava',\n 'cats_vs_dogs',\n 'celeb_a',\n 'celeb_a_hq',\n 'cfq',\n 'cherry_blossoms',\n 'chexpert',\n 'cifar10',\n 'cifar100',\n 'cifar100_n',\n 'cifar10_1',\n 'cifar10_corrupted',\n 'cifar10_h',\n 'cifar10_n',\n 'citrus_leaves',\n 'cityscapes',\n 'civil_comments',\n 'clevr',\n 'clic',\n 'clinc_oos',\n 'cmaterdb',\n 'cmu_franka_exploration_dataset_converted_externally_to_rlds',\n 'cmu_play_fusion',\n 'cmu_stretch',\n 'cnn_dailymail',\n 'coco',\n 'coco_captions',\n 'coil100',\n 'colorectal_histology',\n 'colorectal_histology_large',\n 'columbia_cairlab_pusht_real',\n 'common_voice',\n 'conll2002',\n 'conll2003',\n 'controlled_noisy_web_labels',\n 'coqa',\n 'corr2cause',\n 'cos_e',\n 'cosmos_qa',\n 'covid19',\n 'covid19sum',\n 'crema_d',\n 'criteo',\n 'cs_restaurants',\n 'curated_breast_imaging_ddsm',\n 'cycle_gan',\n 'd4rl_adroit_door',\n 'd4rl_adroit_hammer',\n 'd4rl_adroit_pen',\n 'd4rl_adroit_relocate',\n 'd4rl_antmaze',\n 'd4rl_mujoco_ant',\n 'd4rl_mujoco_halfcheetah',\n 'd4rl_mujoco_hopper',\n 'd4rl_mujoco_walker2d',\n 'dart',\n 'databricks_dolly',\n 'davis',\n 'deep1b',\n 'deep_weeds',\n 'definite_pronoun_resolution',\n 'dementiabank',\n 'diabetic_retinopathy_detection',\n 'diamonds',\n 'div2k',\n 'dlr_edan_shared_control_converted_externally_to_rlds',\n 'dlr_sara_grid_clamp_converted_externally_to_rlds',\n 'dlr_sara_pour_converted_externally_to_rlds',\n 'dmlab',\n 'doc_nli',\n 'dolphin_number_word',\n 'domainnet',\n 'downsampled_imagenet',\n 'drop',\n 'dsprites',\n 'dtd',\n 'duke_ultrasound',\n 'e2e_cleaned',\n 'efron_morris75',\n 'emnist',\n 'eraser_multi_rc',\n 'esnli',\n 'eth_agent_affordances',\n 'eurosat',\n 'fashion_mnist',\n 'flic',\n 'flores',\n 'food101',\n 'forest_fires',\n 'fractal20220817_data',\n 'fuss',\n 'gap',\n 'geirhos_conflict_stimuli',\n 'gem',\n 'genomics_ood',\n 'german_credit_numeric',\n 'gigaword',\n 'glove100_angular',\n 'glue',\n 'goemotions',\n 'gov_report',\n 'gpt3',\n 'gref',\n 'groove',\n 'grounded_scan',\n 'gsm8k',\n 'gtzan',\n 'gtzan_music_speech',\n 'hellaswag',\n 'higgs',\n 'hillstrom',\n 'horses_or_humans',\n 'howell',\n 'i_naturalist2017',\n 'i_naturalist2018',\n 'i_naturalist2021',\n 'iamlab_cmu_pickup_insert_converted_externally_to_rlds',\n 'imagenet2012',\n 'imagenet2012_corrupted',\n 'imagenet2012_fewshot',\n 'imagenet2012_multilabel',\n 'imagenet2012_real',\n 'imagenet2012_subset',\n 'imagenet_a',\n 'imagenet_lt',\n 'imagenet_pi',\n 'imagenet_r',\n 'imagenet_resized',\n 'imagenet_sketch',\n 'imagenet_v2',\n 'imagenette',\n 'imagewang',\n 'imdb_reviews',\n 'imperialcollege_sawyer_wrist_cam',\n 'irc_disentanglement',\n 'iris',\n 'istella',\n 'jaco_play',\n 'kaist_nonprehensile_converted_externally_to_rlds',\n 'kddcup99',\n 'kitti',\n 'kmnist',\n 'kuka',\n 'laion400m',\n 'lambada',\n 'lfw',\n 'librispeech',\n 'librispeech_lm',\n 'libritts',\n 'ljspeech',\n 'lm1b',\n 'locomotion',\n 'lost_and_found',\n 'lsun',\n 'lvis',\n 'malaria',\n 'maniskill_dataset_converted_externally_to_rlds',\n 'math_dataset',\n 'math_qa',\n 'mctaco',\n 'media_sum',\n 'mlqa',\n 'mnist',\n 'mnist_corrupted',\n 'movie_lens',\n 'movie_rationales',\n 'movielens',\n 'moving_mnist',\n 'mrqa',\n 'mslr_web',\n 'mt_opt',\n 'mtnt',\n 'multi_news',\n 'multi_nli',\n 'multi_nli_mismatch',\n 'natural_instructions',\n 'natural_questions',\n 'natural_questions_open',\n 'newsroom',\n 'nsynth',\n 'nyu_depth_v2',\n 'nyu_door_opening_surprising_effectiveness',\n 'nyu_franka_play_dataset_converted_externally_to_rlds',\n 'nyu_rot_dataset_converted_externally_to_rlds',\n 'ogbg_molpcba',\n 'omniglot',\n 'open_images_challenge2019_detection',\n 'open_images_v4',\n 'openbookqa',\n 'opinion_abstracts',\n 'opinosis',\n 'opus',\n 'oxford_flowers102',\n 'oxford_iiit_pet',\n 'para_crawl',\n 'pass',\n 'patch_camelyon',\n 'paws_wiki',\n 'paws_x_wiki',\n 'penguins',\n 'pet_finder',\n 'pg19',\n 'piqa',\n 'places365_small',\n 'placesfull',\n 'plant_leaves',\n 'plant_village',\n 'plantae_k',\n 'protein_net',\n 'q_re_cc',\n 'qa4mre',\n 'qasc',\n 'quac',\n 'quality',\n 'quickdraw_bitmap',\n 'race',\n 'radon',\n 'real_toxicity_prompts',\n 'reddit',\n 'reddit_disentanglement',\n 'reddit_tifu',\n 'ref_coco',\n 'resisc45',\n 'rlu_atari',\n 'rlu_atari_checkpoints',\n 'rlu_atari_checkpoints_ordered',\n 'rlu_control_suite',\n 'rlu_dmlab_explore_object_rewards_few',\n 'rlu_dmlab_explore_object_rewards_many',\n 'rlu_dmlab_rooms_select_nonmatching_object',\n 'rlu_dmlab_rooms_watermaze',\n 'rlu_dmlab_seekavoid_arena01',\n 'rlu_locomotion',\n 'rlu_rwrl',\n 'robomimic_mg',\n 'robomimic_mh',\n 'robomimic_ph',\n 'robonet',\n 'robosuite_panda_pick_place_can',\n 'roboturk',\n 'rock_paper_scissors',\n 'rock_you',\n 's3o4d',\n 'salient_span_wikipedia',\n 'samsum',\n 'savee',\n 'scan',\n 'scene_parse150',\n 'schema_guided_dialogue',\n 'sci_tail',\n 'scicite',\n 'scientific_papers',\n 'scrolls',\n 'segment_anything',\n 'sentiment140',\n 'shapes3d',\n 'sift1m',\n 'simpte',\n 'siscore',\n 'smallnorb',\n 'smartwatch_gestures',\n 'snli',\n 'so2sat',\n 'speech_commands',\n 'spoken_digit',\n 'squad',\n 'squad_question_generation',\n 'stanford_dogs',\n 'stanford_hydra_dataset_converted_externally_to_rlds',\n 'stanford_kuka_multimodal_dataset_converted_externally_to_rlds',\n 'stanford_mask_vit_converted_externally_to_rlds',\n 'stanford_online_products',\n 'stanford_robocook_converted_externally_to_rlds',\n 'star_cfq',\n 'starcraft_video',\n 'stl10',\n 'story_cloze',\n 'summscreen',\n 'sun397',\n 'super_glue',\n 'svhn_cropped',\n 'symmetric_solids',\n 'taco_play',\n 'tao',\n 'tatoeba',\n 'ted_hrlr_translate',\n 'ted_multi_translate',\n 'tedlium',\n 'tf_flowers',\n 'the300w_lp',\n 'tiny_shakespeare',\n 'titanic',\n 'tokyo_u_lsmo_converted_externally_to_rlds',\n 'toto',\n 'trec',\n 'trivia_qa',\n 'tydi_qa',\n 'uc_merced',\n 'ucf101',\n 'ucsd_kitchen_dataset_converted_externally_to_rlds',\n 'ucsd_pick_and_place_dataset_converted_externally_to_rlds',\n 'uiuc_d3field',\n 'unified_qa',\n 'universal_dependencies',\n 'unnatural_instructions',\n 'usc_cloth_sim_converted_externally_to_rlds',\n 'user_libri_audio',\n 'user_libri_text',\n 'utaustin_mutex',\n 'utokyo_pr2_opening_fridge_converted_externally_to_rlds',\n 'utokyo_pr2_tabletop_manipulation_converted_externally_to_rlds',\n 'utokyo_saytap_converted_externally_to_rlds',\n 'utokyo_xarm_bimanual_converted_externally_to_rlds',\n 'utokyo_xarm_pick_and_place_converted_externally_to_rlds',\n 'vctk',\n 'viola',\n 'visual_domain_decathlon',\n 'voc',\n 'voxceleb',\n 'voxforge',\n 'waymo_open_dataset',\n 'web_graph',\n 'web_nlg',\n 'web_questions',\n 'webvid',\n 'wider_face',\n 'wiki40b',\n 'wiki_auto',\n 'wiki_bio',\n 'wiki_dialog',\n 'wiki_table_questions',\n 'wiki_table_text',\n 'wikiann',\n 'wikihow',\n 'wikipedia',\n 'wikipedia_toxicity_subtypes',\n 'wine_quality',\n 'winogrande',\n 'wit',\n 'wit_kaggle',\n 'wmt13_translate',\n 'wmt14_translate',\n 'wmt15_translate',\n 'wmt16_translate',\n 'wmt17_translate',\n 'wmt18_translate',\n 'wmt19_translate',\n 'wmt_t2t_translate',\n 'wmt_translate',\n 'wordnet',\n 'wsc273',\n 'xnli',\n 'xquad',\n 'xsum',\n 'xtreme_pawsx',\n 'xtreme_pos',\n 'xtreme_s',\n 'xtreme_xnli',\n 'yahoo_ltrc',\n 'yelp_polarity_reviews',\n 'yes_no',\n 'youtube_vis',\n 'huggingface:acronym_identification',\n 'huggingface:ade_corpus_v2',\n 'huggingface:adv_glue',\n 'huggingface:adversarial_qa',\n 'huggingface:aeslc',\n 'huggingface:afrikaans_ner_corpus',\n 'huggingface:ag_news',\n 'huggingface:ai2_arc',\n 'huggingface:air_dialogue',\n 'huggingface:ajgt_twitter_ar',\n 'huggingface:allegro_reviews',\n 'huggingface:allocine',\n 'huggingface:alt',\n 'huggingface:amazon_polarity',\n 'huggingface:amazon_reviews_multi',\n 'huggingface:amazon_us_reviews',\n 'huggingface:ambig_qa',\n 'huggingface:americas_nli',\n 'huggingface:ami',\n 'huggingface:amttl',\n 'huggingface:anli',\n 'huggingface:app_reviews',\n 'huggingface:aqua_rat',\n 'huggingface:aquamuse',\n 'huggingface:ar_cov19',\n 'huggingface:ar_res_reviews',\n 'huggingface:ar_sarcasm',\n 'huggingface:arabic_billion_words',\n 'huggingface:arabic_pos_dialect',\n 'huggingface:arabic_speech_corpus',\n 'huggingface:arcd',\n 'huggingface:arsentd_lev',\n 'huggingface:art',\n 'huggingface:arxiv_dataset',\n 'huggingface:ascent_kb',\n 'huggingface:aslg_pc12',\n 'huggingface:asnq',\n 'huggingface:asset',\n 'huggingface:assin',\n 'huggingface:assin2',\n 'huggingface:atomic',\n 'huggingface:autshumato',\n 'huggingface:babi_qa',\n 'huggingface:banking77',\n 'huggingface:bbaw_egyptian',\n 'huggingface:bbc_hindi_nli',\n 'huggingface:bc2gm_corpus',\n 'huggingface:beans',\n 'huggingface:best2009',\n 'huggingface:bianet',\n 'huggingface:bible_para',\n 'huggingface:big_patent',\n 'huggingface:bigbench',\n 'huggingface:billsum',\n 'huggingface:bing_coronavirus_query_set',\n 'huggingface:biomrc',\n 'huggingface:biosses',\n 'huggingface:biwi_kinect_head_pose',\n 'huggingface:blbooks',\n 'huggingface:blbooksgenre',\n 'huggingface:blended_skill_talk',\n 'huggingface:blimp',\n 'huggingface:blog_authorship_corpus',\n 'huggingface:bn_hate_speech',\n 'huggingface:bnl_newspapers',\n 'huggingface:bookcorpus',\n 'huggingface:bookcorpusopen',\n 'huggingface:boolq',\n 'huggingface:bprec',\n 'huggingface:break_data',\n 'huggingface:brwac',\n 'huggingface:bsd_ja_en',\n 'huggingface:bswac',\n 'huggingface:c3',\n 'huggingface:c4',\n 'huggingface:cail2018',\n 'huggingface:caner',\n 'huggingface:capes',\n 'huggingface:casino',\n 'huggingface:catalonia_independence',\n 'huggingface:cats_vs_dogs',\n 'huggingface:cawac',\n 'huggingface:cbt',\n 'huggingface:cc100',\n 'huggingface:cc_news',\n 'huggingface:ccaligned_multilingual',\n 'huggingface:cdsc',\n 'huggingface:cdt',\n 'huggingface:cedr',\n 'huggingface:cfq',\n 'huggingface:chr_en',\n 'huggingface:cifar10',\n 'huggingface:cifar100',\n 'huggingface:circa',\n 'huggingface:civil_comments',\n 'huggingface:clickbait_news_bg',\n 'huggingface:climate_fever',\n 'huggingface:clinc_oos',\n 'huggingface:clue',\n 'huggingface:cmrc2018',\n 'huggingface:cmu_hinglish_dog',\n 'huggingface:cnn_dailymail',\n 'huggingface:coached_conv_pref',\n 'huggingface:coarse_discourse',\n 'huggingface:codah',\n 'huggingface:code_search_net',\n 'huggingface:code_x_glue_cc_clone_detection_big_clone_bench',\n 'huggingface:code_x_glue_cc_clone_detection_poj104',\n 'huggingface:code_x_glue_cc_cloze_testing_all',\n 'huggingface:code_x_glue_cc_cloze_testing_maxmin',\n 'huggingface:code_x_glue_cc_code_completion_line',\n 'huggingface:code_x_glue_cc_code_completion_token',\n 'huggingface:code_x_glue_cc_code_refinement',\n 'huggingface:code_x_glue_cc_code_to_code_trans',\n 'huggingface:code_x_glue_cc_defect_detection',\n 'huggingface:code_x_glue_ct_code_to_text',\n 'huggingface:code_x_glue_tc_nl_code_search_adv',\n 'huggingface:code_x_glue_tc_text_to_code',\n 'huggingface:code_x_glue_tt_text_to_text',\n 'huggingface:com_qa',\n 'huggingface:common_gen',\n 'huggingface:common_language',\n 'huggingface:common_voice',\n 'huggingface:commonsense_qa',\n 'huggingface:competition_math',\n 'huggingface:compguesswhat',\n 'huggingface:conceptnet5',\n 'huggingface:conceptual_12m',\n 'huggingface:conceptual_captions',\n 'huggingface:conll2000',\n 'huggingface:conll2002',\n 'huggingface:conll2003',\n 'huggingface:conll2012_ontonotesv5',\n 'huggingface:conllpp',\n 'huggingface:consumer-finance-complaints',\n 'huggingface:conv_ai',\n 'huggingface:conv_ai_2',\n 'huggingface:conv_ai_3',\n 'huggingface:conv_questions',\n 'huggingface:coqa',\n 'huggingface:cord19',\n 'huggingface:cornell_movie_dialog',\n 'huggingface:cos_e',\n 'huggingface:cosmos_qa',\n 'huggingface:counter',\n 'huggingface:covid_qa_castorini',\n 'huggingface:covid_qa_deepset',\n 'huggingface:covid_qa_ucsd',\n 'huggingface:covid_tweets_japanese',\n 'huggingface:covost2',\n 'huggingface:cppe-5',\n 'huggingface:craigslist_bargains',\n 'huggingface:crawl_domain',\n 'huggingface:crd3',\n 'huggingface:crime_and_punish',\n 'huggingface:crows_pairs',\n 'huggingface:cryptonite',\n 'huggingface:cs_restaurants',\n 'huggingface:cuad',\n 'huggingface:curiosity_dialogs',\n 'huggingface:daily_dialog',\n 'huggingface:dane',\n 'huggingface:danish_political_comments',\n 'huggingface:dart',\n 'huggingface:datacommons_factcheck',\n 'huggingface:dbpedia_14',\n 'huggingface:dbrd',\n 'huggingface:deal_or_no_dialog',\n 'huggingface:definite_pronoun_resolution',\n 'huggingface:dengue_filipino',\n 'huggingface:dialog_re',\n 'huggingface:diplomacy_detection',\n 'huggingface:disaster_response_messages',\n 'huggingface:discofuse',\n 'huggingface:discovery',\n 'huggingface:disfl_qa',\n 'huggingface:doc2dial',\n 'huggingface:docred',\n 'huggingface:doqa',\n 'huggingface:dream',\n 'huggingface:drop',\n 'huggingface:duorc',\n 'huggingface:dutch_social',\n 'huggingface:dyk',\n 'huggingface:e2e_nlg',\n 'huggingface:e2e_nlg_cleaned',\n 'huggingface:ecb',\n 'huggingface:ecthr_cases',\n 'huggingface:eduge',\n 'huggingface:ehealth_kd',\n 'huggingface:eitb_parcc',\n 'huggingface:electricity_load_diagrams',\n 'huggingface:eli5',\n 'huggingface:eli5_category',\n 'huggingface:elkarhizketak',\n 'huggingface:emea',\n 'huggingface:emo',\n 'huggingface:emotion',\n 'huggingface:emotone_ar',\n 'huggingface:empathetic_dialogues',\n 'huggingface:enriched_web_nlg',\n 'huggingface:enwik8',\n 'huggingface:eraser_multi_rc',\n 'huggingface:esnli',\n 'huggingface:eth_py150_open',\n 'huggingface:ethos',\n 'huggingface:ett',\n 'huggingface:eu_regulatory_ir',\n 'huggingface:eurlex',\n 'huggingface:euronews',\n 'huggingface:europa_eac_tm',\n 'huggingface:europa_ecdc_tm',\n 'huggingface:europarl_bilingual',\n 'huggingface:event2Mind',\n 'huggingface:evidence_infer_treatment',\n 'huggingface:exams',\n 'huggingface:factckbr',\n 'huggingface:fake_news_english',\n 'huggingface:fake_news_filipino',\n 'huggingface:farsi_news',\n 'huggingface:fashion_mnist',\n 'huggingface:fever',\n 'huggingface:few_rel',\n 'huggingface:financial_phrasebank',\n 'huggingface:finer',\n 'huggingface:flores',\n 'huggingface:flue',\n 'huggingface:food101',\n 'huggingface:fquad',\n 'huggingface:freebase_qa',\n 'huggingface:gap',\n 'huggingface:gem',\n 'huggingface:generated_reviews_enth',\n 'huggingface:generics_kb',\n 'huggingface:german_legal_entity_recognition',\n 'huggingface:germaner',\n 'huggingface:germeval_14',\n 'huggingface:giga_fren',\n 'huggingface:gigaword',\n 'huggingface:glucose',\n 'huggingface:glue',\n 'huggingface:gnad10',\n 'huggingface:go_emotions',\n 'huggingface:gooaq',\n 'huggingface:google_wellformed_query',\n 'huggingface:grail_qa',\n 'huggingface:great_code',\n 'huggingface:greek_legal_code',\n 'huggingface:gsm8k',\n 'huggingface:guardian_authorship',\n 'huggingface:gutenberg_time',\n 'huggingface:hans',\n 'huggingface:hansards',\n 'huggingface:hard',\n 'huggingface:harem',\n 'huggingface:has_part',\n 'huggingface:hate_offensive',\n 'huggingface:hate_speech18',\n 'huggingface:hate_speech_filipino',\n 'huggingface:hate_speech_offensive',\n 'huggingface:hate_speech_pl',\n 'huggingface:hate_speech_portuguese',\n 'huggingface:hatexplain',\n 'huggingface:hausa_voa_ner',\n 'huggingface:hausa_voa_topics',\n 'huggingface:hda_nli_hindi',\n 'huggingface:head_qa',\n 'huggingface:health_fact',\n 'huggingface:hebrew_projectbenyehuda',\n 'huggingface:hebrew_sentiment',\n 'huggingface:hebrew_this_world',\n 'huggingface:hellaswag',\n 'huggingface:hendrycks_test',\n 'huggingface:hind_encorp',\n 'huggingface:hindi_discourse',\n 'huggingface:hippocorpus',\n 'huggingface:hkcancor',\n 'huggingface:hlgd',\n 'huggingface:hope_edi',\n 'huggingface:hotpot_qa',\n 'huggingface:hover',\n 'huggingface:hrenwac_para',\n 'huggingface:hrwac',\n 'huggingface:humicroedit',\n 'huggingface:hybrid_qa',\n 'huggingface:hyperpartisan_news_detection',\n 'huggingface:iapp_wiki_qa_squad',\n 'huggingface:id_clickbait',\n 'huggingface:id_liputan6',\n 'huggingface:id_nergrit_corpus',\n 'huggingface:id_newspapers_2018',\n 'huggingface:id_panl_bppt',\n 'huggingface:id_puisi',\n 'huggingface:igbo_english_machine_translation',\n 'huggingface:igbo_monolingual',\n 'huggingface:igbo_ner',\n 'huggingface:ilist',\n 'huggingface:imagenet-1k',\n 'huggingface:imagenet_sketch',\n 'huggingface:imdb',\n 'huggingface:imdb_urdu_reviews',\n 'huggingface:imppres',\n 'huggingface:indic_glue',\n 'huggingface:indonli',\n 'huggingface:indonlu',\n 'huggingface:inquisitive_qg',\n 'huggingface:interpress_news_category_tr',\n 'huggingface:interpress_news_category_tr_lite',\n 'huggingface:irc_disentangle',\n 'huggingface:isixhosa_ner_corpus',\n 'huggingface:isizulu_ner_corpus',\n 'huggingface:iwslt2017',\n 'huggingface:jeopardy',\n 'huggingface:jfleg',\n 'huggingface:jigsaw_toxicity_pred',\n 'huggingface:jigsaw_unintended_bias',\n 'huggingface:jnlpba',\n 'huggingface:journalists_questions',\n 'huggingface:kan_hope',\n 'huggingface:kannada_news',\n 'huggingface:kd_conv',\n 'huggingface:kde4',\n 'huggingface:kelm',\n 'huggingface:kilt_tasks',\n 'huggingface:kilt_wikipedia',\n 'huggingface:kinnews_kirnews',\n 'huggingface:klue',\n 'huggingface:kor_3i4k',\n 'huggingface:kor_hate',\n 'huggingface:kor_ner',\n 'huggingface:kor_nli',\n 'huggingface:kor_nlu',\n 'huggingface:kor_qpair',\n 'huggingface:kor_sae',\n 'huggingface:kor_sarcasm',\n 'huggingface:labr',\n 'huggingface:lama',\n 'huggingface:lambada',\n 'huggingface:large_spanish_corpus',\n 'huggingface:laroseda',\n 'huggingface:lc_quad',\n 'huggingface:lccc',\n 'huggingface:lener_br',\n 'huggingface:lex_glue',\n 'huggingface:liar',\n 'huggingface:librispeech_asr',\n 'huggingface:librispeech_lm',\n 'huggingface:limit',\n 'huggingface:lince',\n 'huggingface:linnaeus',\n 'huggingface:liveqa',\n 'huggingface:lj_speech',\n 'huggingface:lm1b',\n 'huggingface:lst20',\n 'huggingface:m_lama',\n 'huggingface:mac_morpho',\n 'huggingface:makhzan',\n 'huggingface:masakhaner',\n 'huggingface:math_dataset',\n 'huggingface:math_qa',\n 'huggingface:matinf',\n 'huggingface:mbpp',\n 'huggingface:mc4',\n 'huggingface:mc_taco',\n 'huggingface:md_gender_bias',\n 'huggingface:mdd',\n 'huggingface:med_hop',\n 'huggingface:medal',\n 'huggingface:medical_dialog',\n 'huggingface:medical_questions_pairs',\n 'huggingface:medmcqa',\n 'huggingface:menyo20k_mt',\n 'huggingface:meta_woz',\n 'huggingface:metashift',\n 'huggingface:metooma',\n 'huggingface:metrec',\n 'huggingface:miam',\n 'huggingface:mkb',\n 'huggingface:mkqa',\n 'huggingface:mlqa',\n 'huggingface:mlsum',\n 'huggingface:mnist',\n 'huggingface:mocha',\n 'huggingface:monash_tsf',\n 'huggingface:moroco',\n 'huggingface:movie_rationales',\n 'huggingface:mrqa',\n 'huggingface:ms_marco',\n 'huggingface:ms_terms',\n 'huggingface:msr_genomics_kbcomp',\n 'huggingface:msr_sqa',\n 'huggingface:msr_text_compression',\n 'huggingface:msr_zhen_translation_parity',\n 'huggingface:msra_ner',\n 'huggingface:mt_eng_vietnamese',\n 'huggingface:muchocine',\n 'huggingface:multi_booked',\n 'huggingface:multi_eurlex',\n 'huggingface:multi_news',\n 'huggingface:multi_nli',\n 'huggingface:multi_nli_mismatch',\n 'huggingface:multi_para_crawl',\n 'huggingface:multi_re_qa',\n 'huggingface:multi_woz_v22',\n 'huggingface:multi_x_science_sum',\n 'huggingface:multidoc2dial',\n 'huggingface:multilingual_librispeech',\n 'huggingface:mutual_friends',\n 'huggingface:mwsc',\n 'huggingface:myanmar_news',\n 'huggingface:narrativeqa',\n 'huggingface:narrativeqa_manual',\n 'huggingface:natural_questions',\n 'huggingface:ncbi_disease',\n 'huggingface:nchlt',\n 'huggingface:ncslgr',\n 'huggingface:nell',\n 'huggingface:neural_code_search',\n 'huggingface:news_commentary',\n 'huggingface:newsgroup',\n 'huggingface:newsph',\n 'huggingface:newsph_nli',\n 'huggingface:newspop',\n 'huggingface:newsqa',\n 'huggingface:newsroom',\n 'huggingface:nkjp-ner',\n 'huggingface:nli_tr',\n 'huggingface:nlu_evaluation_data',\n 'huggingface:norec',\n 'huggingface:norne',\n 'huggingface:norwegian_ner',\n 'huggingface:nq_open',\n 'huggingface:nsmc',\n 'huggingface:numer_sense',\n 'huggingface:numeric_fused_head',\n 'huggingface:oclar',\n 'huggingface:offcombr',\n 'huggingface:offenseval2020_tr',\n 'huggingface:offenseval_dravidian',\n 'huggingface:ofis_publik',\n 'huggingface:ohsumed',\n 'huggingface:ollie',\n 'huggingface:omp',\n 'huggingface:onestop_english',\n 'huggingface:onestop_qa',\n 'huggingface:open_subtitles',\n 'huggingface:openai_humaneval',\n 'huggingface:openbookqa',\n 'huggingface:openslr',\n 'huggingface:openwebtext',\n 'huggingface:opinosis',\n 'huggingface:opus100',\n 'huggingface:opus_books',\n 'huggingface:opus_dgt',\n 'huggingface:opus_dogc',\n 'huggingface:opus_elhuyar',\n 'huggingface:opus_euconst',\n 'huggingface:opus_finlex',\n 'huggingface:opus_fiskmo',\n 'huggingface:opus_gnome',\n 'huggingface:opus_infopankki',\n 'huggingface:opus_memat',\n 'huggingface:opus_montenegrinsubs',\n 'huggingface:opus_openoffice',\n 'huggingface:opus_paracrawl',\n 'huggingface:opus_rf',\n 'huggingface:opus_tedtalks',\n 'huggingface:opus_ubuntu',\n 'huggingface:opus_wikipedia',\n 'huggingface:opus_xhosanavy',\n 'huggingface:orange_sum',\n 'huggingface:oscar',\n 'huggingface:para_crawl',\n 'huggingface:para_pat',\n 'huggingface:parsinlu_reading_comprehension',\n 'huggingface:pass',\n 'huggingface:paws',\n 'huggingface:paws-x',\n 'huggingface:pec',\n 'huggingface:peer_read',\n 'huggingface:peoples_daily_ner',\n 'huggingface:per_sent',\n 'huggingface:persian_ner',\n 'huggingface:pg19',\n 'huggingface:php',\n 'huggingface:piaf',\n 'huggingface:pib',\n 'huggingface:piqa',\n 'huggingface:pn_summary',\n 'huggingface:poem_sentiment',\n 'huggingface:polemo2',\n 'huggingface:poleval2019_cyberbullying',\n 'huggingface:poleval2019_mt',\n 'huggingface:polsum',\n 'huggingface:polyglot_ner',\n 'huggingface:prachathai67k',\n 'huggingface:pragmeval',\n 'huggingface:proto_qa',\n 'huggingface:psc',\n 'huggingface:ptb_text_only',\n 'huggingface:pubmed',\n 'huggingface:pubmed_qa',\n 'huggingface:py_ast',\n 'huggingface:qa4mre',\n 'huggingface:qa_srl',\n 'huggingface:qa_zre',\n 'huggingface:qangaroo',\n 'huggingface:qanta',\n 'huggingface:qasc',\n 'huggingface:qasper',\n 'huggingface:qed',\n 'huggingface:qed_amara',\n 'huggingface:quac',\n 'huggingface:quail',\n 'huggingface:quarel',\n 'huggingface:quartz',\n 'huggingface:quickdraw',\n 'huggingface:quora',\n 'huggingface:quoref',\n 'huggingface:race',\n 'huggingface:re_dial',\n 'huggingface:reasoning_bg',\n 'huggingface:recipe_nlg',\n 'huggingface:reclor',\n 'huggingface:red_caps',\n 'huggingface:reddit',\n 'huggingface:reddit_tifu',\n 'huggingface:refresd',\n 'huggingface:reuters21578',\n 'huggingface:riddle_sense',\n 'huggingface:ro_sent',\n 'huggingface:ro_sts',\n 'huggingface:ro_sts_parallel',\n 'huggingface:roman_urdu',\n 'huggingface:roman_urdu_hate_speech',\n 'huggingface:ronec',\n 'huggingface:ropes',\n 'huggingface:rotten_tomatoes',\n 'huggingface:russian_super_glue',\n 'huggingface:rvl_cdip',\n 'huggingface:s2orc',\n 'huggingface:samsum',\n 'huggingface:sanskrit_classic',\n 'huggingface:saudinewsnet',\n 'huggingface:sberquad',\n 'huggingface:sbu_captions',\n 'huggingface:scan',\n 'huggingface:scb_mt_enth_2020',\n 'huggingface:scene_parse_150',\n 'huggingface:schema_guided_dstc8',\n 'huggingface:scicite',\n 'huggingface:scielo',\n 'huggingface:scientific_papers',\n 'huggingface:scifact',\n 'huggingface:sciq',\n 'huggingface:scitail',\n 'huggingface:scitldr',\n 'huggingface:search_qa',\n 'huggingface:sede',\n 'huggingface:selqa',\n 'huggingface:sem_eval_2010_task_8',\n 'huggingface:sem_eval_2014_task_1',\n 'huggingface:sem_eval_2018_task_1',\n 'huggingface:sem_eval_2020_task_11',\n 'huggingface:sent_comp',\n 'huggingface:senti_lex',\n 'huggingface:senti_ws',\n 'huggingface:sentiment140',\n 'huggingface:sepedi_ner',\n 'huggingface:sesotho_ner_corpus',\n 'huggingface:setimes',\n 'huggingface:setswana_ner_corpus',\n 'huggingface:sharc',\n 'huggingface:sharc_modified',\n 'huggingface:sick',\n 'huggingface:silicone',\n 'huggingface:simple_questions_v2',\n 'huggingface:siswati_ner_corpus',\n 'huggingface:smartdata',\n 'huggingface:sms_spam',\n 'huggingface:snips_built_in_intents',\n 'huggingface:snli',\n 'huggingface:snow_simplified_japanese_corpus',\n 'huggingface:so_stacksample',\n 'huggingface:social_bias_frames',\n 'huggingface:social_i_qa',\n 'huggingface:sofc_materials_articles',\n ...]</pre> <p>We will be using the imdb reviews dataset as our dataset for the session today. This is a text classification dataset where each label can be 0 or 1, indicating a positive or negative review.</p> In\u00a0[\u00a0]: Copied! <pre># Split the training set into 60% and 40% to end up with 15,000 examples\n# for training, 10,000 examples for validation and 25,000 examples for testing.\ntrain_data, validation_data, test_data = tfds.load(\n    name=\"imdb_reviews\",\n    split=('train[:60%]', 'train[60%:]', 'test'),\n    as_supervised=True)\n</pre> # Split the training set into 60% and 40% to end up with 15,000 examples # for training, 10,000 examples for validation and 25,000 examples for testing. train_data, validation_data, test_data = tfds.load(     name=\"imdb_reviews\",     split=('train[:60%]', 'train[60%:]', 'test'),     as_supervised=True) <pre>Downloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\n</pre> <pre>Dl Completed...: 0 url [00:00, ? url/s]</pre> <pre>Dl Size...: 0 MiB [00:00, ? MiB/s]</pre> <pre>Generating splits...:   0%|          | 0/3 [00:00&lt;?, ? splits/s]</pre> <pre>Generating train examples...:   0%|          | 0/25000 [00:00&lt;?, ? examples/s]</pre> <pre>Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteEJYEOD/imdb_reviews-train.tfrecord\u2026</pre> <pre>Generating test examples...:   0%|          | 0/25000 [00:00&lt;?, ? examples/s]</pre> <pre>Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteEJYEOD/imdb_reviews-test.tfrecord*\u2026</pre> <pre>Generating unsupervised examples...:   0%|          | 0/50000 [00:00&lt;?, ? examples/s]</pre> <pre>Shuffling /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteEJYEOD/imdb_reviews-unsupervised.t\u2026</pre> <pre>Dataset imdb_reviews downloaded and prepared to /root/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\n</pre> In\u00a0[\u00a0]: Copied! <pre>train_data = train_data.shuffle(10)\n</pre> train_data = train_data.shuffle(10) In\u00a0[\u00a0]: Copied! <pre>text_batch, label_batch = next(iter(train_data.batch(5)))\nfor i in range(5):\n\n  print(\"Review: \", text_batch.numpy()[i])\n  print(\"Label:\", label_batch.numpy()[i])\n</pre> text_batch, label_batch = next(iter(train_data.batch(5))) for i in range(5):    print(\"Review: \", text_batch.numpy()[i])   print(\"Label:\", label_batch.numpy()[i])  <pre>Review:  b'As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.'\nLabel: 1\nReview:  b'This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.'\nLabel: 1\nReview:  b'Okay, you have:&lt;br /&gt;&lt;br /&gt;Penelope Keith as Miss Herringbone-Tweed, B.B.E. (Backbone of England.) She\\'s killed off in the first scene - that\\'s right, folks; this show has no backbone!&lt;br /&gt;&lt;br /&gt;Peter O\\'Toole as Ol\\' Colonel Cricket from The First War and now the emblazered Lord of the Manor.&lt;br /&gt;&lt;br /&gt;Joanna Lumley as the ensweatered Lady of the Manor, 20 years younger than the colonel and 20 years past her own prime but still glamourous (Brit spelling, not mine) enough to have a toy-boy on the side. It\\'s alright, they have Col. Cricket\\'s full knowledge and consent (they guy even comes \\'round for Christmas!) Still, she\\'s considerate of the colonel enough to have said toy-boy her own age (what a gal!)&lt;br /&gt;&lt;br /&gt;David McCallum as said toy-boy, equally as pointlessly glamourous as his squeeze. Pilcher couldn\\'t come up with any cover for him within the story, so she gave him a hush-hush job at the Circus.&lt;br /&gt;&lt;br /&gt;and finally:&lt;br /&gt;&lt;br /&gt;Susan Hampshire as Miss Polonia Teacups, Venerable Headmistress of the Venerable Girls\\' Boarding-School, serving tea in her office with a dash of deep, poignant advice for life in the outside world just before graduation. Her best bit of advice: \"I\\'ve only been to Nancherrow (the local Stately Home of England) once. I thought it was very beautiful but, somehow, not part of the real world.\" Well, we can\\'t say they didn\\'t warn us.&lt;br /&gt;&lt;br /&gt;Ah, Susan - time was, your character would have been running the whole show. They don\\'t write \\'em like that any more. Our loss, not yours.&lt;br /&gt;&lt;br /&gt;So - with a cast and setting like this, you have the re-makings of \"Brideshead Revisited,\" right?&lt;br /&gt;&lt;br /&gt;Wrong! They took these 1-dimensional supporting roles because they paid so well. After all, acting is one of the oldest temp-jobs there is (YOU name another!)&lt;br /&gt;&lt;br /&gt;First warning sign: lots and lots of backlighting. They get around it by shooting outdoors - \"hey, it\\'s just the sunlight!\"&lt;br /&gt;&lt;br /&gt;Second warning sign: Leading Lady cries a lot. When not crying, her eyes are moist. That\\'s the law of romance novels: Leading Lady is \"dewy-eyed.\"&lt;br /&gt;&lt;br /&gt;Henceforth, Leading Lady shall be known as L.L.&lt;br /&gt;&lt;br /&gt;Third warning sign: L.L. actually has stars in her eyes when she\\'s in love. Still, I\\'ll give Emily Mortimer an award just for having to act with that spotlight in her eyes (I wonder . did they use contacts?)&lt;br /&gt;&lt;br /&gt;And lastly, fourth warning sign: no on-screen female character is \"Mrs.\" She\\'s either \"Miss\" or \"Lady.\"&lt;br /&gt;&lt;br /&gt;When all was said and done, I still couldn\\'t tell you who was pursuing whom and why. I couldn\\'t even tell you what was said and done.&lt;br /&gt;&lt;br /&gt;To sum up: they all live through World War II without anything happening to them at all.&lt;br /&gt;&lt;br /&gt;OK, at the end, L.L. finds she\\'s lost her parents to the Japanese prison camps and baby sis comes home catatonic. Meanwhile (there\\'s always a \"meanwhile,\") some young guy L.L. had a crush on (when, I don\\'t know) comes home from some wartime tough spot and is found living on the street by Lady of the Manor (must be some street if SHE\\'s going to find him there.) Both war casualties are whisked away to recover at Nancherrow (SOMEBODY has to be \"whisked away\" SOMEWHERE in these romance stories!)&lt;br /&gt;&lt;br /&gt;Great drama.'\nLabel: 0\nReview:  b'Cute film about three lively sisters from Switzerland (often seen running about in matching outfits) who want to get their parents back together (seems mom is still carrying the torch for dad) - so they sail off to New York to stop the dad from marrying a blonde gold-digger he calls \"Precious\". Dad hasn\\'t seen his daughters in ten years, they (oddly enough) don\\'t seem to mind and think he\\'s wonderful, and meanwhile Precious seems to lead a life mainly run by her overbearing mother (Alice Brady), a woman who just wants to see to it her daughter marries a rich man. The sisters get the idea of pushing Precious into the path of a drunken Hungarian count, tricking the two gold-digging women into thinking he is one of the richest men in Europe. But a case of mistaken identity makes the girls think the count is good-looking Ray Milland, who goes along with the scheme \\'cause he has a crush on sister Kay.&lt;br /&gt;&lt;br /&gt;This film is enjoyable, light fare. Barbara Read as Kay comes across as sweet and pretty, Ray Milland looks oh so young and handsome here (though, unfortunately, is given little to do), Alice Brady is quite good as the scheming mother - but it is Deanna Durbin, a real charmer and cute as a button playing youngest sister Penny, who pretty much steals the show. With absolutely beautiful vocals, she sings several songs throughout the film, though I actually would have liked to have seen them feature her even more in this. The plot in this film is a bit silly, but nevertheless, I found the film to be entertaining and fun.'\nLabel: 1\nReview:  b'Put the blame on executive producer Wes Craven and financiers the Weinsteins for this big-budget debacle: a thrash-metal updating of \"Dracula\", with a condescending verbal jab at Bram Stoker (who probably wouldn\\'t want his name on this thing anyway) and nothing much for the rest of us except slasher-styled jolts and gore. Christopher Plummer looks winded as Van Helsing in the modern-day--not just a descendant of Van Helsing but the real thing; he keeps himself going with leeches obtained from Count Dracula\\'s corpse, which is exhumed from its coffin after being stolen from Van Helsing\\'s vault and flown to New Orleans. This is just what New Orleans needs in the 21st Century! The film, well-produced but without a single original idea (except for multi-racial victims), is both repulsive and lazy, and after about an hour starts repeating itself. * from ****'\nLabel: 0\n</pre> <p>Tokenization is when we split our text into chunks and assign each of them a unique numerical ID.</p> In\u00a0[\u00a0]: Copied! <pre>word = \"silent\"\nanother_word = \"listen\"\n</pre> word = \"silent\" another_word = \"listen\" In\u00a0[\u00a0]: Copied! <pre>[ord(char) for char in word]\n</pre> [ord(char) for char in word] Out[\u00a0]: <pre>[115, 105, 108, 101, 110, 116]</pre> In\u00a0[\u00a0]: Copied! <pre>[ord(char) for char in another_word]\n</pre> [ord(char) for char in another_word] Out[\u00a0]: <pre>[108, 105, 115, 116, 101, 110]</pre> <p></p> <p></p> In\u00a0[\u00a0]: Copied! <pre>raw_data = text_batch.numpy()\n\nraw_data\n</pre> raw_data = text_batch.numpy()  raw_data Out[\u00a0]: <pre>array([b'As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.',\n       b'This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.',\n       b'Okay, you have:&lt;br /&gt;&lt;br /&gt;Penelope Keith as Miss Herringbone-Tweed, B.B.E. (Backbone of England.) She\\'s killed off in the first scene - that\\'s right, folks; this show has no backbone!&lt;br /&gt;&lt;br /&gt;Peter O\\'Toole as Ol\\' Colonel Cricket from The First War and now the emblazered Lord of the Manor.&lt;br /&gt;&lt;br /&gt;Joanna Lumley as the ensweatered Lady of the Manor, 20 years younger than the colonel and 20 years past her own prime but still glamourous (Brit spelling, not mine) enough to have a toy-boy on the side. It\\'s alright, they have Col. Cricket\\'s full knowledge and consent (they guy even comes \\'round for Christmas!) Still, she\\'s considerate of the colonel enough to have said toy-boy her own age (what a gal!)&lt;br /&gt;&lt;br /&gt;David McCallum as said toy-boy, equally as pointlessly glamourous as his squeeze. Pilcher couldn\\'t come up with any cover for him within the story, so she gave him a hush-hush job at the Circus.&lt;br /&gt;&lt;br /&gt;and finally:&lt;br /&gt;&lt;br /&gt;Susan Hampshire as Miss Polonia Teacups, Venerable Headmistress of the Venerable Girls\\' Boarding-School, serving tea in her office with a dash of deep, poignant advice for life in the outside world just before graduation. Her best bit of advice: \"I\\'ve only been to Nancherrow (the local Stately Home of England) once. I thought it was very beautiful but, somehow, not part of the real world.\" Well, we can\\'t say they didn\\'t warn us.&lt;br /&gt;&lt;br /&gt;Ah, Susan - time was, your character would have been running the whole show. They don\\'t write \\'em like that any more. Our loss, not yours.&lt;br /&gt;&lt;br /&gt;So - with a cast and setting like this, you have the re-makings of \"Brideshead Revisited,\" right?&lt;br /&gt;&lt;br /&gt;Wrong! They took these 1-dimensional supporting roles because they paid so well. After all, acting is one of the oldest temp-jobs there is (YOU name another!)&lt;br /&gt;&lt;br /&gt;First warning sign: lots and lots of backlighting. They get around it by shooting outdoors - \"hey, it\\'s just the sunlight!\"&lt;br /&gt;&lt;br /&gt;Second warning sign: Leading Lady cries a lot. When not crying, her eyes are moist. That\\'s the law of romance novels: Leading Lady is \"dewy-eyed.\"&lt;br /&gt;&lt;br /&gt;Henceforth, Leading Lady shall be known as L.L.&lt;br /&gt;&lt;br /&gt;Third warning sign: L.L. actually has stars in her eyes when she\\'s in love. Still, I\\'ll give Emily Mortimer an award just for having to act with that spotlight in her eyes (I wonder . did they use contacts?)&lt;br /&gt;&lt;br /&gt;And lastly, fourth warning sign: no on-screen female character is \"Mrs.\" She\\'s either \"Miss\" or \"Lady.\"&lt;br /&gt;&lt;br /&gt;When all was said and done, I still couldn\\'t tell you who was pursuing whom and why. I couldn\\'t even tell you what was said and done.&lt;br /&gt;&lt;br /&gt;To sum up: they all live through World War II without anything happening to them at all.&lt;br /&gt;&lt;br /&gt;OK, at the end, L.L. finds she\\'s lost her parents to the Japanese prison camps and baby sis comes home catatonic. Meanwhile (there\\'s always a \"meanwhile,\") some young guy L.L. had a crush on (when, I don\\'t know) comes home from some wartime tough spot and is found living on the street by Lady of the Manor (must be some street if SHE\\'s going to find him there.) Both war casualties are whisked away to recover at Nancherrow (SOMEBODY has to be \"whisked away\" SOMEWHERE in these romance stories!)&lt;br /&gt;&lt;br /&gt;Great drama.',\n       b'Cute film about three lively sisters from Switzerland (often seen running about in matching outfits) who want to get their parents back together (seems mom is still carrying the torch for dad) - so they sail off to New York to stop the dad from marrying a blonde gold-digger he calls \"Precious\". Dad hasn\\'t seen his daughters in ten years, they (oddly enough) don\\'t seem to mind and think he\\'s wonderful, and meanwhile Precious seems to lead a life mainly run by her overbearing mother (Alice Brady), a woman who just wants to see to it her daughter marries a rich man. The sisters get the idea of pushing Precious into the path of a drunken Hungarian count, tricking the two gold-digging women into thinking he is one of the richest men in Europe. But a case of mistaken identity makes the girls think the count is good-looking Ray Milland, who goes along with the scheme \\'cause he has a crush on sister Kay.&lt;br /&gt;&lt;br /&gt;This film is enjoyable, light fare. Barbara Read as Kay comes across as sweet and pretty, Ray Milland looks oh so young and handsome here (though, unfortunately, is given little to do), Alice Brady is quite good as the scheming mother - but it is Deanna Durbin, a real charmer and cute as a button playing youngest sister Penny, who pretty much steals the show. With absolutely beautiful vocals, she sings several songs throughout the film, though I actually would have liked to have seen them feature her even more in this. The plot in this film is a bit silly, but nevertheless, I found the film to be entertaining and fun.',\n       b'Put the blame on executive producer Wes Craven and financiers the Weinsteins for this big-budget debacle: a thrash-metal updating of \"Dracula\", with a condescending verbal jab at Bram Stoker (who probably wouldn\\'t want his name on this thing anyway) and nothing much for the rest of us except slasher-styled jolts and gore. Christopher Plummer looks winded as Van Helsing in the modern-day--not just a descendant of Van Helsing but the real thing; he keeps himself going with leeches obtained from Count Dracula\\'s corpse, which is exhumed from its coffin after being stolen from Van Helsing\\'s vault and flown to New Orleans. This is just what New Orleans needs in the 21st Century! The film, well-produced but without a single original idea (except for multi-racial victims), is both repulsive and lazy, and after about an hour starts repeating itself. * from ****'],\n      dtype=object)</pre> In\u00a0[\u00a0]: Copied! <pre>plain_text = [i.decode(\"utf-8\") for i in raw_data]\n\nprint(plain_text[0])\n</pre> plain_text = [i.decode(\"utf-8\") for i in raw_data]  print(plain_text[0]) <pre>As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.\n</pre> In\u00a0[\u00a0]: Copied! <pre>word_tokenized = []\n\nfor i in range(len(plain_text)):\n  word_tokenized.append(tf.keras.preprocessing.text.text_to_word_sequence(plain_text[i]))\n\nword_tokenized[0]\n</pre> word_tokenized = []  for i in range(len(plain_text)):   word_tokenized.append(tf.keras.preprocessing.text.text_to_word_sequence(plain_text[i]))  word_tokenized[0] Out[\u00a0]: <pre>['as',\n 'others',\n 'have',\n 'mentioned',\n 'all',\n 'the',\n 'women',\n 'that',\n 'go',\n 'nude',\n 'in',\n 'this',\n 'film',\n 'are',\n 'mostly',\n 'absolutely',\n 'gorgeous',\n 'the',\n 'plot',\n 'very',\n 'ably',\n 'shows',\n 'the',\n 'hypocrisy',\n 'of',\n 'the',\n 'female',\n 'libido',\n 'when',\n 'men',\n 'are',\n 'around',\n 'they',\n 'want',\n 'to',\n 'be',\n 'pursued',\n 'but',\n 'when',\n 'no',\n 'men',\n 'are',\n 'around',\n 'they',\n 'become',\n 'the',\n 'pursuers',\n 'of',\n 'a',\n '14',\n 'year',\n 'old',\n 'boy',\n 'and',\n 'the',\n 'boy',\n 'becomes',\n 'a',\n 'man',\n 'really',\n 'fast',\n 'we',\n 'should',\n 'all',\n 'be',\n 'so',\n 'lucky',\n 'at',\n 'this',\n 'age',\n 'he',\n 'then',\n 'gets',\n 'up',\n 'the',\n 'courage',\n 'to',\n 'pursue',\n 'his',\n 'true',\n 'love']</pre> In\u00a0[\u00a0]: Copied! <pre>max_num_words = 10000\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_num_words)\n\ntokenizer.fit_on_texts(word_tokenized)\n\ntokens = tokenizer.texts_to_sequences(word_tokenized)\n\nprint(tokens[0])\n</pre> max_num_words = 10000  tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_num_words)  tokenizer.fit_on_texts(word_tokenized)  tokens = tokenizer.texts_to_sequences(word_tokenized)  print(tokens[0]) <pre>[8, 174, 14, 175, 25, 1, 90, 40, 91, 176, 9, 11, 15, 26, 177, 92, 178, 1, 49, 93, 179, 180, 1, 181, 5, 1, 94, 182, 20, 50, 26, 51, 10, 52, 6, 27, 183, 18, 20, 33, 50, 26, 51, 10, 184, 1, 185, 5, 3, 186, 187, 188, 34, 4, 1, 34, 189, 3, 95, 190, 191, 96, 192, 25, 27, 28, 193, 21, 11, 97, 35, 194, 195, 53, 1, 196, 6, 197, 41, 198, 98]\n</pre> In\u00a0[\u00a0]: Copied! <pre># Convert the dataset from a gen into dataset format\ntext_batch, label_batch = next(iter(train_data.batch(128)))\n\ntext_dataset = tf.data.Dataset.from_tensor_slices(text_batch)\n\ntext_dataset\n</pre> # Convert the dataset from a gen into dataset format text_batch, label_batch = next(iter(train_data.batch(128)))  text_dataset = tf.data.Dataset.from_tensor_slices(text_batch)  text_dataset Out[\u00a0]: <pre>&lt;_TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.string, name=None)&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>vectorize_layer = tf.keras.layers.TextVectorization(\n max_tokens=max_num_words,\n output_mode='int',\n output_sequence_length=10)\n\nvectorize_layer.adapt(text_dataset.batch(64))\n</pre> vectorize_layer = tf.keras.layers.TextVectorization(  max_tokens=max_num_words,  output_mode='int',  output_sequence_length=10)  vectorize_layer.adapt(text_dataset.batch(64)) In\u00a0[\u00a0]: Copied! <pre>vectorize_layer.get_vocabulary()\n</pre> vectorize_layer.get_vocabulary() Out[\u00a0]: <pre>['',\n '[UNK]',\n 'the',\n 'of',\n 'and',\n 'a',\n 'to',\n 'is',\n 'in',\n 'this',\n 'i',\n 'it',\n 'that',\n 'br',\n 'as',\n 'with',\n 'for',\n 'was',\n 'film',\n 'you',\n 'movie',\n 'but',\n 'are',\n 'one',\n 'have',\n 'be',\n 'on',\n 'his',\n 'not',\n 'all',\n 'they',\n 'just',\n 'an',\n 'by',\n 'from',\n 'at',\n 'so',\n 'her',\n 'who',\n 'has',\n 'its',\n 'he',\n 'if',\n 'about',\n 'what',\n 'some',\n 'or',\n 'like',\n 'no',\n 'my',\n 'when',\n 'there',\n 'their',\n 'out',\n 'she',\n 'which',\n 'will',\n 'more',\n 'good',\n 'see',\n 'first',\n 'me',\n 'would',\n 'most',\n 'them',\n 'up',\n 'had',\n 'get',\n 'well',\n 'too',\n 'other',\n 'movies',\n 'do',\n 'even',\n 'story',\n 'people',\n 'only',\n 'into',\n 'dont',\n 'were',\n 'very',\n 'can',\n 'really',\n 'also',\n 'way',\n 'then',\n 'seen',\n 'any',\n 'great',\n 'been',\n 'we',\n 'many',\n 'than',\n 'how',\n 'ever',\n 'think',\n 'these',\n 'should',\n 'time',\n 'much',\n 'make',\n 'films',\n 'actors',\n 'could',\n 'best',\n 'while',\n 'plot',\n 'did',\n 'after',\n 'him',\n 'say',\n 'because',\n 'three',\n 'such',\n 'still',\n 'scenes',\n 'made',\n 'end',\n 'being',\n 'world',\n 'thing',\n 'know',\n 'bad',\n 'why',\n 'want',\n 'two',\n 'through',\n 'real',\n 'pretty',\n 'go',\n 'character',\n 'big',\n 'us',\n 'scene',\n 'own',\n 'nothing',\n 'never',\n 'lot',\n 'im',\n 'every',\n 'your',\n 'yet',\n 'take',\n 'show',\n 'off',\n 'life',\n 'enough',\n 'doesnt',\n 'does',\n 'cast',\n 'acting',\n 'those',\n 'quite',\n 'little',\n 'funny',\n 'few',\n 'whole',\n 'where',\n 'over',\n 'makes',\n 'ive',\n 'back',\n '\\x96',\n 'watch',\n 'saw',\n 'right',\n 'performance',\n 'our',\n 'man',\n 'king',\n 'hollywood',\n 'here',\n 'find',\n 'better',\n 'another',\n 'years',\n 'same',\n 'new',\n 'love',\n 'down',\n 'director',\n 'come',\n 'before',\n 'around',\n 'though',\n 'old',\n 'now',\n 'give',\n 'gets',\n 'characters',\n 'without',\n 'thats',\n 'something',\n 'script',\n 'part',\n 'might',\n 'isnt',\n 'going',\n 'girl',\n 'family',\n 'tv',\n 'throughout',\n 'last',\n 'home',\n 'having',\n 'feel',\n 'each',\n 'didnt',\n 'both',\n 'always',\n 'actually',\n 'actor',\n 'watching',\n 'theres',\n 'simply',\n 'shes',\n 'role',\n 'place',\n 'name',\n 'minutes',\n 'high',\n 'fact',\n 'done',\n 'comedy',\n 'between',\n 'again',\n 'youre',\n 'work',\n 'trying',\n 'true',\n 'tell',\n 'since',\n 'seems',\n 'may',\n 'looks',\n 'enjoy',\n 'young',\n 'thought',\n 'things',\n 'sense',\n 'second',\n 'read',\n 'point',\n 'performances',\n 'mean',\n 'lion',\n 'lets',\n 'let',\n 'head',\n 'guy',\n 'goes',\n 'course',\n 'bit',\n 'believe',\n 'away',\n 'action',\n 'absolutely',\n 'worst',\n 'turn',\n 'seeing',\n 'rather',\n 'production',\n 'once',\n 'must',\n 'lucy',\n 'least',\n 'kids',\n 'instead',\n 'however',\n 'house',\n 'fun',\n 'far',\n 'especially',\n 'day',\n 'yes',\n 'unfortunately',\n 'reason',\n 'quantum',\n 'look',\n 'lives',\n 'liked',\n 'idea',\n 'himself',\n 'hes',\n 'evil',\n 'comes',\n 'cant',\n 'beautiful',\n 'am',\n 'age',\n 'wife',\n 'truly',\n 'town',\n 'theyre',\n 'sure',\n 'short',\n 'series',\n 'seem',\n 'said',\n 'rest',\n 'recommend',\n 'problem',\n 'oh',\n 'nice',\n 'need',\n 'music',\n 'mind',\n 'main',\n 'lots',\n 'line',\n 'kind',\n 'job',\n 'humor',\n 'horror',\n 'given',\n 'fans',\n 'executive',\n 'either',\n 'during',\n 'completely',\n 'classic',\n 'called',\n 'bach',\n 'american',\n 'wont',\n 'wonderful',\n 'wonder',\n 'used',\n 'timon',\n 'takes',\n 'start',\n 'seriously',\n 'seemed',\n 'room',\n 'remember',\n 'pumbaa',\n 'perhaps',\n 'ones',\n 'often',\n 'night',\n 'money',\n 'making',\n 'less',\n 'interesting',\n 'full',\n 'eyes',\n 'excellent',\n 'etc',\n 'episodes',\n 'ending',\n 'dvd',\n 'doing',\n 'couldnt',\n 'casting',\n 'cannot',\n 'camera',\n 'behind',\n 'based',\n 'awful',\n 'audience',\n 'anything',\n 'anyone',\n 'along',\n 'wants',\n 'understand',\n 'top',\n 'took',\n 'times',\n 'style',\n 'studio',\n 'streisand',\n 'stars',\n 'shows',\n 'shots',\n 'sets',\n 'set',\n 'screen',\n 'says',\n 'rachel',\n 'poor',\n 'particularly',\n 'others',\n 'mr',\n 'late',\n 'girls',\n 'gave',\n 'fuqua',\n 'foxx',\n 'found',\n 'fine',\n 'finally',\n 'felt',\n 'fan',\n 'except',\n 'everything',\n 'everyone',\n 'entire',\n 'else',\n 'easy',\n 'early',\n 'close',\n 'city',\n 'christmas',\n 'certainly',\n 'canadian',\n 'book',\n 'beginning',\n 'avoid',\n 'appropriate',\n 'anyway',\n 'although',\n 'almost',\n 'against',\n 'act',\n 'york',\n 'worth',\n 'words',\n 'woman',\n 'warning',\n 'version',\n 'use',\n 'upon',\n 'until',\n 'twist',\n 'turned',\n 'try',\n 'together',\n 'thinking',\n 'tale',\n 'surprised',\n 'stupid',\n 'strong',\n 'star',\n 'sort',\n 'someone',\n 'slugs',\n 'similar',\n 'side',\n 'scrooge',\n 'scott',\n 'school',\n 'probably',\n 'play',\n 'piece',\n 'past',\n 'named',\n 'mother',\n 'michael',\n 'men',\n 'memorable',\n 'meet',\n 'meanwhile',\n 'married',\n 'looking',\n 'local',\n 'lady',\n 'known',\n 'killed',\n 'jokes',\n 'john',\n 'jean',\n 'human',\n 'hilarious',\n 'festival',\n 'female',\n 'famous',\n 'expect',\n 'example',\n 'episode',\n 'entertaining',\n 'deathstalker',\n 'dead',\n 'coming',\n 'buy',\n 'boy',\n 'black',\n 'basic',\n 'barbra',\n 'bait',\n 'arthur',\n 'art',\n 'arent',\n 'appeared',\n 'able',\n 'youve',\n 'youll',\n 'written',\n 'wish',\n 'watched',\n 'wasnt',\n 'war',\n 'waiting',\n 'video',\n 'usually',\n 'under',\n 'type',\n 'therefore',\n 'terrible',\n 'team',\n 'sword',\n 'supporting',\n 'stay',\n 'starts',\n 'spirit',\n 'space',\n 'sometimes',\n 'small',\n 'sister',\n 'shorts',\n 'serious',\n 'sad',\n 'running',\n 'roles',\n 'realize',\n 'reality',\n 'portrayal',\n 'plays',\n 'played',\n 'peter',\n 'perfect',\n 'paul',\n 'parents',\n 'overall',\n 'original',\n 'note',\n 'nor',\n 'negative',\n 'nearly',\n 'missed',\n 'miss',\n 'maybe',\n 'mall',\n 'lord',\n 'long',\n 'leave',\n 'leading',\n 'ill',\n 'ideas',\n 'id',\n 'hit',\n 'history',\n 'hero',\n 'hand',\n 'guys',\n 'got',\n 'gives',\n 'genre',\n 'filmbr',\n 'feeling',\n 'favorite',\n 'fast',\n 'familiar',\n 'eye',\n 'enjoyed',\n 'emotional',\n 'easily',\n 'disappointed',\n 'decent',\n 'days',\n 'couple',\n 'comment',\n 'comic',\n 'cinema',\n 'career',\n 'care',\n 'car',\n 'bunch',\n 'bring',\n 'brilliant',\n 'bright',\n 'books',\n 'band',\n 'animation',\n '3',\n '12',\n '\\x85',\n 'yourself',\n 'younger',\n 'year',\n 'word',\n 'wild',\n 'whom',\n 'werent',\n 'weird',\n 'warrior',\n 'truth',\n 'tries',\n 'thrown',\n 'theory',\n 'themselves',\n 'superb',\n 'street',\n 'strange',\n 'stories',\n 'stooges',\n 'stage',\n 'songs',\n 'song',\n 'somewhat',\n 'slow',\n 'skull',\n 'silly',\n 'sign',\n 'setting',\n 'sadly',\n 'run',\n 'rich',\n 'review',\n 'return',\n 'remarkable',\n 'red',\n 'question',\n 'quality',\n 'previous',\n 'presidents',\n 'premise',\n 'possibly',\n 'physics',\n 'particular',\n 'particles',\n 'oscar',\n 'ok',\n 'number',\n 'nancy',\n 'naked',\n 'muni',\n 'mrs',\n 'moments',\n 'meant',\n 'matter',\n 'material',\n 'male',\n 'lynch',\n 'loved',\n 'lost',\n 'lines',\n 'level',\n 'leaves',\n 'lamm',\n 'kelly',\n 'keller',\n 'keep',\n 'james',\n 'itself',\n 'itbr',\n 'immediately',\n 'imagination',\n 'hours',\n 'highly',\n 'help',\n 'hell',\n 'havent',\n 'harlow',\n 'happens',\n 'happen',\n 'guess',\n 'greatest',\n 'gone',\n 'fuquas',\n 'friend',\n 'fresh',\n 'floor',\n 'fire',\n 'fight',\n 'father',\n 'extremely',\n 'escape',\n 'enjoyable',\n 'effects',\n 'drama',\n 'directors',\n 'directed',\n 'died',\n 'die',\n 'described',\n 'decided',\n 'decide',\n 'david',\n 'daughter',\n 'dark',\n 'cute',\n 'crap',\n 'count',\n 'control',\n 'comments',\n 'change',\n 'case',\n 'carry',\n 'came',\n 'cal',\n 'bourne',\n 'bored',\n 'become',\n 'basically',\n 'available',\n 'attention',\n 'atmosphere',\n 'apparently',\n 'apart',\n 'annoying',\n 'andre',\n 'amusing',\n 'already',\n 'ago',\n 'add',\n 'actress',\n 'ability',\n '2',\n 'wrote',\n 'wrong',\n 'writers',\n 'writer',\n 'worse',\n 'women',\n 'within',\n 'winner',\n 'williams',\n 'whos',\n 'went',\n 'weekly',\n 'ways',\n 'waste',\n 'unlike',\n 'understanding',\n 'tournament',\n 'tough',\n 'touching',\n 'touch',\n 'totally',\n 'tim',\n 'themes',\n 'teachers',\n 'taken',\n 'superhero',\n 'straight',\n 'store',\n 'stop',\n 'spoilers',\n 'sound',\n 'sorry',\n 'solvang',\n 'solid',\n 'smart',\n 'skilled',\n 'sit',\n 'sidney',\n 'sexual',\n 'sex',\n 'sensitive',\n 'screaming',\n 'schools',\n 'saying',\n 'saved',\n 'save',\n 'satisfying',\n 'romance',\n 'roger',\n 'robert',\n 'richard',\n 'rent',\n 'reminds',\n 'pushing',\n 'producers',\n 'problems',\n 'princess',\n 'predictable',\n 'precious',\n 'powerful',\n 'positive',\n 'popular',\n 'playing',\n 'phantom',\n 'peptides',\n 'pathetic',\n 'parts',\n 'originally',\n 'order',\n 'opinion',\n 'opening',\n 'office',\n 'nowhere',\n 'normal',\n 'none',\n 'no2',\n 'no1',\n 'nightmare',\n 'near',\n 'nature',\n 'mystery',\n 'myself',\n 'musical',\n 'moving',\n 'missouri',\n 'middle',\n 'message',\n 'merely',\n 'means',\n 'matt',\n 'magic',\n 'luise',\n 'loss',\n 'live',\n 'list',\n 'light',\n 'left',\n 'leads',\n 'law',\n 'laugh',\n 'lana',\n 'lame',\n 'ladies',\n 'lacks',\n 'kept',\n 'interested',\n 'information',\n 'incredibly',\n 'including',\n 'husband',\n 'hope',\n 'holiday',\n 'hold',\n 'heard',\n 'hasnt',\n 'happy',\n 'half',\n 'group',\n 'gorgeous',\n 'god',\n 'getting',\n 'george',\n 'general',\n 'game',\n 'friends',\n 'frank',\n 'fourth',\n 'forward',\n 'forget',\n 'force',\n 'folks',\n 'finish',\n 'final',\n 'figure',\n 'fights',\n 'feature',\n 'fart',\n 'fantastic',\n 'false',\n 'falls',\n 'fall',\n 'face',\n 'expression',\n 'explanation',\n 'expected',\n 'exactly',\n 'exact',\n 'ends',\n 'emotions',\n 'due',\n 'dixon',\n 'disturbing',\n 'disney',\n 'different',\n 'dialogue',\n 'development',\n 'detail',\n 'definitely',\n 'cut',\n 'critics',\n 'creative',\n 'copy',\n 'convincing',\n 'convey',\n 'continue',\n 'considering',\n 'closer',\n 'clearly',\n 'clarkson',\n 'christopher',\n 'children',\n 'cheap',\n 'candy',\n 'calls',\n 'brothers',\n 'bromwell',\n 'brings',\n 'box',\n 'bottom',\n 'boring',\n 'body',\n 'blood',\n 'beyond',\n 'background',\n 'attempt',\n 'artsy',\n 'approach',\n 'appear',\n 'anybody',\n 'amazing',\n 'al',\n 'air',\n 'advice',\n 'actresses',\n 'accept',\n '20',\n '1',\n 'zombi',\n 'wouldnt',\n 'works',\n 'whose',\n 'whenever',\n 'whats',\n 'whatever',\n 'welcome',\n 'weak',\n 'wasted',\n 'wanted',\n 'wait',\n 'voice',\n 'village',\n 'viewers',\n 'viewer',\n 'view',\n 'various',\n 'variety',\n 'van',\n 'value',\n 'usbr',\n 'unseen',\n 'universal',\n 'uniquely',\n 'unique',\n 'underrated',\n 'unbelievable',\n 'typically',\n 'typical',\n 'transfer',\n 'tragic',\n 'toyboy',\n 'total',\n 'tone',\n 'told',\n 'todesking',\n 'titanic',\n 'tired',\n 'threatening',\n 'thin',\n 'thank',\n 'ten',\n 'tells',\n 'teenagers',\n 'target',\n 'talking',\n 'taking',\n 'sympathy',\n 'susan',\n 'surrounded',\n 'surprises',\n 'supposed',\n 'suppose',\n 'summer',\n 'suffering',\n 'suffer',\n 'subtle',\n 'subatomic',\n 'stuff',\n 'study',\n 'student',\n 'stuck',\n 'stone',\n 'stick',\n 'steven',\n 'state',\n 'started',\n 'standard',\n 'spectacular',\n 'special',\n 'speaking',\n 'spanish',\n 'soundtrack',\n 'son',\n 'somehow',\n 'solar',\n 'society',\n 'social',\n 'slave',\n 'sky',\n 'sixties',\n 'sisters',\n 'sings',\n 'single',\n 'sing',\n 'simple',\n 'shown',\n 'shot',\n 'shame',\n 'several',\n 'sequel',\n 'sell',\n 'segal',\n 'secret',\n ...]</pre> In\u00a0[\u00a0]: Copied! <pre>demo_model = tf.keras.models.Sequential([vectorize_layer])\n\ndemo_model.predict([[\"This is a sentence.\"],\n                    [\"This is another sentence.\"]])\n</pre> demo_model = tf.keras.models.Sequential([vectorize_layer])  demo_model.predict([[\"This is a sentence.\"],                     [\"This is another sentence.\"]]) <pre>1/1 [==============================] - 2s 2s/step\n</pre> Out[\u00a0]: <pre>array([[  9,   7,   5,   1,   0,   0,   0,   0,   0,   0],\n       [  9,   7, 174,   1,   0,   0,   0,   0,   0,   0]])</pre> <p></p> In\u00a0[\u00a0]: Copied! <pre>vector_size = 5\nembedding_layer = tf.keras.layers.Embedding(max_num_words, vector_size)\n\nencodings = demo_model.predict([[\"This is a sentence.\"]])\n\nprint(encodings)\n\nembeddings = embedding_layer(tf.constant(encodings))\n\nprint(embeddings)\n\nprint(encodings.shape, embeddings.shape)\n</pre> vector_size = 5 embedding_layer = tf.keras.layers.Embedding(max_num_words, vector_size)  encodings = demo_model.predict([[\"This is a sentence.\"]])  print(encodings)  embeddings = embedding_layer(tf.constant(encodings))  print(embeddings)  print(encodings.shape, embeddings.shape) <pre>1/1 [==============================] - 0s 29ms/step\n[[9 7 5 1 0 0 0 0 0 0]]\ntf.Tensor(\n[[[-0.00929177 -0.02961112  0.01572095 -0.00546347 -0.04501405]\n  [ 0.04436645  0.02971524 -0.02868166 -0.02332792  0.03999862]\n  [-0.04126833  0.03574758 -0.02727452 -0.03438221  0.01604876]\n  [-0.02054843  0.036926   -0.04109913 -0.03033973  0.02770685]\n  [-0.03339513  0.04378723 -0.02956108 -0.02224633 -0.00406911]\n  [-0.03339513  0.04378723 -0.02956108 -0.02224633 -0.00406911]\n  [-0.03339513  0.04378723 -0.02956108 -0.02224633 -0.00406911]\n  [-0.03339513  0.04378723 -0.02956108 -0.02224633 -0.00406911]\n  [-0.03339513  0.04378723 -0.02956108 -0.02224633 -0.00406911]\n  [-0.03339513  0.04378723 -0.02956108 -0.02224633 -0.00406911]]], shape=(1, 10, 5), dtype=float32)\n(1, 10) (1, 10, 5)\n</pre> <p>We can explore a dataset of complex imbeddings at this site embed_visu .</p> <p>We can also use pre-trained models from tensorflow-hub to create embeddings for our data.</p> <p>For the first model, we only use tokenization to construct the model.</p> In\u00a0[\u00a0]: Copied! <pre>max_num_words = 32 * 10**3\nvectorize_layer = tf.keras.layers.TextVectorization(\n max_tokens=max_num_words,\n output_mode='int',\n output_sequence_length=10)\n\nvectorize_layer.adapt(text_dataset)\n\nmodel = tf.keras.models.Sequential([])\nmodel.add(vectorize_layer)\n\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n</pre> max_num_words = 32 * 10**3 vectorize_layer = tf.keras.layers.TextVectorization(  max_tokens=max_num_words,  output_mode='int',  output_sequence_length=10)  vectorize_layer.adapt(text_dataset)  model = tf.keras.models.Sequential([]) model.add(vectorize_layer)  model.add(tf.keras.layers.Dense(256, activation='relu')) model.add(tf.keras.layers.Dense(128, activation='relu')) model.add(tf.keras.layers.Dense(64, activation='relu')) model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  In\u00a0[\u00a0]: Copied! <pre>model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n</pre> model.compile(optimizer='adam',               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),               metrics=['accuracy']) In\u00a0[\u00a0]: Copied! <pre>history = model.fit(train_data.shuffle(10000).batch(512),\n                    epochs=10,\n                    validation_data=validation_data.batch(512),\n                    verbose=1)\n</pre> history = model.fit(train_data.shuffle(10000).batch(512),                     epochs=10,                     validation_data=validation_data.batch(512),                     verbose=1) <pre>Epoch 1/10\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n</pre> <pre>30/30 [==============================] - 6s 79ms/step - loss: 21.2472 - accuracy: 0.5035 - val_loss: 5.6720 - val_accuracy: 0.5039\nEpoch 2/10\n30/30 [==============================] - 2s 49ms/step - loss: 3.9529 - accuracy: 0.4993 - val_loss: 3.1013 - val_accuracy: 0.5024\nEpoch 3/10\n30/30 [==============================] - 3s 89ms/step - loss: 2.5204 - accuracy: 0.5208 - val_loss: 2.6518 - val_accuracy: 0.5077\nEpoch 4/10\n30/30 [==============================] - 2s 50ms/step - loss: 2.1447 - accuracy: 0.5248 - val_loss: 2.2500 - val_accuracy: 0.5135\nEpoch 5/10\n30/30 [==============================] - 2s 51ms/step - loss: 1.9726 - accuracy: 0.5207 - val_loss: 2.1544 - val_accuracy: 0.5062\nEpoch 6/10\n30/30 [==============================] - 2s 51ms/step - loss: 1.6089 - accuracy: 0.5340 - val_loss: 2.2512 - val_accuracy: 0.5040\nEpoch 7/10\n30/30 [==============================] - 2s 51ms/step - loss: 1.7370 - accuracy: 0.5302 - val_loss: 2.1943 - val_accuracy: 0.5019\nEpoch 8/10\n30/30 [==============================] - 2s 62ms/step - loss: 1.6039 - accuracy: 0.5445 - val_loss: 2.0347 - val_accuracy: 0.5107\nEpoch 9/10\n30/30 [==============================] - 2s 51ms/step - loss: 1.4045 - accuracy: 0.5477 - val_loss: 1.9095 - val_accuracy: 0.5039\nEpoch 10/10\n30/30 [==============================] - 2s 49ms/step - loss: 1.5053 - accuracy: 0.5453 - val_loss: 2.0738 - val_accuracy: 0.5107\n</pre> In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\nplt.plot(history.history['accuracy'])\n\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n</pre> from matplotlib import pyplot as plt plt.plot(history.history['accuracy'])  plt.plot(history.history['val_accuracy']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'val'], loc='upper left') plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.plot(history.history['loss'])\n\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()\n</pre> plt.plot(history.history['loss'])  plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'val'], loc='upper right') plt.show() <p>Now we add an embedding layer to our model.</p> In\u00a0[\u00a0]: Copied! <pre>max_num_words = 32 * 10**3\nvectorize_layer = tf.keras.layers.TextVectorization(\n max_tokens=max_num_words,\n output_mode='int',\n output_sequence_length=10)\n\nvector_size = 16\nembedding_layer = tf.keras.layers.Embedding(max_num_words, vector_size)\n\nvectorize_layer.adapt(text_dataset)\n\nmodel = tf.keras.models.Sequential([])\nmodel.add(vectorize_layer)\nmodel.add(embedding_layer)\nmodel.add(tf.keras.layers.GlobalAveragePooling1D(),)\n\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n</pre> max_num_words = 32 * 10**3 vectorize_layer = tf.keras.layers.TextVectorization(  max_tokens=max_num_words,  output_mode='int',  output_sequence_length=10)  vector_size = 16 embedding_layer = tf.keras.layers.Embedding(max_num_words, vector_size)  vectorize_layer.adapt(text_dataset)  model = tf.keras.models.Sequential([]) model.add(vectorize_layer) model.add(embedding_layer) model.add(tf.keras.layers.GlobalAveragePooling1D(),)  model.add(tf.keras.layers.Dense(256, activation='relu')) model.add(tf.keras.layers.Dense(128, activation='relu')) model.add(tf.keras.layers.Dense(64, activation='relu')) model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  In\u00a0[\u00a0]: Copied! <pre>model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n</pre> model.compile(optimizer='adam',               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),               metrics=['accuracy']) In\u00a0[\u00a0]: Copied! <pre>history = model.fit(train_data.shuffle(10000).batch(512),\n                    epochs=10,\n                    validation_data=validation_data.batch(512),\n                    verbose=1)\n</pre> history = model.fit(train_data.shuffle(10000).batch(512),                     epochs=10,                     validation_data=validation_data.batch(512),                     verbose=1) <pre>Epoch 1/10\n30/30 [==============================] - 10s 224ms/step - loss: 0.6902 - accuracy: 0.5380 - val_loss: 0.6817 - val_accuracy: 0.5790\nEpoch 2/10\n30/30 [==============================] - 6s 203ms/step - loss: 0.6398 - accuracy: 0.6440 - val_loss: 0.6383 - val_accuracy: 0.6364\nEpoch 3/10\n30/30 [==============================] - 4s 105ms/step - loss: 0.5606 - accuracy: 0.7078 - val_loss: 0.6208 - val_accuracy: 0.6532\nEpoch 4/10\n30/30 [==============================] - 3s 93ms/step - loss: 0.5203 - accuracy: 0.7348 - val_loss: 0.6346 - val_accuracy: 0.6518\nEpoch 5/10\n30/30 [==============================] - 3s 88ms/step - loss: 0.5005 - accuracy: 0.7482 - val_loss: 0.6475 - val_accuracy: 0.6528\nEpoch 6/10\n30/30 [==============================] - 4s 128ms/step - loss: 0.4894 - accuracy: 0.7507 - val_loss: 0.6617 - val_accuracy: 0.6506\nEpoch 7/10\n30/30 [==============================] - 3s 93ms/step - loss: 0.4858 - accuracy: 0.7563 - val_loss: 0.6699 - val_accuracy: 0.6465\nEpoch 8/10\n30/30 [==============================] - 3s 78ms/step - loss: 0.4756 - accuracy: 0.7599 - val_loss: 0.6864 - val_accuracy: 0.6469\nEpoch 9/10\n30/30 [==============================] - 3s 82ms/step - loss: 0.4719 - accuracy: 0.7629 - val_loss: 0.6984 - val_accuracy: 0.6415\nEpoch 10/10\n30/30 [==============================] - 4s 118ms/step - loss: 0.4670 - accuracy: 0.7647 - val_loss: 0.6989 - val_accuracy: 0.6417\n</pre> In\u00a0[\u00a0]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n text_vectorization_2 (Text  (None, 10)                0         \n Vectorization)                                                  \n                                                                 \n embedding_1 (Embedding)     (None, 10, 16)            512000    \n                                                                 \n global_average_pooling1d (  (None, 16)                0         \n GlobalAveragePooling1D)                                         \n                                                                 \n dense_4 (Dense)             (None, 256)               4352      \n                                                                 \n dense_5 (Dense)             (None, 128)               32896     \n                                                                 \n dense_6 (Dense)             (None, 64)                8256      \n                                                                 \n dense_7 (Dense)             (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 557569 (2.13 MB)\nTrainable params: 557569 (2.13 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n</pre> In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\nplt.plot(history.history['accuracy'])\n\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n</pre> from matplotlib import pyplot as plt plt.plot(history.history['accuracy'])  plt.plot(history.history['val_accuracy']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'val'], loc='upper left') plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.plot(history.history['loss'])\n\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()\n</pre> plt.plot(history.history['loss'])  plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'val'], loc='upper right') plt.show() In\u00a0[\u00a0]: Copied! <pre>embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[],\n                           dtype=tf.string, trainable=True)\n\n\nmodel = tf.keras.models.Sequential([])\nmodel.add(hub_layer)\n\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()\n</pre> embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\" hub_layer = hub.KerasLayer(embedding, input_shape=[],                            dtype=tf.string, trainable=True)   model = tf.keras.models.Sequential([]) model.add(hub_layer)  model.add(tf.keras.layers.Dense(256, activation='relu')) model.add(tf.keras.layers.Dense(128, activation='relu')) model.add(tf.keras.layers.Dense(64, activation='relu')) model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  model.summary()  <pre>Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n keras_layer (KerasLayer)    (None, 50)                48190600  \n                                                                 \n dense_8 (Dense)             (None, 256)               13056     \n                                                                 \n dense_9 (Dense)             (None, 128)               32896     \n                                                                 \n dense_10 (Dense)            (None, 64)                8256      \n                                                                 \n dense_11 (Dense)            (None, 1)                 65        \n                                                                 \n=================================================================\nTotal params: 48244873 (184.04 MB)\nTrainable params: 48244873 (184.04 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n</pre> In\u00a0[\u00a0]: Copied! <pre>model.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n</pre> model.compile(optimizer='adam',               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),               metrics=['accuracy']) In\u00a0[\u00a0]: Copied! <pre>history = model.fit(train_data.shuffle(10000).batch(512),\n                    epochs=10,\n                    validation_data=validation_data.batch(512),\n                    verbose=1)\n</pre> history = model.fit(train_data.shuffle(10000).batch(512),                     epochs=10,                     validation_data=validation_data.batch(512),                     verbose=1) <pre>Epoch 1/10\n</pre> <pre>/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n  output, from_logits = _get_logits(\n</pre> <pre>30/30 [==============================] - 9s 236ms/step - loss: 0.5568 - accuracy: 0.7300 - val_loss: 0.4629 - val_accuracy: 0.7849\nEpoch 2/10\n30/30 [==============================] - 6s 195ms/step - loss: 0.3504 - accuracy: 0.8473 - val_loss: 0.3545 - val_accuracy: 0.8508\nEpoch 3/10\n30/30 [==============================] - 6s 201ms/step - loss: 0.2080 - accuracy: 0.9231 - val_loss: 0.3448 - val_accuracy: 0.8621\nEpoch 4/10\n30/30 [==============================] - 6s 195ms/step - loss: 0.1106 - accuracy: 0.9653 - val_loss: 0.3862 - val_accuracy: 0.8645\nEpoch 5/10\n30/30 [==============================] - 6s 193ms/step - loss: 0.0471 - accuracy: 0.9887 - val_loss: 0.4697 - val_accuracy: 0.8610\nEpoch 6/10\n30/30 [==============================] - 6s 209ms/step - loss: 0.0180 - accuracy: 0.9968 - val_loss: 0.5547 - val_accuracy: 0.8628\nEpoch 7/10\n30/30 [==============================] - 6s 199ms/step - loss: 0.0065 - accuracy: 0.9993 - val_loss: 0.6277 - val_accuracy: 0.8617\nEpoch 8/10\n30/30 [==============================] - 7s 215ms/step - loss: 0.0031 - accuracy: 0.9996 - val_loss: 0.6809 - val_accuracy: 0.8601\nEpoch 9/10\n30/30 [==============================] - 6s 185ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.7248 - val_accuracy: 0.8592\nEpoch 10/10\n30/30 [==============================] - 5s 165ms/step - loss: 5.6530e-04 - accuracy: 1.0000 - val_loss: 0.7678 - val_accuracy: 0.8572\n</pre> In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\nplt.plot(history.history['accuracy'])\n\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n</pre> from matplotlib import pyplot as plt plt.plot(history.history['accuracy'])  plt.plot(history.history['val_accuracy']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'val'], loc='upper left') plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.plot(history.history['loss'])\n\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()\n</pre> plt.plot(history.history['loss'])  plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'val'], loc='upper right') plt.show() <p>RNN's (Recurrent Neural Networks) are a type of network that can take sequences as input and remember prior parts of the sequence when making predictions.</p> <p>LSTM layers are a type of recurrent layer that addresses the vanishing gradient problem and allows better memory from prior states.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[],\n                           dtype=tf.string, trainable=True)\n\n\nmodel = tf.keras.models.Sequential([])\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Reshape((50, 1)))\nmodel.add(tf.keras.layers.LSTM(16))\n\nmodel.add(tf.keras.layers.Dense(16, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()\n</pre> embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\" hub_layer = hub.KerasLayer(embedding, input_shape=[],                            dtype=tf.string, trainable=True)   model = tf.keras.models.Sequential([]) model.add(hub_layer) model.add(tf.keras.layers.Reshape((50, 1))) model.add(tf.keras.layers.LSTM(16))  model.add(tf.keras.layers.Dense(16, activation='relu')) model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  model.summary()  <pre>Model: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n keras_layer_2 (KerasLayer)  (None, 50)                48190600  \n                                                                 \n reshape_1 (Reshape)         (None, 50, 1)             0         \n                                                                 \n lstm_1 (LSTM)               (None, 16)                1152      \n                                                                 \n dense_14 (Dense)            (None, 16)                272       \n                                                                 \n dense_15 (Dense)            (None, 1)                 17        \n                                                                 \n=================================================================\nTotal params: 48192041 (183.84 MB)\nTrainable params: 48192041 (183.84 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n</pre> In\u00a0[\u00a0]: Copied! <pre>model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n</pre> model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),               metrics=['accuracy']) In\u00a0[\u00a0]: Copied! <pre>history = model.fit(train_data.shuffle(10000).batch(512),\n                    epochs=10,\n                    validation_data=validation_data.batch(512),\n                    verbose=1)\n</pre> history = model.fit(train_data.shuffle(10000).batch(512),                     epochs=10,                     validation_data=validation_data.batch(512),                     verbose=1) <pre>Epoch 1/10\n30/30 [==============================] - 9s 216ms/step - loss: 0.6928 - accuracy: 0.5043 - val_loss: 0.6916 - val_accuracy: 0.5512\nEpoch 2/10\n30/30 [==============================] - 6s 193ms/step - loss: 0.6879 - accuracy: 0.5925 - val_loss: 0.6812 - val_accuracy: 0.6426\nEpoch 3/10\n30/30 [==============================] - 6s 205ms/step - loss: 0.6622 - accuracy: 0.6781 - val_loss: 0.6318 - val_accuracy: 0.7070\nEpoch 4/10\n30/30 [==============================] - 6s 195ms/step - loss: 0.5489 - accuracy: 0.7720 - val_loss: 0.4957 - val_accuracy: 0.7904\nEpoch 5/10\n30/30 [==============================] - 6s 205ms/step - loss: 0.4011 - accuracy: 0.8538 - val_loss: 0.4377 - val_accuracy: 0.8236\nEpoch 6/10\n30/30 [==============================] - 5s 159ms/step - loss: 0.2919 - accuracy: 0.9005 - val_loss: 0.4013 - val_accuracy: 0.8293\nEpoch 7/10\n30/30 [==============================] - 6s 197ms/step - loss: 0.2149 - accuracy: 0.9330 - val_loss: 0.4094 - val_accuracy: 0.8427\nEpoch 8/10\n30/30 [==============================] - 6s 182ms/step - loss: 0.1555 - accuracy: 0.9559 - val_loss: 0.4375 - val_accuracy: 0.8429\nEpoch 9/10\n30/30 [==============================] - 6s 199ms/step - loss: 0.1173 - accuracy: 0.9711 - val_loss: 0.4656 - val_accuracy: 0.8419\nEpoch 10/10\n30/30 [==============================] - 7s 221ms/step - loss: 0.0913 - accuracy: 0.9787 - val_loss: 0.4900 - val_accuracy: 0.8416\n</pre> In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\nplt.plot(history.history['accuracy'])\n\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n</pre> from matplotlib import pyplot as plt plt.plot(history.history['accuracy'])  plt.plot(history.history['val_accuracy']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'val'], loc='upper left') plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.plot(history.history['loss'])\n\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()\n</pre> plt.plot(history.history['loss'])  plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'val'], loc='upper right') plt.show() <p>Bi-directional LSTMs allow information to propagate both forwards and backwards to improve performance.</p> In\u00a0[\u00a0]: Copied! <pre>embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\"\nhub_layer = hub.KerasLayer(embedding, input_shape=[],\n                           dtype=tf.string, trainable=True)\n\n\nmodel = tf.keras.models.Sequential([])\nmodel.add(hub_layer)\nmodel.add(tf.keras.layers.Reshape((50, 1)))\nmodel.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences = True)))\nmodel.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences = True)))\nmodel.add(tf.keras.layers.Flatten())\n\nmodel.add(tf.keras.layers.Dense(8, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()\n</pre> embedding = \"https://tfhub.dev/google/nnlm-en-dim50/2\" hub_layer = hub.KerasLayer(embedding, input_shape=[],                            dtype=tf.string, trainable=True)   model = tf.keras.models.Sequential([]) model.add(hub_layer) model.add(tf.keras.layers.Reshape((50, 1))) model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences = True))) model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16, return_sequences = True))) model.add(tf.keras.layers.Flatten())  model.add(tf.keras.layers.Dense(8, activation='relu')) model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  model.summary()  <pre>Model: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n keras_layer_3 (KerasLayer)  (None, 50)                48190600  \n                                                                 \n reshape_2 (Reshape)         (None, 50, 1)             0         \n                                                                 \n bidirectional (Bidirection  (None, 50, 64)            8704      \n al)                                                             \n                                                                 \n bidirectional_1 (Bidirecti  (None, 50, 32)            10368     \n onal)                                                           \n                                                                 \n flatten (Flatten)           (None, 1600)              0         \n                                                                 \n dense_16 (Dense)            (None, 8)                 12808     \n                                                                 \n dense_17 (Dense)            (None, 1)                 9         \n                                                                 \n=================================================================\nTotal params: 48222489 (183.95 MB)\nTrainable params: 48222489 (183.95 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n</pre> In\u00a0[\u00a0]: Copied! <pre>model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n</pre> model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),               metrics=['accuracy']) In\u00a0[\u00a0]: Copied! <pre>history = model.fit(train_data.shuffle(10000).batch(512),\n                    epochs=10,\n                    validation_data=validation_data.batch(512),\n                    verbose=1)\n</pre> history = model.fit(train_data.shuffle(10000).batch(512),                     epochs=10,                     validation_data=validation_data.batch(512),                     verbose=1) <pre>Epoch 1/10\n30/30 [==============================] - 16s 280ms/step - loss: 0.6776 - accuracy: 0.6296 - val_loss: 0.6324 - val_accuracy: 0.6945\nEpoch 2/10\n30/30 [==============================] - 6s 183ms/step - loss: 0.5371 - accuracy: 0.7417 - val_loss: 0.4750 - val_accuracy: 0.7739\nEpoch 3/10\n30/30 [==============================] - 7s 218ms/step - loss: 0.3657 - accuracy: 0.8377 - val_loss: 0.4010 - val_accuracy: 0.8190\nEpoch 4/10\n30/30 [==============================] - 8s 260ms/step - loss: 0.2527 - accuracy: 0.8991 - val_loss: 0.3866 - val_accuracy: 0.8404\nEpoch 5/10\n30/30 [==============================] - 6s 195ms/step - loss: 0.1735 - accuracy: 0.9369 - val_loss: 0.4058 - val_accuracy: 0.8455\nEpoch 6/10\n30/30 [==============================] - 5s 176ms/step - loss: 0.1111 - accuracy: 0.9641 - val_loss: 0.4692 - val_accuracy: 0.8467\nEpoch 7/10\n30/30 [==============================] - 6s 176ms/step - loss: 0.0623 - accuracy: 0.9831 - val_loss: 0.5583 - val_accuracy: 0.8444\nEpoch 8/10\n30/30 [==============================] - 5s 159ms/step - loss: 0.0345 - accuracy: 0.9923 - val_loss: 0.6550 - val_accuracy: 0.8458\nEpoch 9/10\n30/30 [==============================] - 5s 175ms/step - loss: 0.0181 - accuracy: 0.9971 - val_loss: 0.7235 - val_accuracy: 0.8436\nEpoch 10/10\n30/30 [==============================] - 5s 170ms/step - loss: 0.0116 - accuracy: 0.9983 - val_loss: 0.7968 - val_accuracy: 0.8418\n</pre> In\u00a0[\u00a0]: Copied! <pre>from matplotlib import pyplot as plt\nplt.plot(history.history['accuracy'])\n\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()\n</pre> from matplotlib import pyplot as plt plt.plot(history.history['accuracy'])  plt.plot(history.history['val_accuracy']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'val'], loc='upper left') plt.show() In\u00a0[\u00a0]: Copied! <pre>plt.plot(history.history['loss'])\n\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()\n</pre> plt.plot(history.history['loss'])  plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'val'], loc='upper right') plt.show() In\u00a0[\u00a0]: Copied! <pre>max_num_words = 32 * 10**3\nvectorize_layer = tf.keras.layers.TextVectorization(\n max_tokens=max_num_words,\n output_mode='int',\n output_sequence_length=10)\n\nvector_size = 50\nembedding_layer = tf.keras.layers.Embedding(max_num_words, vector_size)\n\nvectorize_layer.adapt(text_dataset)\n\nmodel = tf.keras.models.Sequential([])\nmodel.add(vectorize_layer)\nmodel.add(embedding_layer)\nmodel.add(tf.keras.layers.LSTM(16))\n\nmodel.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel.add(tf.keras.layers.Dense(64, activation='relu'))\nmodel.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n</pre> max_num_words = 32 * 10**3 vectorize_layer = tf.keras.layers.TextVectorization(  max_tokens=max_num_words,  output_mode='int',  output_sequence_length=10)  vector_size = 50 embedding_layer = tf.keras.layers.Embedding(max_num_words, vector_size)  vectorize_layer.adapt(text_dataset)  model = tf.keras.models.Sequential([]) model.add(vectorize_layer) model.add(embedding_layer) model.add(tf.keras.layers.LSTM(16))  model.add(tf.keras.layers.Dense(256, activation='relu')) model.add(tf.keras.layers.Dense(128, activation='relu')) model.add(tf.keras.layers.Dense(64, activation='relu')) model.add(tf.keras.layers.Dense(1, activation='sigmoid'))  In\u00a0[\u00a0]: Copied! <pre>model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n</pre> model.compile(optimizer=tf.keras.optimizers.Adam(0.0005),               loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),               metrics=['accuracy']) In\u00a0[\u00a0]: Copied! <pre>history = model.fit(train_data.shuffle(10000).batch(512),\n                    epochs=10,\n                    validation_data=validation_data.batch(512),\n                    verbose=1)\n</pre> history = model.fit(train_data.shuffle(10000).batch(512),                     epochs=10,                     validation_data=validation_data.batch(512),                     verbose=1) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"ML%20Essentials%20-%20NLP/#natural-language-processing","title":"Natural Language Processing\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#natural-language-processing","title":"Natural Language Processing\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#1-text-pre-processing","title":"1. Text Pre-processing\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#11-loading-the-dataset","title":"1.1 Loading the Dataset\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#12-tokenization","title":"1.2 Tokenization\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#13-vectorization","title":"1.3 Vectorization\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#14-word-embeddings","title":"1.4 Word Embeddings\u00b6","text":"<p>Rather than simply converting the words into integers each word is converted into a vector of floats that can better represent the meanings and relations between different words.</p>"},{"location":"ML%20Essentials%20-%20NLP/#2-training-a-model","title":"2. Training a Model\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#21-model-with-tokenization","title":"2.1 Model with Tokenization\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#22-model-with-tokenization-and-embeddings","title":"2.2 Model with Tokenization and Embeddings\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#23-model-with-pre-trained-embeddings","title":"2.3 Model with Pre-trained Embeddings\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#24-rnns-with-the-lstm-layer","title":"2.4 RNN's with the LSTM Layer\u00b6","text":""},{"location":"ML%20Essentials%20-%20NLP/#25-bi-directional-lstms","title":"2.5 Bi-directional LSTM's\u00b6","text":""},{"location":"ML%20Essentials-%20Computer%20vision/","title":"Computer Vision &amp; Image Processing","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np #Python library used for working with arrays\nimport matplotlib.pyplot as plt #Library for interactive vizualizations in python\nfrom PIL import Image #Python Imaging Library\n</pre> import numpy as np #Python library used for working with arrays import matplotlib.pyplot as plt #Library for interactive vizualizations in python from PIL import Image #Python Imaging Library In\u00a0[\u00a0]: Copied! <pre>pic = Image.open('sample_data/codechamp.png')\npic\n</pre> pic = Image.open('sample_data/codechamp.png') pic Out[\u00a0]: In\u00a0[\u00a0]: Copied! <pre>type(pic)\n</pre> type(pic) Out[\u00a0]: <pre>PIL.PngImagePlugin.PngImageFiledef __init__(fp=None, filename=None)</pre><pre>/usr/local/lib/python3.10/dist-packages/PIL/PngImagePlugin.pyBase class for image file format handlers.</pre> In\u00a0[\u00a0]: Copied! <pre>pic_arr = np.asarray(pic)\nprint(pic_arr)\ntype(pic_arr)\n</pre> pic_arr = np.asarray(pic) print(pic_arr) type(pic_arr) <pre>[[[ 14  14  14 255]\n  [ 14  14  14 255]\n  [ 14  14  14 255]\n  ...\n  [ 25  57  80 255]\n  [ 25  57  80 255]\n  [ 14  14  14 255]]\n\n [[ 14  14  14 255]\n  [ 14  14  14 255]\n  [ 14  14  14 255]\n  ...\n  [ 25  57  80 255]\n  [ 25  57  80 255]\n  [ 14  14  14 255]]\n\n [[ 14  14  14 255]\n  [ 14  14  14 255]\n  [ 14  14  14 255]\n  ...\n  [ 25  57  80 255]\n  [ 25  57  80 255]\n  [ 14  14  14 255]]\n\n ...\n\n [[ 14  14  14 255]\n  [ 14  14  14 255]\n  [ 14  14  14 255]\n  ...\n  [ 14  14  14 255]\n  [ 14  14  14 255]\n  [ 14  14  14 255]]\n\n [[ 14  14  14 255]\n  [ 14  14  14 255]\n  [ 14  14  14 255]\n  ...\n  [ 14  14  14 255]\n  [ 14  14  14 255]\n  [ 14  14  14 255]]\n\n [[ 14  14  14 255]\n  [ 14  14  14 255]\n  [ 14  14  14 255]\n  ...\n  [ 14  14  14 255]\n  [ 14  14  14 255]\n  [ 14  14  14 255]]]\n</pre> Out[\u00a0]: <pre>numpy.ndarray</pre> <p>Dimensions of the picture</p> In\u00a0[\u00a0]: Copied! <pre>pic_arr.shape\n</pre> pic_arr.shape Out[\u00a0]: <pre>(1276, 1272, 4)</pre> <p>Graph plotting of the picture</p> In\u00a0[\u00a0]: Copied! <pre>plt.imshow(pic_arr)\n</pre> plt.imshow(pic_arr) Out[\u00a0]: <pre>&lt;matplotlib.image.AxesImage at 0x7fa031a5b430&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>import cv2\nimg1 = cv2.imread('sample_data/dog_backpack.png')\nimg1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\nimg2 = cv2.imread('sample_data/do_not_copy.png')\nimg21= cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n\nplt.imshow(img1)\nimg1.shape\n</pre> import cv2 img1 = cv2.imread('sample_data/dog_backpack.png') img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) img2 = cv2.imread('sample_data/do_not_copy.png') img21= cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)  plt.imshow(img1) img1.shape Out[\u00a0]: <pre>(901, 1198, 3)</pre> In\u00a0[\u00a0]: Copied! <pre>plt.imshow(img2)\nimg2.shape\n</pre> plt.imshow(img2) img2.shape Out[\u00a0]: <pre>(1081, 1077, 3)</pre> <p>Resizing images</p> In\u00a0[\u00a0]: Copied! <pre>img1=cv2.resize(img1,(1200,1200))\nimg2=cv2.resize(img2,(1200,1200))\n</pre> img1=cv2.resize(img1,(1200,1200)) img2=cv2.resize(img2,(1200,1200)) In\u00a0[\u00a0]: Copied! <pre>blended = cv2.addWeighted(src1=img1, alpha=0.9, src2=img2, beta=0.3, gamma=0)\nplt.imshow(blended)\n</pre> blended = cv2.addWeighted(src1=img1, alpha=0.9, src2=img2, beta=0.3, gamma=0) plt.imshow(blended) Out[\u00a0]: <pre>&lt;matplotlib.image.AxesImage at 0x7fa031a3b5e0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Importing Image and ImageFilter module from PIL package\nfrom PIL import Image, ImageFilter\n\n# creating a image object\nim1 = Image.open(\"sample_data/leave.jpg\")\n\n# applying the Gaussian Blur filter\nim2 = im1.filter(ImageFilter.GaussianBlur(radius = 2))\n\nplt.imshow(im2)\n</pre> # Importing Image and ImageFilter module from PIL package from PIL import Image, ImageFilter  # creating a image object im1 = Image.open(\"sample_data/leave.jpg\")  # applying the Gaussian Blur filter im2 = im1.filter(ImageFilter.GaussianBlur(radius = 2))  plt.imshow(im2) In\u00a0[\u00a0]: Copied! <pre>!pip install tensorflow_datasets\n</pre> !pip install tensorflow_datasets <pre>Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.10/dist-packages (4.9.4)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.4.0)\nRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (8.1.7)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.1.8)\nRequirement already satisfied: etils[enp,epath,etree]&gt;=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.7.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.25.2)\nRequirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.3)\nRequirement already satisfied: protobuf&gt;=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (3.20.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (5.9.5)\nRequirement already satisfied: requests&gt;=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.31.0)\nRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.14.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.4.0)\nRequirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.10.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (4.66.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.14.1)\nRequirement already satisfied: array-record&gt;=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.5.0)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow_datasets) (2023.6.0)\nRequirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow_datasets) (6.1.1)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow_datasets) (4.9.0)\nRequirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,etree]&gt;=0.9.0-&gt;tensorflow_datasets) (3.17.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (3.6)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2.0.7)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests&gt;=2.19.0-&gt;tensorflow_datasets) (2024.2.2)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise-&gt;tensorflow_datasets) (1.16.0)\nRequirement already satisfied: googleapis-common-protos&lt;2,&gt;=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata-&gt;tensorflow_datasets) (1.62.0)\n</pre> In\u00a0[\u00a0]: Copied! <pre>import tensorflow as tf\nimport tensorflow_datasets as tfds\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n</pre> import tensorflow as tf import tensorflow_datasets as tfds import os  import pandas as pd import numpy as np import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>dataset, info = tfds.load('cats_vs_dogs', with_info=True, as_supervised=True)\n</pre> dataset, info = tfds.load('cats_vs_dogs', with_info=True, as_supervised=True) <pre>Downloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\n</pre> <pre>Dl Completed...: 0 url [00:00, ? url/s]</pre> <pre>Dl Size...: 0 MiB [00:00, ? MiB/s]</pre> <pre>Generating splits...:   0%|          | 0/1 [00:00&lt;?, ? splits/s]</pre> <pre>Generating train examples...:   0%|          | 0/23262 [00:00&lt;?, ? examples/s]</pre> <pre>WARNING:absl:1738 images were corrupted and were skipped\n</pre> <pre>Shuffling /root/tensorflow_datasets/cats_vs_dogs/4.0.1.incompleteTM33FF/cats_vs_dogs-train.tfrecord*...:   0%|\u2026</pre> <pre>Dataset cats_vs_dogs downloaded and prepared to /root/tensorflow_datasets/cats_vs_dogs/4.0.1. Subsequent calls will reuse this data.\n</pre> In\u00a0[\u00a0]: Copied! <pre>class_names = info.features['label'].names\nclass_names\n</pre> class_names = info.features['label'].names class_names Out[\u00a0]: <pre>['cat', 'dog']</pre> In\u00a0[\u00a0]: Copied! <pre>for i, example in enumerate(dataset['train']):\n  # example = (image, label)\n  image, label = example\n  save_dir = './cats_vs_dogs/train/{}'.format(class_names[label])\n  os.makedirs(save_dir, exist_ok=True)\n\n  filename = save_dir + \"/\" + \"{}_{}.jpg\".format(class_names[label], i)\n  tf.keras.preprocessing.image.save_img(filename, image.numpy())\n</pre> for i, example in enumerate(dataset['train']):   # example = (image, label)   image, label = example   save_dir = './cats_vs_dogs/train/{}'.format(class_names[label])   os.makedirs(save_dir, exist_ok=True)    filename = save_dir + \"/\" + \"{}_{}.jpg\".format(class_names[label], i)   tf.keras.preprocessing.image.save_img(filename, image.numpy()) <pre>\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\n&lt;ipython-input-4-3103315d82e1&gt; in &lt;cell line: 1&gt;()\n      2   # example = (image, label)\n      3   image, label = example\n----&gt; 4   save_dir = './cats_vs_dogs/train/{}'.format(class_names[label])\n      5   os.makedirs(save_dir, exist_ok=True)\n      6 \n\nNameError: name 'class_names' is not defined</pre> In\u00a0[\u00a0]: Copied! <pre>from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Sequential\n</pre> from tensorflow.keras.preprocessing.image import ImageDataGenerator from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization from tensorflow.keras.models import Sequential <p></p> In\u00a0[\u00a0]: Copied! <pre>datagen = ImageDataGenerator(rescale=1/255, validation_split=0.2, rotation_range=10,\n                              width_shift_range=0.1, height_shift_range=0.1,\n                             shear_range=0.1, zoom_range=0.10, horizontal_flip=True)\n\ntrain_generator = datagen.flow_from_directory('/content/cats_vs_dogs/train',\n                                              target_size = (150, 150),\n                                              batch_size=32,\n                                              class_mode='binary',\n                                              subset='training')\n\nvalidation_generator = datagen.flow_from_directory('/content/cats_vs_dogs/train',\n                                              target_size = (150, 150),\n                                              batch_size=32,\n                                              class_mode='binary',\n                                              subset='validation')\n</pre> datagen = ImageDataGenerator(rescale=1/255, validation_split=0.2, rotation_range=10,                               width_shift_range=0.1, height_shift_range=0.1,                              shear_range=0.1, zoom_range=0.10, horizontal_flip=True)  train_generator = datagen.flow_from_directory('/content/cats_vs_dogs/train',                                               target_size = (150, 150),                                               batch_size=32,                                               class_mode='binary',                                               subset='training')  validation_generator = datagen.flow_from_directory('/content/cats_vs_dogs/train',                                               target_size = (150, 150),                                               batch_size=32,                                               class_mode='binary',                                               subset='validation') <p></p> <ul> <li>CNN Building Blocks<ul> <li>Input Layer</li> <li>Convolutional Layer</li> <li>Pooling Layer</li> <li>Dropout Layer</li> <li>Batch Normalization Layer</li> <li>Activation Layer</li> <li>Fully Connected Layer</li> <li>Flatten Layer</li> <li>Output Layer</li> </ul> </li> </ul> In\u00a0[\u00a0]: Copied! <pre>from keras.backend import batch_normalization\nmodel = Sequential()\n\n# 1st layer CNN\nmodel.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(150, 150, 3)))\nmodel.add(MaxPooling2D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n# 2nd layer CNN\nmodel.add(Conv2D(64, kernel_size=3, activation='relu'))\nmodel.add(MaxPooling2D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\n# 3rd Layer\nmodel.add(Conv2D(128, kernel_size=3, activation='relu'))\nmodel.add(MaxPooling2D(2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.2))\n\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n</pre> from keras.backend import batch_normalization model = Sequential()  # 1st layer CNN model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(150, 150, 3))) model.add(MaxPooling2D(2)) model.add(BatchNormalization()) model.add(Dropout(0.2))  # 2nd layer CNN model.add(Conv2D(64, kernel_size=3, activation='relu')) model.add(MaxPooling2D(2)) model.add(BatchNormalization()) model.add(Dropout(0.2))  # 3rd Layer model.add(Conv2D(128, kernel_size=3, activation='relu')) model.add(MaxPooling2D(2)) model.add(BatchNormalization()) model.add(Dropout(0.2))  model.add(Flatten()) model.add(Dropout(0.5)) model.add(Dense(512, activation='relu')) model.add(Dense(1, activation='sigmoid'))    In\u00a0[\u00a0]: Copied! <pre>model.summary()\n</pre> model.summary() <pre>Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d (Conv2D)             (None, 148, 148, 32)      896       \n                                                                 \n max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n D)                                                              \n                                                                 \n conv2d_1 (Conv2D)           (None, 72, 72, 64)        18496     \n                                                                 \n max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0         \n g2D)                                                            \n                                                                 \n conv2d_2 (Conv2D)           (None, 34, 34, 128)       73856     \n                                                                 \n max_pooling2d_2 (MaxPoolin  (None, 17, 17, 128)       0         \n g2D)                                                            \n                                                                 \n flatten (Flatten)           (None, 36992)             0         \n                                                                 \n dropout (Dropout)           (None, 36992)             0         \n                                                                 \n dense (Dense)               (None, 512)               18940416  \n                                                                 \n dense_1 (Dense)             (None, 1)                 513       \n                                                                 \n=================================================================\nTotal params: 19034177 (72.61 MB)\nTrainable params: 19034177 (72.61 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n</pre> In\u00a0[\u00a0]: Copied! <pre>model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nhistory = model.fit(train_generator, epochs=10, validation_data=validation_generator)\n</pre> model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  history = model.fit(train_generator, epochs=10, validation_data=validation_generator) <pre>Epoch 1/10\n582/582 [==============================] - 1207s 2s/step - loss: 0.6556 - accuracy: 0.6181 - val_loss: 0.6249 - val_accuracy: 0.6237\nEpoch 2/10\n582/582 [==============================] - 1245s 2s/step - loss: 0.5548 - accuracy: 0.7135 - val_loss: 0.5331 - val_accuracy: 0.7334\nEpoch 3/10\n582/582 [==============================] - 1242s 2s/step - loss: 0.5061 - accuracy: 0.7513 - val_loss: 0.4657 - val_accuracy: 0.7790\nEpoch 4/10\n582/582 [==============================] - 1226s 2s/step - loss: 0.4628 - accuracy: 0.7807 - val_loss: 0.4379 - val_accuracy: 0.7899\nEpoch 5/10\n582/582 [==============================] - 1224s 2s/step - loss: 0.4440 - accuracy: 0.7913 - val_loss: 0.4302 - val_accuracy: 0.8033\nEpoch 6/10\n582/582 [==============================] - 1236s 2s/step - loss: 0.4205 - accuracy: 0.8079 - val_loss: 0.4180 - val_accuracy: 0.8052\nEpoch 7/10\n582/582 [==============================] - 1238s 2s/step - loss: 0.4022 - accuracy: 0.8170 - val_loss: 0.3939 - val_accuracy: 0.8237\nEpoch 8/10\n582/582 [==============================] - 1231s 2s/step - loss: 0.3840 - accuracy: 0.8245 - val_loss: 0.3596 - val_accuracy: 0.8381\nEpoch 9/10\n582/582 [==============================] - 1237s 2s/step - loss: 0.3755 - accuracy: 0.8293 - val_loss: 0.3485 - val_accuracy: 0.8557\nEpoch 10/10\n582/582 [==============================] - 1239s 2s/step - loss: 0.3590 - accuracy: 0.8400 - val_loss: 0.3366 - val_accuracy: 0.8527\n</pre> In\u00a0[\u00a0]: Copied! <pre>history.history\n\nplt.plot(history.history['accuracy'], label='Training')\nplt.plot(history.history['val_accuracy'], label='Validation')\nplt.legend(['Training', 'Validation'])\n</pre> history.history  plt.plot(history.history['accuracy'], label='Training') plt.plot(history.history['val_accuracy'], label='Validation') plt.legend(['Training', 'Validation'])  Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x7839e80c70a0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>model.save('cats_vs_dogs.h5')\n</pre> model.save('cats_vs_dogs.h5') <pre>/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n  saving_api.save_model(\n</pre> In\u00a0[\u00a0]: Copied! <pre>model_load = tf.keras.models.load_model('cats_vs_dogs.h5')\n</pre> model_load = tf.keras.models.load_model('cats_vs_dogs.h5') In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom PIL import Image\nfrom tensorflow.keras.preprocessing import image\n#testing our model\nimg_url = \"https://i.natgeofe.com/n/548467d8-c5f1-4551-9f58-6817a8d2c45e/NationalGeographic_2572187_square.jpg\"\nimg = Image.open(requests.get(img_url, stream=True).raw).resize((150, 150))\n\nimage_array = image.img_to_array(img)\n\nimg = np.expand_dims(image_array, axis=0)\n\nimg = img/255\n\nprediction = model.predict(img)\n\nTH = 0.5\nprediction = int(prediction[0][0]&gt;TH)\nclasses = {v:k for k,v in train_generator.class_indices.items()}\nclasses[prediction]\n</pre> import requests from PIL import Image from tensorflow.keras.preprocessing import image #testing our model img_url = \"https://i.natgeofe.com/n/548467d8-c5f1-4551-9f58-6817a8d2c45e/NationalGeographic_2572187_square.jpg\" img = Image.open(requests.get(img_url, stream=True).raw).resize((150, 150))  image_array = image.img_to_array(img)  img = np.expand_dims(image_array, axis=0)  img = img/255  prediction = model.predict(img)  TH = 0.5 prediction = int(prediction[0][0]&gt;TH) classes = {v:k for k,v in train_generator.class_indices.items()} classes[prediction] <pre>1/1 [==============================] - 0s 201ms/step\n</pre> Out[\u00a0]: <pre>'cat'</pre>"},{"location":"ML%20Essentials-%20Computer%20vision/#computer-vision-image-processing","title":"Computer Vision &amp; Image Processing\u00b6","text":""},{"location":"ML%20Essentials-%20Computer%20vision/#how-does-computer-vision-work","title":"How does Computer Vision work\u00b6","text":""},{"location":"ML%20Essentials-%20Computer%20vision/#blending-images-of-the-same-size","title":"BLENDING IMAGES OF THE SAME SIZE\u00b6","text":""},{"location":"ML%20Essentials-%20Computer%20vision/#blurring","title":"Blurring\u00b6","text":"<p>Gaussian Blur: Applies a Gaussian filter for smoother blurring. Preserves edges better than averaging blur.</p>"},{"location":"ML%20Essentials-%20Computer%20vision/#computer-vision-challenges","title":"Computer Vision Challenges\u00b6","text":"<p>Challenges to becoming a leading technology:</p> <p>o Reasoning and analytical issues: To become a computer vision expert, you must have strong reasoning and analytical skills.</p> <p>o Privacy and security: Vision-powered surveillance is also having various serious privacy issues for lots of countries. It restricts users from accessing unauthorized content. Further, various countries also avoid such face recognition and detection techniques for privacy and security reasons.</p> <p>o Duplicate and false content:  A data breach can lead to serious problems, such as creating duplicate images and videos over the internet.</p>"},{"location":"ML%20Essentials-%20Computer%20vision/#deep-learning","title":"<code>DEEP LEARNING</code>\u00b6","text":""},{"location":"ML%20Essentials-%20Computer%20vision/#dataset-details","title":"Dataset Details\u00b6","text":""},{"location":"ML%20Essentials-%20Computer%20vision/#dogs-and-cats-dataset","title":"Dogs and Cats Dataset\u00b6","text":"<p>https://www.tensorflow.org/datasets/catalog/cats_vs_dogs</p> <p></p> <ul> <li>The Dogs and Cats dataset contains around 23k images of dogs and cats.</li> <li>We will be downloading the dataset from the TensorFlow Datasets library.</li> <li>The dataset is already split into training and validation sets, so we don't need to split it manually.</li> <li>The dataset is also already preprocessed and all images are of the same size (256 x 256 x 3).</li> <li>For better understanding of the dataset, I would be downloading the dataset and then visualizing it.</li> </ul>"},{"location":"ML%20Essentials-%20Computer%20vision/#what-is-overfitting","title":"What is Overfitting?\u00b6","text":"<p> Overfitting is a common problem in deep learning where the model becomes too complex and starts to fit the training data too well, but fails to generalize well to new, unseen data. Here are some characteristics of overfitting in deep learning:</p> <ul> <li>High training accuracy: the model achieves very high accuracy on the training set.</li> <li>Low validation accuracy: the model has low accuracy on the validation set, which indicates that it is not generalizing well to new data.</li> <li>Large gap between training and validation accuracy: there is a large difference between the training accuracy and the validation accuracy, which indicates that the model is memorizing the training data rather than learning the underlying patterns.</li> <li>Poor performance on test data: when tested on new, unseen data, the model performs poorly, indicating that it is not able to generalize well beyond the training data.</li> </ul>"},{"location":"ML%20Essentials-%20Computer%20vision/#what-is-regularization","title":"What is Regularization?\u00b6","text":"<ul> <li>Regularization is a technique used in deep learning to prevent overfitting of the model. Overfitting occurs when the model fits too well to the training data and fails to generalize well to new, unseen data.</li> <li>Regularization adds a penalty term to the loss function of the model, which encourages the model to learn simpler, smoother decision boundaries rather than complex, jagged ones. This helps the model to avoid overfitting and generalize better to new data.</li> </ul>"},{"location":"ML%20Essentials-%20Computer%20vision/#types-of-regularization","title":"Types of Regularization\u00b6","text":"<ul> <li><p>L1 and L2 Regularization: add a penalty term to the loss function that encourages the model to learn smaller weights</p> </li> <li><p>Dropout: randomly drops out some of the neurons during training to prevent co-adaptation of neurons and encourage the model to learn more robust features</p> <p>For more information about Dropout, please check here https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout</p> </li> <li><p>Early Stopping: stops the training process when the validation loss stops improving to prevent overfitting</p> </li> <li><p>Data Augmentation: artificially increases the size of the training set by applying random transformations to the data to improve the generalization ability of the model</p> </li> <li><p>Batch Normalization: normalizes the input data to each layer of the network, which helps to prevent overfitting and improve the training speed</p> </li> <li><p>Image augmentation: a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset.</p> <p>For more information about ImageDataGenerator, please check the official documentation: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator</p> </li> </ul>"},{"location":"ML%20Essentials-%20Computer%20vision/#building-convolutions-neural-network-cnn-model-with-image-augmentation","title":"Building Convolutions Neural Network (CNN) Model with Image Augmentation\u00b6","text":"<p>https://poloclub.github.io/cnn-explainer/</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/","title":"EDA with Tabular Datasets","text":"In\u00a0[\u00a0]: Copied! <pre># The script below is to download the required dataset from github\n\n!git clone https://github.com/acmbpdc/ML-Bootcamp-2024.git\n</pre> # The script below is to download the required dataset from github  !git clone https://github.com/acmbpdc/ML-Bootcamp-2024.git <pre>fatal: destination path 'ML-Bootcamp-2024' already exists and is not an empty directory.\n</pre> <p>Copying the files into /content for easy access</p> In\u00a0[\u00a0]: Copied! <pre>!cp \"/content/ML-Bootcamp-2024/Session 1/medical_ds/Job_Placement_Data.csv\" /content\n\n!cp \"/content/ML-Bootcamp-2024/Session 1/medical_ds/diabetes.csv\" /content\n</pre> !cp \"/content/ML-Bootcamp-2024/Session 1/medical_ds/Job_Placement_Data.csv\" /content  !cp \"/content/ML-Bootcamp-2024/Session 1/medical_ds/diabetes.csv\" /content <p>Let's create a predictive model that can predict whether or not a person can develop Diabetes. To do this, we will use patient history data available in the dataset below. If the definition is not well done, it will compromise our work.</p> <p>From this definition, we started our work of dataset collection, the transformation of variables and the division of Training and Test data.</p> <p>Dataset: Pima Indians Diabetes Data Set http://archive.ics.uci.edu/ml/datasets/diabetes</p> <p>This dataset describes the medical records among Pima Inidians patients and each record is marked whether or not the patient developed diabetes. It contains multivariate data and can be used for time series.</p> <p>There are several considerations when uploading data to the Machine Learning process. For example, does your data have a header? If not, you will need to set the title for each column. Do your files have comments? What is the delimiter of the columns? Are some data in quotation marks, single or double?</p> In\u00a0[\u00a0]: Copied! <pre># Loading csv file using Pandas\n# Pandas Library Provides Data Structure for Data Storage\nimport pandas as pd\nfile = '/content/diabetes.csv'\ncolumns = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n# I already know that the file has no header and Fill on import\ndata = pd.read_csv(file, names=columns, skiprows=1)\nprint(data.shape)\n</pre> # Loading csv file using Pandas # Pandas Library Provides Data Structure for Data Storage import pandas as pd file = '/content/diabetes.csv' columns = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] # I already know that the file has no header and Fill on import data = pd.read_csv(file, names=columns, skiprows=1) print(data.shape) <pre>(768, 9)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Viewing the first 20 lines\ndata.head(20)\n</pre> # Viewing the first 20 lines data.head(20) Out[\u00a0]: preg plas pres skin test mass pedi age class 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 2 8 183 64 0 0 23.3 0.672 32 1 3 1 89 66 23 94 28.1 0.167 21 0 4 0 137 40 35 168 43.1 2.288 33 1 5 5 116 74 0 0 25.6 0.201 30 0 6 3 78 50 32 88 31.0 0.248 26 1 7 10 115 0 0 0 35.3 0.134 29 0 8 2 197 70 45 543 30.5 0.158 53 1 9 8 125 96 0 0 0.0 0.232 54 1 10 4 110 92 0 0 37.6 0.191 30 0 11 10 168 74 0 0 38.0 0.537 34 1 12 10 139 80 0 0 27.1 1.441 57 0 13 1 189 60 23 846 30.1 0.398 59 1 14 5 166 72 19 175 25.8 0.587 51 1 15 7 100 0 0 0 30.0 0.484 32 1 16 0 118 84 47 230 45.8 0.551 31 1 17 7 107 74 0 0 29.6 0.254 31 1 18 1 103 30 38 83 43.3 0.183 33 0 19 1 115 70 30 96 34.6 0.529 32 1 <p>If the number of lines in the file is vast, the algorithm can take a long time to be trained, and If the number of records is too small, we may not have enough records to train the model. It is reasonable to have up to one million records for the machine to process without difficulty. Above that, the record numbers will demand computationally from the engine.</p> <p>If we have many columns in the file, the algorithm can present performance problems due to the high dimensionality. In this case, we can apply the dimensionality reduction if necessary. The best solution will depend on each situation. Remember: train the model in a subset of the more extensive Dataset and then apply it to new data. That is, it is not necessary to use the entire Dataset. It is enough to use a representative subset to train the algorithm.</p> In\u00a0[\u00a0]: Copied! <pre># Viewing dimensions\ndata.shape\n</pre> # Viewing dimensions data.shape Out[\u00a0]: <pre>(768, 9)</pre> <p>The type of data is essential. It may be necessary to convert strings, or columns with integers may represent categorical variables or ordinary values.</p> In\u00a0[\u00a0]: Copied! <pre># Statistical summary\ndata.describe()\n</pre> # Statistical summary data.describe() Out[\u00a0]: preg plas pres skin test mass pedi age class count 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 768.000000 mean 3.845052 120.894531 69.105469 20.536458 79.799479 31.992578 0.471876 33.240885 0.348958 std 3.369578 31.972618 19.355807 15.952218 115.244002 7.884160 0.331329 11.760232 0.476951 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000 25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243750 24.000000 0.000000 50% 3.000000 117.000000 72.000000 23.000000 30.500000 32.000000 0.372500 29.000000 0.000000 75% 6.000000 140.250000 80.000000 32.000000 127.250000 36.600000 0.626250 41.000000 1.000000 max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000 <p>In classification problems, it may be necessary to balance the classes. Unbalanced classes (more significant volumes of one of the class types) are common and need to be addressed during the pre-processing phase.</p> <p>We can see below that there is an evident disproportion between classes 0 (non-occurrence of diabetes) and 1 (occurrence of diabetes). We apply the class variable to the Pandas groupby function and measure its size:</p> In\u00a0[\u00a0]: Copied! <pre># Distribution of classes\ndata.groupby('class').size()\n</pre> # Distribution of classes data.groupby('class').size() Out[\u00a0]: <pre>class\n0    500\n1    268\ndtype: int64</pre> <p>We have 500 records with class 0 and 268 class 1 records, meaning we have more records of people who have not developed Diabetes than people who have developed. When the algorithm goes through these numbers, it will end up learning more about a person who does not have Diabetes than a person who has Diabetes\u200a-\u200awe need to have the same proportion for the two classes of the set.</p> <p>One way to understand the relationship between data is through correlation. The Correlation is the relationship between 2 variables. The most common method for calculating correlation is the Pearson method, which assumes a normal distribution of data. A correlation of -1 shows a negative correlation, while A correlation of +1 shows a positive correlation. A correlation of 0 indicates that there is no relationship between the variables.</p> <p>Some algorithms such as a linear regression and logistic regression can present performance problems with highly correlated (collinear) attributes. How do I know if the algorithm requires modification? Consult the algorithm's documentation to find out what the algorithm specifically needs</p> In\u00a0[\u00a0]: Copied! <pre># Apply Pearson's correlation between all variables in the dataset\ndata.corr(method = 'pearson', numeric_only=True)\n</pre> # Apply Pearson's correlation between all variables in the dataset data.corr(method = 'pearson', numeric_only=True) Out[\u00a0]: preg plas pres skin test mass pedi age class preg 1.000000 0.129459 0.141282 -0.081672 -0.073535 0.017683 -0.033523 0.544341 0.221898 plas 0.129459 1.000000 0.152590 0.057328 0.331357 0.221071 0.137337 0.263514 0.466581 pres 0.141282 0.152590 1.000000 0.207371 0.088933 0.281805 0.041265 0.239528 0.065068 skin -0.081672 0.057328 0.207371 1.000000 0.436783 0.392573 0.183928 -0.113970 0.074752 test -0.073535 0.331357 0.088933 0.436783 1.000000 0.197859 0.185071 -0.042163 0.130548 mass 0.017683 0.221071 0.281805 0.392573 0.197859 1.000000 0.140647 0.036242 0.292695 pedi -0.033523 0.137337 0.041265 0.183928 0.185071 0.140647 1.000000 0.033561 0.173844 age 0.544341 0.263514 0.239528 -0.113970 -0.042163 0.036242 0.033561 1.000000 0.238356 class 0.221898 0.466581 0.065068 0.074752 0.130548 0.292695 0.173844 0.238356 1.000000 In\u00a0[\u00a0]: Copied! <pre>data = data.drop(0)\n\nfor col in data:\n  data[col] = data[col].astype(float)\n\ndata.corr(method = 'pearson', numeric_only=True)\n</pre> data = data.drop(0)  for col in data:   data[col] = data[col].astype(float)  data.corr(method = 'pearson', numeric_only=True) Out[\u00a0]: preg plas pres skin test mass pedi age class preg 1.000000 0.128846 0.141197 -0.082495 -0.072999 0.017518 -0.033927 0.544018 0.221087 plas 0.128846 1.000000 0.152498 0.056381 0.332383 0.220955 0.136903 0.262408 0.465856 pres 0.141197 0.152498 1.000000 0.207308 0.089098 0.281777 0.041180 0.239571 0.064882 skin -0.082495 0.056381 0.207308 1.000000 0.437974 0.392553 0.183498 -0.115873 0.073265 test -0.072999 0.332383 0.089098 0.437974 1.000000 0.198111 0.185579 -0.040942 0.131984 mass 0.017518 0.220955 0.281777 0.392553 0.198111 1.000000 0.140546 0.035911 0.292695 pedi -0.033927 0.136903 0.041180 0.183498 0.185579 0.140546 1.000000 0.032738 0.173245 age 0.544018 0.262408 0.239571 -0.115873 -0.040942 0.035911 0.032738 1.000000 0.236417 class 0.221087 0.465856 0.064882 0.073265 0.131984 0.292695 0.173245 0.236417 1.000000 <p>What is relevant in the correlation's return is to observe the value of the predictor variables concerning the target. Example: Age x Target\u200a-\u200apositive correlation, as you get older, the more likely you are to develop Diabetes.</p> <p>Skew (or symmetry) refers to the distribution of data assumed to be normal or Gaussian (bell curve). Many Machine Learning algorithms assume that the data has a normal distribution. Knowing the symmetry of the data allows you to make a preparation and deliver what the algorithm expects to receive, thus increasing the predictive model's accuracy.</p> <p>Don't expect the data to come ready to use. At some point, we need to modify the variables so that they have a format of a normal distribution\u200a-\u200athis does not mean changing the variable but changing the scale so that it can be in the normal distribution format.</p> In\u00a0[\u00a0]: Copied! <pre># Checking each attribute's skew\ndata.skew()\n</pre> # Checking each attribute's skew data.skew() Out[\u00a0]: <pre>preg     0.903976\nplas     0.176412\npres    -1.841911\nskin     0.112058\ntest     2.270630\nmass    -0.427950\npedi     1.921190\nage      1.135165\nclass    0.638949\ndtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre># It may be more interesting to generate the charts in a separate window as it is a set of smaller graphs.\nimport matplotlib.pyplot as plt\n\n# To be able to generate graphics within this window\n%matplotlib inline\n</pre> # It may be more interesting to generate the charts in a separate window as it is a set of smaller graphs. import matplotlib.pyplot as plt  # To be able to generate graphics within this window %matplotlib inline <p>If we want to view the histogram more broadly in another window, we need to reset the Jupyter Notebook and remove the %matplotlib inline.</p> <p>With the histogram, we can quickly assess the distribution of each attribute. Histograms group data into bins and provide a count of the number of observations in each container.</p> <p>We can quickly check the data's symmetry with the histogram and whether it is in a normal distribution. It will help to identify outliers.</p> In\u00a0[\u00a0]: Copied! <pre># Univariate Histogram\ndata.hist()\nplt.show()\n\n# Graphics appear small due to% matplotlib inline\n # To view more broadly:\n  # Kernel &gt; Restart e Clear Output\n   # Remove the inline line, and the graph will appear in another window\n</pre> # Univariate Histogram data.hist() plt.show()  # Graphics appear small due to% matplotlib inline  # To view more broadly:   # Kernel &gt; Restart e Clear Output    # Remove the inline line, and the graph will appear in another window <p>Density Plots are another way to visualize the distribution of data for each attribute. The plot is like a kind of abstract histogram with a smooth curve through the top of a histogram's bins.</p> <p>It may be easier to identify the distribution of the data using a density plot. The class seems to have two peaks because there are two classes \u2014 0|1.</p> In\u00a0[\u00a0]: Copied! <pre># Density Plot Univariate 'Density'\ndata.plot(kind = 'density', subplots = True, layout = (3,3), sharex = False)\nplt.show()\n</pre> # Density Plot Univariate 'Density' data.plot(kind = 'density', subplots = True, layout = (3,3), sharex = False) plt.show() <p>With boxplots, we can also review the data distribution for each attribute. Boxplot helps to get an idea of the dispersion of the data and identifies outliers quickly: values that deviate a lot from the data's average. If you leave the outlier in the Dataset, it can affect the predictive model.</p> <p>We can see that the dispersion of the data is quite different among the attributes. The age, skin, and test columns have symmetry very close to smaller data values.</p> In\u00a0[\u00a0]: Copied! <pre># Box and Whisker Plots\ndata.plot(kind = 'box', subplots = True, layout = (3,3), sharex = False, sharey = False)\nplt.show()\n</pre> # Box and Whisker Plots data.plot(kind = 'box', subplots = True, layout = (3,3), sharex = False, sharey = False) plt.show() In\u00a0[\u00a0]: Copied! <pre># Correlation matrix with variable names\ncorrelations = data.corr()\n\n# The correlations variable receives all data correlations\n\nimport numpy as np             # Call Numpy\nfig = plt.figure ()            # Plot figure\nax = fig.add_subplot (111)     # Add subplot\ncax = ax.matshow (correlations, vmin = -1, vmax = 1) # Show correlations in the range of -1 to +1\nfig.colorbar (cax)             # Coloring boxplot\nticks = np.arange (0, 9, 1)    # The array defines the size of the 9x9 square to be plotted\nax.set_xticks (ticks)          # Take the size of \"ticks\" and place it on the x axis\nax.set_yticks (ticks)          # Take the size of \"ticks\" and place it on the axis\nax.set_xticklabels (columns)   # Apply the columns listed at the beginning as labels\nax.set_yticklabels (columns)   # Applies the columns listed at the beginning as labels\nplt.show () # Plot\n</pre> # Correlation matrix with variable names correlations = data.corr()  # The correlations variable receives all data correlations  import numpy as np             # Call Numpy fig = plt.figure ()            # Plot figure ax = fig.add_subplot (111)     # Add subplot cax = ax.matshow (correlations, vmin = -1, vmax = 1) # Show correlations in the range of -1 to +1 fig.colorbar (cax)             # Coloring boxplot ticks = np.arange (0, 9, 1)    # The array defines the size of the 9x9 square to be plotted ax.set_xticks (ticks)          # Take the size of \"ticks\" and place it on the x axis ax.set_yticks (ticks)          # Take the size of \"ticks\" and place it on the axis ax.set_xticklabels (columns)   # Apply the columns listed at the beginning as labels ax.set_yticklabels (columns)   # Applies the columns listed at the beginning as labels plt.show () # Plot In\u00a0[\u00a0]: Copied! <pre># Simplified generic correlation matrix\ncorrelations = data.corr()\n\n# Plot\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin = -1, vmax = 1)\nfig.colorbar(cax)\nplt.show()\n</pre> # Simplified generic correlation matrix correlations = data.corr()  # Plot fig = plt.figure() ax = fig.add_subplot(111) cax = ax.matshow(correlations, vmin = -1, vmax = 1) fig.colorbar(cax) plt.show() In\u00a0[\u00a0]: Copied! <pre># Scatter Plot\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(data)\nplt.show()\n</pre> # Scatter Plot from pandas.plotting import scatter_matrix scatter_matrix(data) plt.show() In\u00a0[\u00a0]: Copied! <pre>import seaborn as sns\n</pre> import seaborn as sns In\u00a0[\u00a0]: Copied! <pre># Pairplot\nsns.pairplot(data)\n</pre> # Pairplot sns.pairplot(data) Out[\u00a0]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x7aa1ec67cb80&gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Boxplot with vertical orientation, variables side by side\n# Much simpler parameters\nsns.boxplot(data = data, orient = \"v\")\n</pre> # Boxplot with vertical orientation, variables side by side # Much simpler parameters sns.boxplot(data = data, orient = \"v\") Out[\u00a0]: <pre>&lt;Axes: &gt;</pre> In\u00a0[\u00a0]: Copied! <pre># Clustermap to see how the dataset is organized\nsns.clustermap(data)\n</pre> # Clustermap to see how the dataset is organized sns.clustermap(data) Out[\u00a0]: <pre>&lt;seaborn.matrix.ClusterGrid at 0x7aa1ec18f220&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>data.describe()\n</pre> data.describe() Out[\u00a0]: preg plas pres skin test mass pedi age class count 767.000000 767.000000 767.000000 767.000000 767.000000 767.000000 767.000000 767.000000 767.000000 mean 3.842243 120.859192 69.101695 20.517601 79.903520 31.990482 0.471674 33.219035 0.348110 std 3.370877 31.978468 19.368155 15.954059 115.283105 7.889091 0.331497 11.752296 0.476682 min 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.078000 21.000000 0.000000 25% 1.000000 99.000000 62.000000 0.000000 0.000000 27.300000 0.243500 24.000000 0.000000 50% 3.000000 117.000000 72.000000 23.000000 32.000000 32.000000 0.371000 29.000000 0.000000 75% 6.000000 140.000000 80.000000 32.000000 127.500000 36.600000 0.625000 41.000000 1.000000 max 17.000000 199.000000 122.000000 99.000000 846.000000 67.100000 2.420000 81.000000 1.000000 In\u00a0[\u00a0]: Copied! <pre>from scipy import stats\nsns.distplot(data.pedi, fit = stats.norm);\n</pre> from scipy import stats sns.distplot(data.pedi, fit = stats.norm); <pre>&lt;ipython-input-115-e4017740686f&gt;:2: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  sns.distplot(data.pedi, fit = stats.norm);\n</pre> <p>After performing the Exploratory Analysis to understand the data, we are ready to start the pre-processing step. This step embraces the transformation of the variables, selects the best ones for model creation, reduces the dimensionality for massive data sets, sampling, and other techniques depending on the data, business problem, or algorithm.</p> <p>Many algorithms expect to receive data in a specific format. It is our job to prepare the data in a structure that is suitable for the algorithm we are using. The challenge is that each algorithm requires a different system, which may require other transformations in the data. But it is possible in some cases to obtain good results without pre-processing work.</p> <p>Therefore, the important thing is not to decorate the process but rather to understand when it has to be done. As we work on different projects, the data may already arrive pre-processed.</p> <p>https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html</p> <p>Normalization, binarization, and standardization are techniques exclusively applied to quantitative variables.</p> <p>Normalization changes the scale of the data, and we have two techniques in scikit-learn; while standardization does not alter the distribution of data, it only places the distribution in a Gaussian format. That is, if the data is already in a normal distribution, we can only standardize it. And we still have binarization, which puts the data with the value 0 or 1 according to a rule that we specify.</p> <p>It is one of the first tasks within the pre-processing. It is to put data on the same scale. Many Machine Learning algorithms will benefit from this and produce better results. This step is also called Normalization and means putting the data on a scale with a range between 0 and 1. Normalization is valuable for optimization, being used in the core of the Machine Learning algorithms, such as gradient descent.</p> <p>It helps algorithms such as regression and neural networks and algorithms that use distance measurements, such as KNN. Scikit-learn has a function for this step, called MinMaxScaler ().</p> In\u00a0[\u00a0]: Copied! <pre># Transforming data to the same scale (between 0 and 1)\n\n# Import of modules\nfrom pandas import read_csv\n\n# Importing MinMaxScaler function\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Loading data\nfile = '/content/diabetes.csv'\ncolumns = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndata = read_csv (file, names=columns, skiprows=1)\narray = data.values # Take the data and place it in an objectc called array\n\n# Separating the array into input (X) and output (Y) components\n\n# The dataset has 9 columns, the first 8 of which are predictors.\nX = array [:, 0: 8]\n\n# The last column is the target class\nY = array [:, 8]\n\n# Generating the new scale (normalizing the data)\nscaler = MinMaxScaler (feature_range = (0, 1))\n\n# Fit for predictor variables\nrescaledX = scaler.fit_transform (X)\n\n# Summarizing the transformed data\nprint (\"Original Data: \\ n \\ n\", data.values)\nprint (\"\\ nStandardized Data: \\ n \\ n\", rescaledX [0: 5 ,:])\n</pre> # Transforming data to the same scale (between 0 and 1)  # Import of modules from pandas import read_csv  # Importing MinMaxScaler function from sklearn.preprocessing import MinMaxScaler  # Loading data file = '/content/diabetes.csv' columns = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class'] data = read_csv (file, names=columns, skiprows=1) array = data.values # Take the data and place it in an objectc called array  # Separating the array into input (X) and output (Y) components  # The dataset has 9 columns, the first 8 of which are predictors. X = array [:, 0: 8]  # The last column is the target class Y = array [:, 8]  # Generating the new scale (normalizing the data) scaler = MinMaxScaler (feature_range = (0, 1))  # Fit for predictor variables rescaledX = scaler.fit_transform (X)  # Summarizing the transformed data print (\"Original Data: \\ n \\ n\", data.values) print (\"\\ nStandardized Data: \\ n \\ n\", rescaledX [0: 5 ,:]) <pre>Original Data: \\ n \\ n [[  6.    148.     72.    ...   0.627  50.      1.   ]\n [  1.     85.     66.    ...   0.351  31.      0.   ]\n [  8.    183.     64.    ...   0.672  32.      1.   ]\n ...\n [  5.    121.     72.    ...   0.245  30.      0.   ]\n [  1.    126.     60.    ...   0.349  47.      1.   ]\n [  1.     93.     70.    ...   0.315  23.      0.   ]]\n\\ nStandardized Data: \\ n \\ n [[0.35294118 0.74371859 0.59016393 0.35353535 0.         0.50074516\n  0.23441503 0.48333333]\n [0.05882353 0.42713568 0.54098361 0.29292929 0.         0.39642325\n  0.11656704 0.16666667]\n [0.47058824 0.91959799 0.52459016 0.         0.         0.34724292\n  0.25362938 0.18333333]\n [0.05882353 0.44723618 0.54098361 0.23232323 0.11111111 0.41877794\n  0.03800171 0.        ]\n [0.         0.68844221 0.32786885 0.35353535 0.19858156 0.64232489\n  0.94363792 0.2       ]]\n</pre> <p>Here we transform the data to the same scale. We import the MinMaxScaler function for Normalization and read_csv function for reading the dataset.Next, we define the column names, pass the data values to an array, apply slicing in subsets, whereas the first eight columns are X predictors and the last column is the target variable y.</p> <p>With the parameter feature_range of the MinMaxScaler function, we specify the scale between 0 and 1. After creating the scaler object, we use the fit process to apply the scaler to the X predictor data set\u200a-\u200awe do not need to normalize the output y variable in this case. That is, we use Normalization only to quantitative predictor variables.</p> <p></p> <p>Does this look familiar?</p> <p></p> <p>We will be doing a similar thing in python, but in a more advanced level.</p> <p>A short explanation:</p> <p>Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. The goal of linear regression is to find the linear equation that best describes the relationship between the variables.</p> <p>In a simple linear regression, there is one independent variable and one dependent variable.</p> <p>The linear equation takes the form of:</p> <p>$y = a + bx$</p> <p>where y is the dependent variable, x is the independent variable, a is the intercept, and b is the slope. The intercept represents the value of y when x is equal to zero, and the slope represents the change in y for a one-unit increase in x.</p> <p>All the math involved in building the model is abstracted away into functions, so that machine learning engineers need not implement the algorithm from scratch!</p> In\u00a0[\u00a0]: Copied! <pre># Importing the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n</pre> # Importing the necessary libraries import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt In\u00a0[\u00a0]: Copied! <pre>sns.get_dataset_names()\n</pre> sns.get_dataset_names() Out[\u00a0]: <pre>['anagrams',\n 'anscombe',\n 'attention',\n 'brain_networks',\n 'car_crashes',\n 'diamonds',\n 'dots',\n 'dowjones',\n 'exercise',\n 'flights',\n 'fmri',\n 'geyser',\n 'glue',\n 'healthexp',\n 'iris',\n 'mpg',\n 'penguins',\n 'planets',\n 'seaice',\n 'taxis',\n 'tips',\n 'titanic']</pre> In\u00a0[\u00a0]: Copied! <pre>tips_data = sns.load_dataset('tips')\n</pre> tips_data = sns.load_dataset('tips') In\u00a0[\u00a0]: Copied! <pre>sns.load_dataset('diamonds')\n</pre> sns.load_dataset('diamonds') Out[\u00a0]: carat cut color clarity depth table price x y z 0 0.23 Ideal E SI2 61.5 55.0 326 3.95 3.98 2.43 1 0.21 Premium E SI1 59.8 61.0 326 3.89 3.84 2.31 2 0.23 Good E VS1 56.9 65.0 327 4.05 4.07 2.31 3 0.29 Premium I VS2 62.4 58.0 334 4.20 4.23 2.63 4 0.31 Good J SI2 63.3 58.0 335 4.34 4.35 2.75 ... ... ... ... ... ... ... ... ... ... ... 53935 0.72 Ideal D SI1 60.8 57.0 2757 5.75 5.76 3.50 53936 0.72 Good D SI1 63.1 55.0 2757 5.69 5.75 3.61 53937 0.70 Very Good D SI1 62.8 60.0 2757 5.66 5.68 3.56 53938 0.86 Premium H SI2 61.0 58.0 2757 6.15 6.12 3.74 53939 0.75 Ideal D SI2 62.2 55.0 2757 5.83 5.87 3.64 <p>53940 rows \u00d7 10 columns</p> In\u00a0[\u00a0]: Copied! <pre>sns.load_dataset('attention')\n</pre> sns.load_dataset('attention') Out[\u00a0]: Unnamed: 0 subject attention solutions score 0 0 1 divided 1 2.0 1 1 2 divided 1 3.0 2 2 3 divided 1 3.0 3 3 4 divided 1 5.0 4 4 5 divided 1 4.0 5 5 6 divided 1 5.0 6 6 7 divided 1 5.0 7 7 8 divided 1 5.0 8 8 9 divided 1 2.0 9 9 10 divided 1 6.0 10 10 11 focused 1 6.0 11 11 12 focused 1 8.0 12 12 13 focused 1 6.0 13 13 14 focused 1 8.0 14 14 15 focused 1 8.0 15 15 16 focused 1 6.0 16 16 17 focused 1 7.0 17 17 18 focused 1 7.0 18 18 19 focused 1 5.0 19 19 20 focused 1 6.0 20 20 1 divided 2 4.0 21 21 2 divided 2 4.0 22 22 3 divided 2 5.0 23 23 4 divided 2 7.0 24 24 5 divided 2 5.0 25 25 6 divided 2 5.0 26 26 7 divided 2 4.5 27 27 8 divided 2 7.0 28 28 9 divided 2 3.0 29 29 10 divided 2 5.0 30 30 11 focused 2 5.0 31 31 12 focused 2 9.0 32 32 13 focused 2 5.0 33 33 14 focused 2 8.0 34 34 15 focused 2 8.0 35 35 16 focused 2 8.0 36 36 17 focused 2 7.0 37 37 18 focused 2 8.0 38 38 19 focused 2 6.0 39 39 20 focused 2 6.0 40 40 1 divided 3 7.0 41 41 2 divided 3 5.0 42 42 3 divided 3 6.0 43 43 4 divided 3 5.0 44 44 5 divided 3 8.0 45 45 6 divided 3 6.0 46 46 7 divided 3 6.0 47 47 8 divided 3 8.0 48 48 9 divided 3 7.0 49 49 10 divided 3 6.0 50 50 11 focused 3 6.0 51 51 12 focused 3 8.0 52 52 13 focused 3 9.0 53 53 14 focused 3 7.0 54 54 15 focused 3 7.0 55 55 16 focused 3 7.0 56 56 17 focused 3 6.0 57 57 18 focused 3 6.0 58 58 19 focused 3 6.0 59 59 20 focused 3 5.0 In\u00a0[\u00a0]: Copied! <pre>tips_data\n</pre> tips_data Out[\u00a0]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 ... ... ... ... ... ... ... ... 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 <p>244 rows \u00d7 7 columns</p> <p>The Tips dataset is a data frame with 244 rows and 7 variables which represents some tipping data where one waiter recorded information about each tip he received over a period of a few months working in one restaurant. The waiter collected several variables: Tip in dollars, the bill in dollars Sex of the bill payer Whether there were smokers in the party Day of the week Time of day Size of the party.</p> In\u00a0[\u00a0]: Copied! <pre>sns.pairplot(tips_data)\n</pre> sns.pairplot(tips_data) Out[\u00a0]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x7aa1e83fc970&gt;</pre> <p>From the pairplot we can see that there is almost a linear correspondence between total_bill and tip.</p> <p>Let's try to make a linear regression model between <code>total_bill</code> and <code>tip</code>, where the model predicts the tip given to a waiter based on the total bill the customer pays.</p> In\u00a0[\u00a0]: Copied! <pre>sns.scatterplot(x='total_bill', y='tip', data=tips_data)\n</pre> sns.scatterplot(x='total_bill', y='tip', data=tips_data) Out[\u00a0]: <pre>&lt;Axes: xlabel='total_bill', ylabel='tip'&gt;</pre> <p>This will create a scatterplot of the <code>total_bill</code> vs <code>tip</code> variables.</p> <p>Checking for a linear model:</p> In\u00a0[\u00a0]: Copied! <pre>sns.lmplot(x='total_bill', y='tip', data=tips_data)\n</pre> sns.lmplot(x='total_bill', y='tip', data=tips_data) Out[\u00a0]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7aa1ebe4fcd0&gt;</pre> <p>This will add a regression line to the scatterplot, showing the relationship between 'total_bill' and 'tip'.</p> <p>First we will set a variable <code>x</code> equal to the numerical features of the tips data and a variable <code>y</code> equal to the <code>tips</code> column.</p> In\u00a0[\u00a0]: Copied! <pre>y = tips_data['tip']\n</pre> y = tips_data['tip'] In\u00a0[\u00a0]: Copied! <pre>y\n</pre> y Out[\u00a0]: <pre>0      1.01\n1      1.66\n2      3.50\n3      3.31\n4      3.61\n       ... \n239    5.92\n240    2.00\n241    2.00\n242    1.75\n243    3.00\nName: tip, Length: 244, dtype: float64</pre> In\u00a0[\u00a0]: Copied! <pre>x = tips_data['total_bill']\n</pre> x = tips_data['total_bill'] <p>Use model_selection.train_test_split from sklearn to split the data into training and testing sets.</p> In\u00a0[\u00a0]: Copied! <pre>TEST_SIZE = 0.1\nRANDOM_STATE = 2\n</pre> TEST_SIZE = 0.1 RANDOM_STATE = 2 In\u00a0[\u00a0]: Copied! <pre>from sklearn.model_selection import train_test_split\n</pre> from sklearn.model_selection import train_test_split In\u00a0[\u00a0]: Copied! <pre>from sklearn.linear_model import LinearRegression\n</pre> from sklearn.linear_model import LinearRegression <p>Creating an instance of a linear regression model:</p> In\u00a0[\u00a0]: Copied! <pre>lm = LinearRegression(fit_intercept=False)\n</pre> lm = LinearRegression(fit_intercept=False) <p><code>fit_intercept</code> sets the intercept value to zero.</p> <p>Basically, if total_bill is zero then tip is also zero.</p> In\u00a0[\u00a0]: Copied! <pre>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n</pre> x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE) In\u00a0[\u00a0]: Copied! <pre>lm.fit(x_train, y_train)\n</pre> lm.fit(x_train, y_train) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-134-f524e915cff4&gt; in &lt;cell line: 1&gt;()\n----&gt; 1 lm.fit(x_train, y_train)\n\n/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_base.py in fit(self, X, y, sample_weight)\n    646         accept_sparse = False if self.positive else [\"csr\", \"csc\", \"coo\"]\n    647 \n--&gt; 648         X, y = self._validate_data(\n    649             X, y, accept_sparse=accept_sparse, y_numeric=True, multi_output=True\n    650         )\n\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)\n    582                 y = check_array(y, input_name=\"y\", **check_y_params)\n    583             else:\n--&gt; 584                 X, y = check_X_y(X, y, **check_params)\n    585             out = X, y\n    586 \n\n/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n   1104         )\n   1105 \n-&gt; 1106     X = check_array(\n   1107         X,\n   1108         accept_sparse=accept_sparse,\n\n/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\n    900             # If input is 1D raise error\n    901             if array.ndim == 1:\n--&gt; 902                 raise ValueError(\n    903                     \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n    904                     \"Reshape your data either using array.reshape(-1, 1) if \"\n\nValueError: Expected 2D array, got 1D array instead:\narray=[16.45 17.92 18.43 12.46 13.28 17.46 22.12 12.43 24.08 25.89 39.42 30.14\n 13.16 14.83 25.71 34.63 28.15 18.26 18.15 14.26 17.81 45.35 12.76 13.42\n 16.43 19.77 40.55 12.66 30.4  24.27  8.77 21.7  15.42 19.65 27.2  16.97\n 31.85 25.29 20.27 21.01 17.47  7.25 11.69 10.07 24.52 10.77 11.38 14.73\n 20.9  14.52 38.07 14.78 28.97 27.18 18.29 22.49 20.76 29.03 32.9  28.55\n 13.94 12.74 15.81 18.71 26.41 13.42 13.39 17.29 13.13 16.4  15.53 14.31\n 21.5  38.73 17.31 10.34 35.26 26.59 12.16 15.04 16.49 28.17 10.63 34.65\n  8.51 19.08 20.49  8.58 21.01 22.67 48.17 20.29 24.55 11.61 13.   12.26\n 22.75 22.76 12.9  23.17 11.87 20.53 18.24  7.25 15.48 17.51 10.07 16.47\n 23.33 24.01 41.19 16.31  5.75 11.02 16.99 12.69 11.35 13.51 28.44 17.89\n 27.05 16.29 48.27 18.78 29.85  7.51 15.95 19.49 10.33 43.11 23.1  20.69\n 18.28 18.64 12.03 29.8  13.81 16.66 20.23 22.82 38.01 20.65 13.03 12.02\n 14.15 20.29 17.78 16.04 16.   17.92 30.46 13.37 15.06 35.83 27.28 32.68\n 14.   15.36 25.28 10.65 14.07  8.35 15.69 17.82  9.6  10.33 16.82 31.27\n 18.04 20.45 11.17 12.54 34.81 19.44 16.58 10.09 13.42 15.69 13.   22.23\n 10.34 24.71 15.01  3.07 15.98 11.24 20.69 18.69 22.42 10.29 25.21 50.81\n 24.59 44.3   7.56 31.71 16.93 29.93 12.48  9.78  7.74 18.35 18.29 32.4\n 13.27 12.6  40.17 14.48 16.21 26.88 20.92 10.51 30.06  9.68 15.77 26.86\n 32.83 21.58 10.59].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.</pre> <p>Hmm, why is the above statement giving an error..</p> <p>Because in most cases linear regression in multivariate, i.e, <code>y</code> depends on more than one variable.</p> <p>Our <code>x</code> is a single dimensional array, but the model only works on multi-dimesional arrays..</p> <p>In our case, we could consider the <code>size</code> column of the data set to take into account the dependence in size as well.</p> <p><code>random_state</code> sets the seed for the random generator so that we can ensure that the results that we get can be reproduced.</p> <p><code>test_size</code> sets aside part of the data for testing purpose.</p> In\u00a0[\u00a0]: Copied! <pre>x = tips_data[['total_bill', 'size']]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n</pre> x = tips_data[['total_bill', 'size']] x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE) In\u00a0[\u00a0]: Copied! <pre>lm.fit(x_train, y_train)\n</pre> lm.fit(x_train, y_train) Out[\u00a0]: <pre>LinearRegression(fit_intercept=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression(fit_intercept=False)</pre> In\u00a0[\u00a0]: Copied! <pre>print('Coefficients: \\n', lm.coef_)\n</pre> print('Coefficients: \\n', lm.coef_) <pre>Coefficients: \n [0.09632358 0.37962516]\n</pre> <p>But if you see the pair plot from the initial steps, there is no correlation between size and waiter tip..</p> <p>So, you could reshape the original total_bill array, as suggested by the error message.</p> <p>Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.</p> In\u00a0[\u00a0]: Copied! <pre>x = tips_data['total_bill'].values.reshape(-1, 1)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n</pre> x = tips_data['total_bill'].values.reshape(-1, 1) x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE) In\u00a0[\u00a0]: Copied! <pre>lm.fit(x_train, y_train)\n</pre> lm.fit(x_train, y_train) Out[\u00a0]: <pre>LinearRegression(fit_intercept=False)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegression<pre>LinearRegression(fit_intercept=False)</pre> In\u00a0[\u00a0]: Copied! <pre>print('Coefficients: \\n', lm.coef_)\n</pre> print('Coefficients: \\n', lm.coef_) <pre>Coefficients: \n [0.14113406]\n</pre> In\u00a0[\u00a0]: Copied! <pre>tip_predictions = lm.predict(x_test)\n</pre> tip_predictions = lm.predict(x_test) In\u00a0[\u00a0]: Copied! <pre>tip_predictions\n</pre> tip_predictions Out[\u00a0]: <pre>array([4.91569945, 3.60738668, 1.20246223, 2.30330792, 3.38016083,\n       4.8408984 , 1.4028726 , 2.83397201, 3.5283516 , 6.82100931,\n       1.44944684, 2.48254819, 2.98639679, 2.40915847, 1.34783031,\n       3.34205464, 1.94906142, 2.25532234, 1.6357438 , 2.79586581,\n       2.29625122, 2.1706419 , 3.39568558, 2.79727715, 2.43597394])</pre> In\u00a0[\u00a0]: Copied! <pre>y_test\n</pre> y_test Out[\u00a0]: <pre>85     5.17\n54     4.34\n126    1.48\n93     4.30\n113    2.55\n141    6.70\n53     1.56\n65     3.15\n157    3.75\n212    9.00\n10     1.71\n64     2.64\n89     3.00\n71     3.00\n30     1.45\n3      3.31\n163    2.00\n84     2.03\n217    1.50\n191    4.19\n225    2.50\n101    3.00\n35     3.60\n24     3.18\n152    2.74\nName: tip, dtype: float64</pre> <p>Let's now create a scatterplot of the real test values versus the predicted values.</p> In\u00a0[\u00a0]: Copied! <pre>plt.scatter(y_test, tip_predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\n</pre> plt.scatter(y_test, tip_predictions) plt.xlabel('Y Test') plt.ylabel('Predicted Y') Out[\u00a0]: <pre>Text(0, 0.5, 'Predicted Y')</pre> In\u00a0[\u00a0]: Copied! <pre>from sklearn.metrics import mean_squared_error, r2_score\n</pre> from sklearn.metrics import mean_squared_error, r2_score In\u00a0[\u00a0]: Copied! <pre>mse = mean_squared_error(y_test, tip_predictions)\nr2 = r2_score(y_test, tip_predictions)\n</pre> mse = mean_squared_error(y_test, tip_predictions) r2 = r2_score(y_test, tip_predictions) In\u00a0[\u00a0]: Copied! <pre>print('Mean squared error:', mse)\nprint('R-squared:', r2)\n</pre> print('Mean squared error:', mse) print('R-squared:', r2) <pre>Mean squared error: 0.6884456481075553\nR-squared: 0.7602517788014654\n</pre> <p>Lower the MSE, the more accurate a model is. An MSE of zero is a perfect model.</p> <p>R-squared gives a accuracy measure in terms of percentage.</p> In\u00a0[\u00a0]: Copied! <pre>accuracy = round(r2*100, 2)\nprint(f'Model accuracy \u2248', accuracy, '%')\n</pre> accuracy = round(r2*100, 2) print(f'Model accuracy \u2248', accuracy, '%') <pre>Model accuracy \u2248 76.03 %\n</pre> <p>Let's compare the predicted and actual values:</p> In\u00a0[\u00a0]: Copied! <pre>tip_predictions = [round(prediction, 2) for prediction in tip_predictions]\ny_test = list(y_test)\n</pre> tip_predictions = [round(prediction, 2) for prediction in tip_predictions] y_test = list(y_test) In\u00a0[\u00a0]: Copied! <pre>comparision_df = pd.DataFrame({'Predicted': tip_predictions, 'Actual': y_test})\n</pre> comparision_df = pd.DataFrame({'Predicted': tip_predictions, 'Actual': y_test}) In\u00a0[\u00a0]: Copied! <pre>comparision_df\n</pre> comparision_df Out[\u00a0]: Predicted Actual 0 4.92 5.17 1 3.61 4.34 2 1.20 1.48 3 2.30 4.30 4 3.38 2.55 5 4.84 6.70 6 1.40 1.56 7 2.83 3.15 8 3.53 3.75 9 6.82 9.00 10 1.45 1.71 11 2.48 2.64 12 2.99 3.00 13 2.41 3.00 14 1.35 1.45 15 3.34 3.31 16 1.95 2.00 17 2.26 2.03 18 1.64 1.50 19 2.80 4.19 20 2.30 2.50 21 2.17 3.00 22 3.40 3.60 23 2.80 3.18 24 2.44 2.74 <p>Linear regression is a powerful method for modeling the relationship between input features and target variables, and Seaborn makes it easy to perform and visualize linear regression in Python.</p> <p>Its similar to linear regression with a bit of difference. Linear Regression is used to handle regression problems whereas Logistic regression is used to handle the classification problems. Linear regression provides a continuous output but Logistic regression provides discreet output.</p> In\u00a0[\u00a0]: Copied! <pre>import io\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport scipy.stats\nimport scipy.special\nimport seaborn as sns\nsns.set_style('white')\nsns.set_context('notebook')\n</pre> import io import matplotlib.pyplot as plt import pandas as pd import numpy as np import scipy.stats import scipy.special import seaborn as sns sns.set_style('white') sns.set_context('notebook') <p>Spider data from Suzuki et al. (2006) In following cells, what we analyze is the same as the analysis made in the paper. However we won't go in that direction because of obvious reasons.</p> In\u00a0[\u00a0]: Copied! <pre>data = \"\"\"Grain size (mm)\tSpiders\n0.245\tabsent\n0.247\tabsent\n0.285\tpresent\n0.299\tpresent\n0.327\tpresent\n0.347\tpresent\n0.356\tabsent\n0.36\tpresent\n0.363\tabsent\n0.364\tpresent\n0.398\tabsent\n0.4\tpresent\n0.409\tabsent\n0.421\tpresent\n0.432\tabsent\n0.473\tpresent\n0.509\tpresent\n0.529\tpresent\n0.561\tabsent\n0.569\tabsent\n0.594\tpresent\n0.638\tpresent\n0.656\tpresent\n0.816\tpresent\n0.853\tpresent\n0.938\tpresent\n1.036\tpresent\n1.045\tpresent\n\"\"\"\ndf = pd.read_table(io.StringIO(data))\ndf.Spiders = df.Spiders == 'present'\ndf.head()\n</pre> data = \"\"\"Grain size (mm)\tSpiders 0.245\tabsent 0.247\tabsent 0.285\tpresent 0.299\tpresent 0.327\tpresent 0.347\tpresent 0.356\tabsent 0.36\tpresent 0.363\tabsent 0.364\tpresent 0.398\tabsent 0.4\tpresent 0.409\tabsent 0.421\tpresent 0.432\tabsent 0.473\tpresent 0.509\tpresent 0.529\tpresent 0.561\tabsent 0.569\tabsent 0.594\tpresent 0.638\tpresent 0.656\tpresent 0.816\tpresent 0.853\tpresent 0.938\tpresent 1.036\tpresent 1.045\tpresent \"\"\" df = pd.read_table(io.StringIO(data)) df.Spiders = df.Spiders == 'present' df.head() Out[\u00a0]: Grain size (mm) Spiders 0 0.245 False 1 0.247 False 2 0.285 True 3 0.299 True 4 0.327 True In\u00a0[\u00a0]: Copied! <pre>df\n</pre> df Out[\u00a0]: Grain size (mm) Spiders 0 0.245 False 1 0.247 False 2 0.285 True 3 0.299 True 4 0.327 True 5 0.347 True 6 0.356 False 7 0.360 True 8 0.363 False 9 0.364 True 10 0.398 False 11 0.400 True 12 0.409 False 13 0.421 True 14 0.432 False 15 0.473 True 16 0.509 True 17 0.529 True 18 0.561 False 19 0.569 False 20 0.594 True 21 0.638 True 22 0.656 True 23 0.816 True 24 0.853 True 25 0.938 True 26 1.036 True 27 1.045 True In\u00a0[\u00a0]: Copied! <pre>df[\"Spiders\"]\n</pre> df[\"Spiders\"] Out[\u00a0]: <pre>0     False\n1     False\n2      True\n3      True\n4      True\n5      True\n6     False\n7      True\n8     False\n9      True\n10    False\n11     True\n12    False\n13     True\n14    False\n15     True\n16     True\n17     True\n18    False\n19    False\n20     True\n21     True\n22     True\n23     True\n24     True\n25     True\n26     True\n27     True\nName: Spiders, dtype: bool</pre> In\u00a0[\u00a0]: Copied! <pre>plt.scatter(df[\"Grain size (mm)\"], df[\"Spiders\"])\nplt.ylabel('Spiders present?')\nsns.despine()\n</pre> plt.scatter(df[\"Grain size (mm)\"], df[\"Spiders\"]) plt.ylabel('Spiders present?') sns.despine() In\u00a0[\u00a0]: Copied! <pre>import sklearn.linear_model\n</pre> import sklearn.linear_model <p>Scikit-learn has a logisitic regression classifier. This classifier uses regularization. To eliminate regularization, we set the regularization parameter $C$ to $10^{12}$</p> In\u00a0[\u00a0]: Copied! <pre># C=1e12 is effectively no regularization - see https://github.com/scikit-learn/scikit-learn/issues/6738\nclf = sklearn.linear_model.LogisticRegression(C=1e12, random_state=0)\nclf.fit(df['Grain size (mm)'].values.reshape(-1, 1), df['Spiders'])\nprint(clf.intercept_, clf.coef_)\n</pre> # C=1e12 is effectively no regularization - see https://github.com/scikit-learn/scikit-learn/issues/6738 clf = sklearn.linear_model.LogisticRegression(C=1e12, random_state=0) clf.fit(df['Grain size (mm)'].values.reshape(-1, 1), df['Spiders']) print(clf.intercept_, clf.coef_) <pre>[-1.64761964] [[5.12153717]]\n</pre> In\u00a0[\u00a0]: Copied! <pre>def plot_log_reg(x, y, data, clf, xmin=None, xmax=None, alpha=1, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.figure\n    ax.scatter(data[x], data[y], color='black', zorder=20, alpha=alpha)\n    if xmin is None:\n        xmin = x.min()\n    if xmax is None:\n        xmax = x.max()\n    X_test = np.linspace(xmin, xmax, 300)\n\n    loss = scipy.special.expit(X_test * clf.coef_ + clf.intercept_).ravel()\n    ax.plot(X_test, loss, linewidth=3)\n\n    ax.set_xlabel(x)\n    ax.set_ylabel(y)\n    fig.tight_layout()\n    sns.despine()\n    return fig, ax\n</pre> def plot_log_reg(x, y, data, clf, xmin=None, xmax=None, alpha=1, ax=None):     if ax is None:         fig, ax = plt.subplots()     else:         fig = ax.figure     ax.scatter(data[x], data[y], color='black', zorder=20, alpha=alpha)     if xmin is None:         xmin = x.min()     if xmax is None:         xmax = x.max()     X_test = np.linspace(xmin, xmax, 300)      loss = scipy.special.expit(X_test * clf.coef_ + clf.intercept_).ravel()     ax.plot(X_test, loss, linewidth=3)      ax.set_xlabel(x)     ax.set_ylabel(y)     fig.tight_layout()     sns.despine()     return fig, ax In\u00a0[\u00a0]: Copied! <pre>plot_log_reg(x='Grain size (mm)', y='Spiders', data=df, clf=clf, xmin=0, xmax=1.5);\n</pre> plot_log_reg(x='Grain size (mm)', y='Spiders', data=df, clf=clf, xmin=0, xmax=1.5); <p>While linear and logistic regression are good for making simple models for regression and classification respectively, modern tree-based models are known to have the best performance on tabular datasets.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nimport pandas as pd\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\n\nfrom sklearn import metrics\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n</pre> import numpy as np import pandas as pd  from sklearn.tree import DecisionTreeClassifier from sklearn.tree import DecisionTreeRegressor  from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import RandomForestRegressor  from xgboost import XGBRegressor from xgboost import XGBClassifier  from sklearn import metrics from sklearn import tree import matplotlib.pyplot as plt from matplotlib.pyplot import figure <p></p> <p>Below we have implemented the above processs in python.</p> <p>To convert the dataset to numbers:</p> <ol> <li>Yes / No becomes 1 and 0.</li> <li>The company type is written like this (corporation - 0, startup - 1, mid-teir - 2, conglomerate - 3).</li> </ol> <p>X is a variable with the inputs and Y is a variable with the targets.</p> <p>To create the modelwe only need to specify a max_depth which is the maximum number of decisions it is allowed to take before it has to give us an answer. Larger max_depth give better accuracy but smaller max_depth models run and train faster. Based on your data try tweaking this to give the best results.</p> In\u00a0[\u00a0]: Copied! <pre>X = np.array([[55000, 0, 0, 0],\n              [50000, 1, 1, 1],\n              [45000, 1, 1, 2],\n              [49000, 0, 1, 1],\n              [69000, 0, 0, 3],\n              [75000, 1, 0, 0],\n              [50000, 0, 1, 2],\n              [55000, 1, 1, 3]])\n\nY = np.array([0, 1, 0, 0, 0, 1, 1, 1])\n\nclf_tree = DecisionTreeClassifier(max_depth = 3, random_state=6)\n\nclf_tree.fit(X, Y)\n\nprint(\"Loss :\", metrics.log_loss(clf_tree.predict(X), Y))\n</pre> X = np.array([[55000, 0, 0, 0],               [50000, 1, 1, 1],               [45000, 1, 1, 2],               [49000, 0, 1, 1],               [69000, 0, 0, 3],               [75000, 1, 0, 0],               [50000, 0, 1, 2],               [55000, 1, 1, 3]])  Y = np.array([0, 1, 0, 0, 0, 1, 1, 1])  clf_tree = DecisionTreeClassifier(max_depth = 3, random_state=6)  clf_tree.fit(X, Y)  print(\"Loss :\", metrics.log_loss(clf_tree.predict(X), Y)) <pre>Loss : 2.2204460492503136e-16\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"Accuracy :\", metrics.accuracy_score(clf_tree.predict(X), Y))\n</pre> print(\"Accuracy :\", metrics.accuracy_score(clf_tree.predict(X), Y)) <pre>Acc : 1.0\n</pre> In\u00a0[\u00a0]: Copied! <pre>figure(figsize=(20, 6), dpi=200)\n\ntree.plot_tree(clf_tree, filled = True, feature_names = [\"Salary\", \"Is Near Home\", \"Offers Cab\", \"Company Type\"], class_names = [\"No\", \"Yes\"])\n\n\nplt.show()\n</pre> figure(figsize=(20, 6), dpi=200)  tree.plot_tree(clf_tree, filled = True, feature_names = [\"Salary\", \"Is Near Home\", \"Offers Cab\", \"Company Type\"], class_names = [\"No\", \"Yes\"])   plt.show() In\u00a0[\u00a0]: Copied! <pre>data = pd.read_csv(\"/content/ML-Bootcamp-2024/Session 1/medical_ds/Training.csv\")\n\ndata.drop('Unnamed: 133', axis = 1, inplace = True)\n\ndata.head()\n</pre> data = pd.read_csv(\"/content/ML-Bootcamp-2024/Session 1/medical_ds/Training.csv\")  data.drop('Unnamed: 133', axis = 1, inplace = True)  data.head() Out[\u00a0]: itching skin_rash nodal_skin_eruptions continuous_sneezing shivering chills joint_pain stomach_pain acidity ulcers_on_tongue ... blackheads scurring skin_peeling silver_like_dusting small_dents_in_nails inflammatory_nails blister red_sore_around_nose yellow_crust_ooze prognosis 0 1 1 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 Fungal infection 1 0 1 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 Fungal infection 2 1 0 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 Fungal infection 3 1 1 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 Fungal infection 4 1 1 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 Fungal infection <p>5 rows \u00d7 133 columns</p> In\u00a0[\u00a0]: Copied! <pre>test_df = pd.read_csv(\"/content/ML-Bootcamp-2024/Session 1/medical_ds/Testing.csv\")\n\ntest_df.head()\n</pre> test_df = pd.read_csv(\"/content/ML-Bootcamp-2024/Session 1/medical_ds/Testing.csv\")  test_df.head() Out[\u00a0]: itching skin_rash nodal_skin_eruptions continuous_sneezing shivering chills joint_pain stomach_pain acidity ulcers_on_tongue ... blackheads scurring skin_peeling silver_like_dusting small_dents_in_nails inflammatory_nails blister red_sore_around_nose yellow_crust_ooze prognosis 0 1 1 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 Fungal infection 1 0 0 0 1 1 1 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 Allergy 2 0 0 0 0 0 0 0 1 1 1 ... 0 0 0 0 0 0 0 0 0 GERD 3 1 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 Chronic cholestasis 4 1 1 0 0 0 0 0 1 0 0 ... 0 0 0 0 0 0 0 0 0 Drug Reaction <p>5 rows \u00d7 133 columns</p> In\u00a0[\u00a0]: Copied! <pre>conditions =list(data['prognosis'].unique())\n\nprint(conditions)\n\nfor i in range(len(conditions)):\n    data['prognosis'] = data['prognosis'].replace([conditions[i]], i)\n    test_df['prognosis'] = test_df['prognosis'].replace([conditions[i]], i)\n\ndata['prognosis'] = data['prognosis'].astype(int)\ntest_df['prognosis'] = test_df['prognosis'].astype(int)\n\ndata.head()\n</pre> conditions =list(data['prognosis'].unique())  print(conditions)  for i in range(len(conditions)):     data['prognosis'] = data['prognosis'].replace([conditions[i]], i)     test_df['prognosis'] = test_df['prognosis'].replace([conditions[i]], i)  data['prognosis'] = data['prognosis'].astype(int) test_df['prognosis'] = test_df['prognosis'].astype(int)  data.head() <pre>['Fungal infection', 'Allergy', 'GERD', 'Chronic cholestasis', 'Drug Reaction', 'Peptic ulcer diseae', 'AIDS', 'Diabetes ', 'Gastroenteritis', 'Bronchial Asthma', 'Hypertension ', 'Migraine', 'Cervical spondylosis', 'Paralysis (brain hemorrhage)', 'Jaundice', 'Malaria', 'Chicken pox', 'Dengue', 'Typhoid', 'hepatitis A', 'Hepatitis B', 'Hepatitis C', 'Hepatitis D', 'Hepatitis E', 'Alcoholic hepatitis', 'Tuberculosis', 'Common Cold', 'Pneumonia', 'Dimorphic hemmorhoids(piles)', 'Heart attack', 'Varicose veins', 'Hypothyroidism', 'Hyperthyroidism', 'Hypoglycemia', 'Osteoarthristis', 'Arthritis', '(vertigo) Paroymsal  Positional Vertigo', 'Acne', 'Urinary tract infection', 'Psoriasis', 'Impetigo']\n</pre> Out[\u00a0]: itching skin_rash nodal_skin_eruptions continuous_sneezing shivering chills joint_pain stomach_pain acidity ulcers_on_tongue ... blackheads scurring skin_peeling silver_like_dusting small_dents_in_nails inflammatory_nails blister red_sore_around_nose yellow_crust_ooze prognosis 0 1 1 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 2 1 0 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 3 1 1 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 4 1 1 1 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 <p>5 rows \u00d7 133 columns</p> In\u00a0[\u00a0]: Copied! <pre>len(conditions)\n</pre> len(conditions) Out[\u00a0]: <pre>41</pre> In\u00a0[\u00a0]: Copied! <pre># Create training and testing data\n\nX_train_clf = data.drop(\"prognosis\", axis = 1).to_numpy()\nY_train_clf = data[\"prognosis\"].to_numpy()\n\n\nX_test_clf = test_df.drop(\"prognosis\", axis = 1).to_numpy()\nY_test_clf = test_df[\"prognosis\"].to_numpy()\n</pre> # Create training and testing data  X_train_clf = data.drop(\"prognosis\", axis = 1).to_numpy() Y_train_clf = data[\"prognosis\"].to_numpy()   X_test_clf = test_df.drop(\"prognosis\", axis = 1).to_numpy() Y_test_clf = test_df[\"prognosis\"].to_numpy() In\u00a0[\u00a0]: Copied! <pre>clf_tree_3 = DecisionTreeClassifier(max_depth = 25, random_state=6)\n\nclf_tree_3.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training accuracy :\", metrics.accuracy_score(clf_tree_3.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing accuracy :\", metrics.accuracy_score(clf_tree_3.predict(X_test_clf), Y_test_clf))\n</pre> clf_tree_3 = DecisionTreeClassifier(max_depth = 25, random_state=6)  clf_tree_3.fit(X_train_clf, Y_train_clf)  print(\"Training accuracy :\", metrics.accuracy_score(clf_tree_3.predict(X_train_clf), Y_train_clf))  print(\"Testing accuracy :\", metrics.accuracy_score(clf_tree_3.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 0.6658536585365854\nTesting Loss : 0.6904761904761905\n</pre> In\u00a0[\u00a0]: Copied! <pre>clf_tree_4 = DecisionTreeClassifier(max_depth = 40, random_state=6)\n\nclf_tree_4.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training accuracy :\", metrics.accuracy_score(clf_tree_4.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing accuracy :\", metrics.accuracy_score(clf_tree_4.predict(X_test_clf), Y_test_clf))\n</pre> clf_tree_4 = DecisionTreeClassifier(max_depth = 40, random_state=6)  clf_tree_4.fit(X_train_clf, Y_train_clf)  print(\"Training accuracy :\", metrics.accuracy_score(clf_tree_4.predict(X_train_clf), Y_train_clf))  print(\"Testing accuracy :\", metrics.accuracy_score(clf_tree_4.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 0.9768292682926829\nTesting Loss : 0.9761904761904762\n</pre> In\u00a0[\u00a0]: Copied! <pre>clf_tree_5 = DecisionTreeClassifier(max_depth = 100, random_state=6)\n\nclf_tree_5.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training accuracy :\", metrics.accuracy_score(clf_tree_5.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing accuracy :\", metrics.accuracy_score(clf_tree_5.predict(X_test_clf), Y_test_clf))\n</pre> clf_tree_5 = DecisionTreeClassifier(max_depth = 100, random_state=6)  clf_tree_5.fit(X_train_clf, Y_train_clf)  print(\"Training accuracy :\", metrics.accuracy_score(clf_tree_5.predict(X_train_clf), Y_train_clf))  print(\"Testing accuracy :\", metrics.accuracy_score(clf_tree_5.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 1.0\nTesting Loss : 0.9761904761904762\n</pre> In\u00a0[\u00a0]: Copied! <pre># Visualisation of the best tree\n\ncolumns_clf = list(data.columns)\n\ncolumns_clf.pop(columns_clf.index('prognosis'))\n\nfigure(figsize=(20, 6), dpi=200)\n\ntree.plot_tree(clf_tree_4, filled = True, feature_names = columns_clf, class_names = conditions, max_depth = 4)\n\n\nplt.show()\n</pre> # Visualisation of the best tree  columns_clf = list(data.columns)  columns_clf.pop(columns_clf.index('prognosis'))  figure(figsize=(20, 6), dpi=200)  tree.plot_tree(clf_tree_4, filled = True, feature_names = columns_clf, class_names = conditions, max_depth = 4)   plt.show() <p>Sklearn also allows us to see how important certain features are. This returns a percentage where a percentage x at index i means that the feature i had an x% effect on the outcome.</p> In\u00a0[\u00a0]: Copied! <pre>imp = clf_tree_3.feature_importances_\n\nfor i in range(len(columns_clf)):\n  print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = clf_tree_3.feature_importances_  for i in range(len(columns_clf)):   print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")  <pre>The feature itching contributed 0.0% to the final outcome.\nThe feature skin_rash contributed 0.0% to the final outcome.\nThe feature nodal_skin_eruptions contributed 0.0% to the final outcome.\nThe feature continuous_sneezing contributed 0.0% to the final outcome.\nThe feature shivering contributed 0.0% to the final outcome.\nThe feature chills contributed 3.615233723371911% to the final outcome.\nThe feature joint_pain contributed 0.0% to the final outcome.\nThe feature stomach_pain contributed 0.0% to the final outcome.\nThe feature acidity contributed 0.0% to the final outcome.\nThe feature ulcers_on_tongue contributed 0.0% to the final outcome.\nThe feature muscle_wasting contributed 0.0% to the final outcome.\nThe feature vomiting contributed 0.0% to the final outcome.\nThe feature burning_micturition contributed 0.0% to the final outcome.\nThe feature spotting_ urination contributed 0.0% to the final outcome.\nThe feature fatigue contributed 0.0% to the final outcome.\nThe feature weight_gain contributed 0.0% to the final outcome.\nThe feature anxiety contributed 0.0% to the final outcome.\nThe feature cold_hands_and_feets contributed 0.0% to the final outcome.\nThe feature mood_swings contributed 0.0% to the final outcome.\nThe feature weight_loss contributed 0.0% to the final outcome.\nThe feature restlessness contributed 0.0% to the final outcome.\nThe feature lethargy contributed 0.0% to the final outcome.\nThe feature patches_in_throat contributed 0.0% to the final outcome.\nThe feature irregular_sugar_level contributed 0.0% to the final outcome.\nThe feature cough contributed 0.0% to the final outcome.\nThe feature high_fever contributed 3.608321230593001% to the final outcome.\nThe feature sunken_eyes contributed 0.0% to the final outcome.\nThe feature breathlessness contributed 0.0% to the final outcome.\nThe feature sweating contributed 0.0% to the final outcome.\nThe feature dehydration contributed 0.0% to the final outcome.\nThe feature indigestion contributed 0.0% to the final outcome.\nThe feature headache contributed 0.0% to the final outcome.\nThe feature yellowish_skin contributed 0.0% to the final outcome.\nThe feature dark_urine contributed 0.0% to the final outcome.\nThe feature nausea contributed 0.0% to the final outcome.\nThe feature loss_of_appetite contributed 0.0% to the final outcome.\nThe feature pain_behind_the_eyes contributed 3.808783521181501% to the final outcome.\nThe feature back_pain contributed 0.0% to the final outcome.\nThe feature constipation contributed 3.5989148429805224% to the final outcome.\nThe feature abdominal_pain contributed 0.0% to the final outcome.\nThe feature diarrhoea contributed 0.0% to the final outcome.\nThe feature mild_fever contributed 0.0% to the final outcome.\nThe feature yellow_urine contributed 0.0% to the final outcome.\nThe feature yellowing_of_eyes contributed 3.808783521181501% to the final outcome.\nThe feature acute_liver_failure contributed 0.0% to the final outcome.\nThe feature fluid_overload contributed 0.0% to the final outcome.\nThe feature swelling_of_stomach contributed 0.0% to the final outcome.\nThe feature swelled_lymph_nodes contributed 0.0% to the final outcome.\nThe feature malaise contributed 3.808783521181501% to the final outcome.\nThe feature blurred_and_distorted_vision contributed 0.0% to the final outcome.\nThe feature phlegm contributed 0.0% to the final outcome.\nThe feature throat_irritation contributed 3.808783521181501% to the final outcome.\nThe feature redness_of_eyes contributed 0.0% to the final outcome.\nThe feature sinus_pressure contributed 0.0% to the final outcome.\nThe feature runny_nose contributed 0.0% to the final outcome.\nThe feature congestion contributed 0.0% to the final outcome.\nThe feature chest_pain contributed 0.0% to the final outcome.\nThe feature weakness_in_limbs contributed 0.0% to the final outcome.\nThe feature fast_heart_rate contributed 0.0% to the final outcome.\nThe feature pain_during_bowel_movements contributed 0.0% to the final outcome.\nThe feature pain_in_anal_region contributed 0.0% to the final outcome.\nThe feature bloody_stool contributed 0.0% to the final outcome.\nThe feature irritation_in_anus contributed 0.0% to the final outcome.\nThe feature neck_pain contributed 0.0% to the final outcome.\nThe feature dizziness contributed 0.0% to the final outcome.\nThe feature cramps contributed 0.0% to the final outcome.\nThe feature bruising contributed 3.6056229620853726% to the final outcome.\nThe feature obesity contributed 0.0% to the final outcome.\nThe feature swollen_legs contributed 0.0% to the final outcome.\nThe feature swollen_blood_vessels contributed 0.0% to the final outcome.\nThe feature puffy_face_and_eyes contributed 0.0% to the final outcome.\nThe feature enlarged_thyroid contributed 3.808783521181501% to the final outcome.\nThe feature brittle_nails contributed 0.0% to the final outcome.\nThe feature swollen_extremeties contributed 0.0% to the final outcome.\nThe feature excessive_hunger contributed 0.0% to the final outcome.\nThe feature extra_marital_contacts contributed 0.0% to the final outcome.\nThe feature drying_and_tingling_lips contributed 0.0% to the final outcome.\nThe feature slurred_speech contributed 0.0% to the final outcome.\nThe feature knee_pain contributed 3.610050980744315% to the final outcome.\nThe feature hip_joint_pain contributed 0.0% to the final outcome.\nThe feature muscle_weakness contributed 3.5964035467853006% to the final outcome.\nThe feature stiff_neck contributed 0.0% to the final outcome.\nThe feature swelling_joints contributed 0.0% to the final outcome.\nThe feature movement_stiffness contributed 0.0% to the final outcome.\nThe feature spinning_movements contributed 0.0% to the final outcome.\nThe feature loss_of_balance contributed 0.0% to the final outcome.\nThe feature unsteadiness contributed 0.0% to the final outcome.\nThe feature weakness_of_one_body_side contributed 0.0% to the final outcome.\nThe feature loss_of_smell contributed 0.0% to the final outcome.\nThe feature bladder_discomfort contributed 3.606755075798255% to the final outcome.\nThe feature foul_smell_of urine contributed 0.0% to the final outcome.\nThe feature continuous_feel_of_urine contributed 0.0% to the final outcome.\nThe feature passage_of_gases contributed 0.0% to the final outcome.\nThe feature internal_itching contributed 3.6043164416653326% to the final outcome.\nThe feature toxic_look_(typhos) contributed 0.0% to the final outcome.\nThe feature depression contributed 3.6077425023326075% to the final outcome.\nThe feature irritability contributed 0.0% to the final outcome.\nThe feature muscle_pain contributed 3.808783521181501% to the final outcome.\nThe feature altered_sensorium contributed 3.6086088894853208% to the final outcome.\nThe feature red_spots_over_body contributed 0.0% to the final outcome.\nThe feature belly_pain contributed 0.0% to the final outcome.\nThe feature abnormal_menstruation contributed 3.808783521181501% to the final outcome.\nThe feature dischromic _patches contributed 0.0% to the final outcome.\nThe feature watering_from_eyes contributed 0.0% to the final outcome.\nThe feature increased_appetite contributed 0.0% to the final outcome.\nThe feature polyuria contributed 3.808783521181501% to the final outcome.\nThe feature family_history contributed 0.0% to the final outcome.\nThe feature mucoid_sputum contributed 3.609373243440318% to the final outcome.\nThe feature rusty_sputum contributed 3.808783521181501% to the final outcome.\nThe feature lack_of_concentration contributed 0.0% to the final outcome.\nThe feature visual_disturbances contributed 0.0% to the final outcome.\nThe feature receiving_blood_transfusion contributed 3.808783521181501% to the final outcome.\nThe feature receiving_unsterile_injections contributed 0.0% to the final outcome.\nThe feature coma contributed 3.808783521181501% to the final outcome.\nThe feature stomach_bleeding contributed 0.0% to the final outcome.\nThe feature distention_of_abdomen contributed 0.0% to the final outcome.\nThe feature history_of_alcohol_consumption contributed 3.602797751262589% to the final outcome.\nThe feature fluid_overload.1 contributed 0.0% to the final outcome.\nThe feature blood_in_sputum contributed 3.808783521181501% to the final outcome.\nThe feature prominent_veins_on_calf contributed 0.0% to the final outcome.\nThe feature palpitations contributed 3.808783521181501% to the final outcome.\nThe feature painful_walking contributed 0.0% to the final outcome.\nThe feature pus_filled_pimples contributed 0.0% to the final outcome.\nThe feature blackheads contributed 0.0% to the final outcome.\nThe feature scurring contributed 0.0% to the final outcome.\nThe feature skin_peeling contributed 0.0% to the final outcome.\nThe feature silver_like_dusting contributed 0.0% to the final outcome.\nThe feature small_dents_in_nails contributed 0.0% to the final outcome.\nThe feature inflammatory_nails contributed 3.601018321844881% to the final outcome.\nThe feature blister contributed 0.0% to the final outcome.\nThe feature red_sore_around_nose contributed 0.0% to the final outcome.\nThe feature yellow_crust_ooze contributed 3.610654712250774% to the final outcome.\n</pre> In\u00a0[\u00a0]: Copied! <pre>imp = clf_tree_4.feature_importances_\n\nfor i in range(len(columns_clf)):\n  print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")\n  if i == 15:\n      break\n</pre> imp = clf_tree_4.feature_importances_  for i in range(len(columns_clf)):   print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")   if i == 15:       break <pre>The feature itching contributed 0.24353027343749994% to the final outcome.\nThe feature skin_rash contributed 0.0% to the final outcome.\nThe feature nodal_skin_eruptions contributed 2.05302819973085% to the final outcome.\nThe feature continuous_sneezing contributed 0.0% to the final outcome.\nThe feature shivering contributed 0.24876868206521757% to the final outcome.\nThe feature chills contributed 2.433209150975497% to the final outcome.\nThe feature joint_pain contributed 0.128173828125% to the final outcome.\nThe feature stomach_pain contributed 2.146559495192308% to the final outcome.\nThe feature acidity contributed 0.24353027343749994% to the final outcome.\nThe feature ulcers_on_tongue contributed 0.0% to the final outcome.\nThe feature muscle_wasting contributed 0.0% to the final outcome.\nThe feature vomiting contributed 0.0% to the final outcome.\nThe feature burning_micturition contributed 0.0% to the final outcome.\nThe feature spotting_ urination contributed 1.8541667772376018% to the final outcome.\nThe feature fatigue contributed 0.3479003906249999% to the final outcome.\nThe feature weight_gain contributed 0.0% to the final outcome.\n</pre> In\u00a0[\u00a0]: Copied! <pre>data = pd.read_csv(\"/content/Job_Placement_Data.csv\")\n\ndata = data.drop(\"specialisation\", axis = 1)\ndata.head()\n</pre> data = pd.read_csv(\"/content/Job_Placement_Data.csv\")  data = data.drop(\"specialisation\", axis = 1) data.head() Out[\u00a0]: gender ssc_percentage ssc_board hsc_percentage hsc_board hsc_subject degree_percentage undergrad_degree work_experience emp_test_percentage mba_percent status 0 M 67.00 Others 91.00 Others Commerce 58.00 Sci&amp;Tech No 55.0 58.80 Placed 1 M 79.33 Central 78.33 Others Science 77.48 Sci&amp;Tech Yes 86.5 66.28 Placed 2 M 65.00 Central 68.00 Central Arts 64.00 Comm&amp;Mgmt No 75.0 57.80 Placed 3 M 56.00 Central 52.00 Central Science 52.00 Sci&amp;Tech No 66.0 59.43 Not Placed 4 M 85.80 Central 73.60 Central Commerce 73.30 Comm&amp;Mgmt No 96.8 55.50 Placed In\u00a0[\u00a0]: Copied! <pre># Convert the string columns to numerical datatypes\n\ncolumns_clf = ['gender', 'ssc_percentage', 'ssc_board', 'hsc_percentage', 'hsc_board',\n       'hsc_subject', 'degree_percentage', 'undergrad_degree',\n       'work_experience', 'emp_test_percentage', 'mba_percent']\n\ndata[\"gender\"] = data[\"gender\"].replace([\"M\"], 0)\ndata[\"gender\"] = data[\"gender\"].replace([\"F\"], 1)\ndata[\"gender\"] = data[\"gender\"].astype(int)\n\ndata[\"ssc_board\"] = data[\"ssc_board\"].replace([\"Others\"], 0)\ndata[\"ssc_board\"] = data[\"ssc_board\"].replace([\"Central\"], 1)\ndata[\"ssc_board\"] = data[\"ssc_board\"].astype(int)\n\ndata[\"hsc_board\"] = data[\"hsc_board\"].replace([\"Others\"], 0)\ndata[\"hsc_board\"] = data[\"hsc_board\"].replace([\"Central\"], 1)\ndata[\"hsc_board\"] = data[\"hsc_board\"].astype(int)\n\ndata[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Commerce\"], 0)\ndata[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Science\"], 1)\ndata[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Arts\"], 2)\ndata[\"hsc_subject\"] = data[\"hsc_subject\"].astype(int)\n\ndata[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Sci&amp;Tech\"], 0)\ndata[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Comm&amp;Mgmt\"], 1)\ndata[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Others\"], 2)\ndata[\"undergrad_degree\"] = data[\"undergrad_degree\"].astype(int)\n\ndata[\"work_experience\"] = data[\"work_experience\"].replace([\"No\"], 0)\ndata[\"work_experience\"] = data[\"work_experience\"].replace([\"Yes\"], 1)\ndata[\"work_experience\"] = data[\"work_experience\"].astype(int)\n\ndata[\"status\"] = data[\"status\"].replace([\"Not Placed\"], 0)\ndata[\"status\"] = data[\"status\"].replace([\"Placed\"], 1)\ndata[\"status\"] = data[\"status\"].astype(int)\n\ndata.head()\n</pre> # Convert the string columns to numerical datatypes  columns_clf = ['gender', 'ssc_percentage', 'ssc_board', 'hsc_percentage', 'hsc_board',        'hsc_subject', 'degree_percentage', 'undergrad_degree',        'work_experience', 'emp_test_percentage', 'mba_percent']  data[\"gender\"] = data[\"gender\"].replace([\"M\"], 0) data[\"gender\"] = data[\"gender\"].replace([\"F\"], 1) data[\"gender\"] = data[\"gender\"].astype(int)  data[\"ssc_board\"] = data[\"ssc_board\"].replace([\"Others\"], 0) data[\"ssc_board\"] = data[\"ssc_board\"].replace([\"Central\"], 1) data[\"ssc_board\"] = data[\"ssc_board\"].astype(int)  data[\"hsc_board\"] = data[\"hsc_board\"].replace([\"Others\"], 0) data[\"hsc_board\"] = data[\"hsc_board\"].replace([\"Central\"], 1) data[\"hsc_board\"] = data[\"hsc_board\"].astype(int)  data[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Commerce\"], 0) data[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Science\"], 1) data[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Arts\"], 2) data[\"hsc_subject\"] = data[\"hsc_subject\"].astype(int)  data[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Sci&amp;Tech\"], 0) data[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Comm&amp;Mgmt\"], 1) data[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Others\"], 2) data[\"undergrad_degree\"] = data[\"undergrad_degree\"].astype(int)  data[\"work_experience\"] = data[\"work_experience\"].replace([\"No\"], 0) data[\"work_experience\"] = data[\"work_experience\"].replace([\"Yes\"], 1) data[\"work_experience\"] = data[\"work_experience\"].astype(int)  data[\"status\"] = data[\"status\"].replace([\"Not Placed\"], 0) data[\"status\"] = data[\"status\"].replace([\"Placed\"], 1) data[\"status\"] = data[\"status\"].astype(int)  data.head() Out[\u00a0]: gender ssc_percentage ssc_board hsc_percentage hsc_board hsc_subject degree_percentage undergrad_degree work_experience emp_test_percentage mba_percent status 0 0 67.00 0 91.00 0 0 58.00 0 0 55.0 58.80 1 1 0 79.33 1 78.33 0 1 77.48 0 1 86.5 66.28 1 2 0 65.00 1 68.00 1 2 64.00 1 0 75.0 57.80 1 3 0 56.00 1 52.00 1 1 52.00 0 0 66.0 59.43 0 4 0 85.80 1 73.60 1 0 73.30 1 0 96.8 55.50 1 In\u00a0[\u00a0]: Copied! <pre># Create training and testing data\n\n\nthresh = int(len(data) * 90 / 100) # 90% of the data for training, 10% for testing\n\nX_train_clf = data.drop(\"status\", axis = 1).to_numpy()[:thresh]\nY_train_clf = data[\"status\"].to_numpy()[:thresh]\n\nX_test_clf = data.drop(\"status\", axis = 1).to_numpy()[thresh:]\nY_test_clf = data[\"status\"].to_numpy()[thresh:]\n</pre> # Create training and testing data   thresh = int(len(data) * 90 / 100) # 90% of the data for training, 10% for testing  X_train_clf = data.drop(\"status\", axis = 1).to_numpy()[:thresh] Y_train_clf = data[\"status\"].to_numpy()[:thresh]  X_test_clf = data.drop(\"status\", axis = 1).to_numpy()[thresh:] Y_test_clf = data[\"status\"].to_numpy()[thresh:] In\u00a0[\u00a0]: Copied! <pre>clf_tree = DecisionTreeClassifier(max_depth = 5, random_state=6)\n\nclf_tree.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training Loss :\", metrics.log_loss(clf_tree.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing Loss :\", metrics.log_loss(clf_tree.predict(X_test_clf), Y_test_clf))\n</pre> clf_tree = DecisionTreeClassifier(max_depth = 5, random_state=6)  clf_tree.fit(X_train_clf, Y_train_clf)  print(\"Training Loss :\", metrics.log_loss(clf_tree.predict(X_train_clf), Y_train_clf))  print(\"Testing Loss :\", metrics.log_loss(clf_tree.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 2.4278108500441604\nTesting Loss : 1.638347881323507\n</pre> In\u00a0[\u00a0]: Copied! <pre>clf_tree_3 = DecisionTreeClassifier(max_depth = 7, random_state=6)\n\nclf_tree_3.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training Loss :\", metrics.log_loss(clf_tree_3.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing Loss :\", metrics.log_loss(clf_tree_3.predict(X_test_clf), Y_test_clf))\n</pre> clf_tree_3 = DecisionTreeClassifier(max_depth = 7, random_state=6)  clf_tree_3.fit(X_train_clf, Y_train_clf)  print(\"Training Loss :\", metrics.log_loss(clf_tree_3.predict(X_train_clf), Y_train_clf))  print(\"Testing Loss :\", metrics.log_loss(clf_tree_3.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 0.7470187230905111\nTesting Loss : 3.276695762647014\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"Training ACC :\", metrics.accuracy_score(clf_tree.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing ACC :\", metrics.accuracy_score(clf_tree.predict(X_test_clf), Y_test_clf))\n</pre> print(\"Training ACC :\", metrics.accuracy_score(clf_tree.predict(X_train_clf), Y_train_clf))  print(\"Testing ACC :\", metrics.accuracy_score(clf_tree.predict(X_test_clf), Y_test_clf)) <pre>Training ACC : 0.9326424870466321\nTesting ACC : 0.9545454545454546\n</pre> In\u00a0[\u00a0]: Copied! <pre>figure(figsize=(22, 6), dpi=200)\n\nplt.bar(columns_clf, clf_tree.feature_importances_)\nplt.title(\"Classification Feature Importantance for XGBoost\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Features\")\nplt.show()\n</pre> figure(figsize=(22, 6), dpi=200)  plt.bar(columns_clf, clf_tree.feature_importances_) plt.title(\"Classification Feature Importantance for XGBoost\") plt.xlabel(\"Columns\") plt.ylabel(\"Features\") plt.show() In\u00a0[\u00a0]: Copied! <pre>len(data)\n</pre> len(data) Out[\u00a0]: <pre>215</pre> <p></p> <p></p> In\u00a0[\u00a0]: Copied! <pre>clf_rf = RandomForestClassifier(n_estimators = 300, max_depth=4, n_jobs = -1, random_state=4)\n\nclf_rf.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training Loss :\", metrics.log_loss(clf_rf.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing Loss :\", metrics.log_loss(clf_rf.predict(X_test_clf), Y_test_clf))\n</pre> clf_rf = RandomForestClassifier(n_estimators = 300, max_depth=4, n_jobs = -1, random_state=4)  clf_rf.fit(X_train_clf, Y_train_clf)  print(\"Training Loss :\", metrics.log_loss(clf_rf.predict(X_train_clf), Y_train_clf))  print(\"Testing Loss :\", metrics.log_loss(clf_rf.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 2.8013202115894162\nTesting Loss : 3.276695762647014\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"Training ACC :\", metrics.accuracy_score(clf_rf.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing ACC :\", metrics.accuracy_score(clf_rf.predict(X_test_clf), Y_test_clf))\n</pre>  print(\"Training ACC :\", metrics.accuracy_score(clf_rf.predict(X_train_clf), Y_train_clf))  print(\"Testing ACC :\", metrics.accuracy_score(clf_rf.predict(X_test_clf), Y_test_clf)) <pre>Training ACC : 0.9222797927461139\nTesting ACC : 0.9090909090909091\n</pre> In\u00a0[\u00a0]: Copied! <pre>imp = clf_rf.feature_importances_\nfor i in range(len(columns_clf)):\n  print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = clf_rf.feature_importances_ for i in range(len(columns_clf)):   print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\") <pre>The feature gender contributed 1.1193099400553044% to the final outcome.\nThe feature ssc_percentage contributed 35.04548179149328% to the final outcome.\nThe feature ssc_board contributed 0.6148386199017176% to the final outcome.\nThe feature hsc_percentage contributed 23.6140777205394% to the final outcome.\nThe feature hsc_board contributed 0.4779820451332016% to the final outcome.\nThe feature hsc_subject contributed 1.028407193720827% to the final outcome.\nThe feature degree_percentage contributed 20.208270513644273% to the final outcome.\nThe feature undergrad_degree contributed 1.4975219308484267% to the final outcome.\nThe feature work_experience contributed 3.5194714166195413% to the final outcome.\nThe feature emp_test_percentage contributed 5.249591937745275% to the final outcome.\nThe feature mba_percent contributed 7.625046890298765% to the final outcome.\n</pre> In\u00a0[\u00a0]: Copied! <pre>figure(figsize=(22, 6), dpi=200)\n\nplt.bar(columns_clf, clf_rf.feature_importances_)\nplt.title(\"Classification Feature Importantance for RF\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Features\")\nplt.show()\n</pre> figure(figsize=(22, 6), dpi=200)  plt.bar(columns_clf, clf_rf.feature_importances_) plt.title(\"Classification Feature Importantance for RF\") plt.xlabel(\"Columns\") plt.ylabel(\"Features\") plt.show() <p>Note that the xgboost library stores trees in a different format and therefore is not compatible with the plot_tree method of sklearn.</p> In\u00a0[\u00a0]: Copied! <pre>clf_xgb = XGBClassifier(n_estimators = 20, max_depth=6, n_jobs = -1, seed=3)\n\nclf_xgb.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training Loss :\", metrics.log_loss(clf_xgb.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing Loss :\", metrics.log_loss(clf_xgb.predict(X_test_clf), Y_test_clf))\n</pre> clf_xgb = XGBClassifier(n_estimators = 20, max_depth=6, n_jobs = -1, seed=3)  clf_xgb.fit(X_train_clf, Y_train_clf)  print(\"Training Loss :\", metrics.log_loss(clf_xgb.predict(X_train_clf), Y_train_clf))  print(\"Testing Loss :\", metrics.log_loss(clf_xgb.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 2.2204460492503136e-16\nTesting Loss : 1.638347881323507\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"Training ACC :\", metrics.accuracy_score(clf_xgb.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing ACC :\", metrics.accuracy_score(clf_xgb.predict(X_test_clf), Y_test_clf))\n</pre> print(\"Training ACC :\", metrics.accuracy_score(clf_xgb.predict(X_train_clf), Y_train_clf))  print(\"Testing ACC :\", metrics.accuracy_score(clf_xgb.predict(X_test_clf), Y_test_clf)) <pre>Training ACC : 1.0\nTesting ACC : 0.9545454545454546\n</pre> In\u00a0[\u00a0]: Copied! <pre>imp = clf_xgb.feature_importances_\n\nfor i in range(len(columns_clf)):\n  print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = clf_xgb.feature_importances_  for i in range(len(columns_clf)):   print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\") <pre>The feature gender contributed 5.102251842617989% to the final outcome.\nThe feature ssc_percentage contributed 32.35481083393097% to the final outcome.\nThe feature ssc_board contributed 4.403411224484444% to the final outcome.\nThe feature hsc_percentage contributed 11.843376606702805% to the final outcome.\nThe feature hsc_board contributed 2.9443010687828064% to the final outcome.\nThe feature hsc_subject contributed 4.654479399323463% to the final outcome.\nThe feature degree_percentage contributed 11.035174876451492% to the final outcome.\nThe feature undergrad_degree contributed 4.487524181604385% to the final outcome.\nThe feature work_experience contributed 11.305423825979233% to the final outcome.\nThe feature emp_test_percentage contributed 4.736224561929703% to the final outcome.\nThe feature mba_percent contributed 7.133029401302338% to the final outcome.\n</pre> In\u00a0[\u00a0]: Copied! <pre>figure(figsize=(22, 6), dpi=200)\n\nplt.bar(columns_clf, clf_xgb.feature_importances_)\nplt.title(\"Classification Feature Importantance for XGBoost\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Features\")\nplt.show()\n</pre> figure(figsize=(22, 6), dpi=200)  plt.bar(columns_clf, clf_xgb.feature_importances_) plt.title(\"Classification Feature Importantance for XGBoost\") plt.xlabel(\"Columns\") plt.ylabel(\"Features\") plt.show() In\u00a0[\u00a0]: Copied! <pre>tree_loss = metrics.log_loss(clf_tree.predict(X_test_clf), Y_test_clf)\nrf_loss = metrics.log_loss(clf_rf.predict(X_test_clf), Y_test_clf)\nxgb_loss = metrics.log_loss(clf_xgb.predict(X_test_clf), Y_test_clf)\n\n\nfigure(figsize=(6, 4), dpi=200)\n\nplt.bar([\"Decision Tree\", \"Random Forest\", \"XGBoost\"], [tree_loss, rf_loss, xgb_loss])\nplt.title(\"Model Loss on Classification Task (lower is better)\")\nplt.xlabel(\"Models\")\nplt.ylabel(\"Log Loss\")\nplt.show()\n</pre> tree_loss = metrics.log_loss(clf_tree.predict(X_test_clf), Y_test_clf) rf_loss = metrics.log_loss(clf_rf.predict(X_test_clf), Y_test_clf) xgb_loss = metrics.log_loss(clf_xgb.predict(X_test_clf), Y_test_clf)   figure(figsize=(6, 4), dpi=200)  plt.bar([\"Decision Tree\", \"Random Forest\", \"XGBoost\"], [tree_loss, rf_loss, xgb_loss]) plt.title(\"Model Loss on Classification Task (lower is better)\") plt.xlabel(\"Models\") plt.ylabel(\"Log Loss\") plt.show() In\u00a0[\u00a0]: Copied! <pre>tree_loss = metrics.accuracy_score(clf_tree.predict(X_train_clf), Y_train_clf)\nrf_loss = metrics.accuracy_score(clf_rf.predict(X_train_clf), Y_train_clf)\nxgb_loss = metrics.accuracy_score(clf_xgb.predict(X_train_clf), Y_train_clf)\n\nprint(tree_loss, rf_loss, xgb_loss)\n</pre> tree_loss = metrics.accuracy_score(clf_tree.predict(X_train_clf), Y_train_clf) rf_loss = metrics.accuracy_score(clf_rf.predict(X_train_clf), Y_train_clf) xgb_loss = metrics.accuracy_score(clf_xgb.predict(X_train_clf), Y_train_clf)  print(tree_loss, rf_loss, xgb_loss) <pre>0.9326424870466321 0.9222797927461139 1.0\n</pre> In\u00a0[\u00a0]: Copied! <pre>train_data = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n\ntrain_data.head()\n</pre> train_data = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")  train_data.head() Out[\u00a0]: longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value 0 -114.31 34.19 15.0 5612.0 1283.0 1015.0 472.0 1.4936 66900.0 1 -114.47 34.40 19.0 7650.0 1901.0 1129.0 463.0 1.8200 80100.0 2 -114.56 33.69 17.0 720.0 174.0 333.0 117.0 1.6509 85700.0 3 -114.57 33.64 14.0 1501.0 337.0 515.0 226.0 3.1917 73400.0 4 -114.57 33.57 20.0 1454.0 326.0 624.0 262.0 1.9250 65500.0 In\u00a0[\u00a0]: Copied! <pre>test_data = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n\ntest_data.head()\n</pre> test_data = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")  test_data.head() Out[\u00a0]: longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value 0 -122.05 37.37 27.0 3885.0 661.0 1537.0 606.0 6.6085 344700.0 1 -118.30 34.26 43.0 1510.0 310.0 809.0 277.0 3.5990 176500.0 2 -117.81 33.78 27.0 3589.0 507.0 1484.0 495.0 5.7934 270500.0 3 -118.36 33.82 28.0 67.0 15.0 49.0 11.0 6.1359 330000.0 4 -119.67 36.33 19.0 1241.0 244.0 850.0 237.0 2.9375 81700.0 In\u00a0[\u00a0]: Copied! <pre>train_data.columns\n</pre> train_data.columns Out[\u00a0]: <pre>Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income',\n       'median_house_value'],\n      dtype='object')</pre> In\u00a0[\u00a0]: Copied! <pre>len(train_data)\n</pre> len(train_data) Out[\u00a0]: <pre>17000</pre> In\u00a0[\u00a0]: Copied! <pre>X_train_reg = train_data.drop(\"median_house_value\", axis = 1).to_numpy()\nY_train_reg = train_data[\"median_house_value\"].to_numpy()\n\nX_test_reg = test_data.drop(\"median_house_value\", axis = 1).to_numpy()\nY_test_reg = test_data[\"median_house_value\"].to_numpy()\n</pre> X_train_reg = train_data.drop(\"median_house_value\", axis = 1).to_numpy() Y_train_reg = train_data[\"median_house_value\"].to_numpy()  X_test_reg = test_data.drop(\"median_house_value\", axis = 1).to_numpy() Y_test_reg = test_data[\"median_house_value\"].to_numpy() In\u00a0[\u00a0]: Copied! <pre>reg_tree = DecisionTreeRegressor(max_depth = 10, random_state=6)\n\nreg_tree.fit(X_train_reg, Y_train_reg)\n\nprint(\"Training Loss :\", metrics.mean_absolute_error(reg_tree.predict(X_train_reg), Y_train_reg))\n\nprint(\"Testing Loss :\", metrics.mean_absolute_error(reg_tree.predict(X_test_reg), Y_test_reg))\n</pre> reg_tree = DecisionTreeRegressor(max_depth = 10, random_state=6)  reg_tree.fit(X_train_reg, Y_train_reg)  print(\"Training Loss :\", metrics.mean_absolute_error(reg_tree.predict(X_train_reg), Y_train_reg))  print(\"Testing Loss :\", metrics.mean_absolute_error(reg_tree.predict(X_test_reg), Y_test_reg)) <pre>Training Loss : 30800.111026813956\nTesting Loss : 40980.01788411865\n</pre> In\u00a0[\u00a0]: Copied! <pre># Visualisation of the best tree\n\nreg_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income']\n\nfigure(figsize=(20, 15), dpi=400)\n\ntree.plot_tree(reg_tree, filled = True, feature_names = reg_columns,\n               max_depth = 3, fontsize=10) # limit the tree to only show the first 3 nodes\n\n\nplt.show()\n</pre> # Visualisation of the best tree  reg_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',        'total_bedrooms', 'population', 'households', 'median_income']  figure(figsize=(20, 15), dpi=400)  tree.plot_tree(reg_tree, filled = True, feature_names = reg_columns,                max_depth = 3, fontsize=10) # limit the tree to only show the first 3 nodes   plt.show() In\u00a0[\u00a0]: Copied! <pre>imp = reg_tree.feature_importances_\n\nfor i in range(len(reg_columns)):\n  print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\")\n\nfigure(figsize=(12, 6), dpi=200)\n\nplt.bar(reg_columns, reg_tree.feature_importances_)\nplt.title(\"Regression Feature Importantance for Regression Tree\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Features\")\nplt.show()\n</pre> imp = reg_tree.feature_importances_  for i in range(len(reg_columns)):   print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\")  figure(figsize=(12, 6), dpi=200)  plt.bar(reg_columns, reg_tree.feature_importances_) plt.title(\"Regression Feature Importantance for Regression Tree\") plt.xlabel(\"Columns\") plt.ylabel(\"Features\") plt.show() <pre>The feature longitude contributed 15.563208438870904% to the final outcome.\nThe feature latitude contributed 16.638618610215868% to the final outcome.\nThe feature housing_median_age contributed 5.47142054781178% to the final outcome.\nThe feature total_rooms contributed 0.756339018155957% to the final outcome.\nThe feature total_bedrooms contributed 1.1302896974804952% to the final outcome.\nThe feature population contributed 1.3768003197204541% to the final outcome.\nThe feature households contributed 0.7456319540808211% to the final outcome.\nThe feature median_income contributed 58.31769141366372% to the final outcome.\n</pre> In\u00a0[\u00a0]: Copied! <pre>reg_rf = RandomForestRegressor(n_estimators = 50, max_depth=15, n_jobs = -1, random_state=3)\n\nreg_rf.fit(X_train_reg, Y_train_reg)\n\nprint(\"Training Loss :\", metrics.mean_absolute_error(reg_rf.predict(X_train_reg), Y_train_reg))\n\nprint(\"Testing Loss :\", metrics.mean_absolute_error(reg_rf.predict(X_test_reg), Y_test_reg))\n</pre> reg_rf = RandomForestRegressor(n_estimators = 50, max_depth=15, n_jobs = -1, random_state=3)  reg_rf.fit(X_train_reg, Y_train_reg)  print(\"Training Loss :\", metrics.mean_absolute_error(reg_rf.predict(X_train_reg), Y_train_reg))  print(\"Testing Loss :\", metrics.mean_absolute_error(reg_rf.predict(X_test_reg), Y_test_reg)) <pre>Training Loss : 16885.276048778334\nTesting Loss : 32885.322945253574\n</pre> In\u00a0[\u00a0]: Copied! <pre>imp = reg_rf.feature_importances_\n\nfor i in range(len(reg_columns)):\n  print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\")\n\nfigure(figsize=(12, 6), dpi=200)\n\nplt.bar(reg_columns, reg_rf.feature_importances_)\nplt.title(\"Regression Feature Importantance for Random Forest\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Features\")\nplt.show()\n</pre> imp = reg_rf.feature_importances_  for i in range(len(reg_columns)):   print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\")  figure(figsize=(12, 6), dpi=200)  plt.bar(reg_columns, reg_rf.feature_importances_) plt.title(\"Regression Feature Importantance for Random Forest\") plt.xlabel(\"Columns\") plt.ylabel(\"Features\") plt.show() <pre>The feature longitude contributed 16.289192107492408% to the final outcome.\nThe feature latitude contributed 15.194999333209443% to the final outcome.\nThe feature housing_median_age contributed 5.924326019352224% to the final outcome.\nThe feature total_rooms contributed 2.1172321958206175% to the final outcome.\nThe feature total_bedrooms contributed 2.4519246097281338% to the final outcome.\nThe feature population contributed 3.2494812901481893% to the final outcome.\nThe feature households contributed 1.8252991866551616% to the final outcome.\nThe feature median_income contributed 52.94754525759383% to the final outcome.\n</pre> In\u00a0[\u00a0]: Copied! <pre>reg_xgb = XGBRegressor(n_estimators = 10, max_depth=12, n_jobs = -1, seed=3)\n\nreg_xgb.fit(X_train_reg, Y_train_reg)\n\nprint(\"Training Loss :\", metrics.mean_absolute_error(reg_xgb.predict(X_train_reg), Y_train_reg))\n\nprint(\"Testing Loss :\", metrics.mean_absolute_error(reg_xgb.predict(X_test_reg), Y_test_reg))\n</pre> reg_xgb = XGBRegressor(n_estimators = 10, max_depth=12, n_jobs = -1, seed=3)  reg_xgb.fit(X_train_reg, Y_train_reg)  print(\"Training Loss :\", metrics.mean_absolute_error(reg_xgb.predict(X_train_reg), Y_train_reg))  print(\"Testing Loss :\", metrics.mean_absolute_error(reg_xgb.predict(X_test_reg), Y_test_reg)) <pre>Training Loss : 15020.764377757352\nTesting Loss : 33602.127217447916\n</pre> In\u00a0[\u00a0]: Copied! <pre>imp = reg_xgb.feature_importances_\n\nfor i in range(len(reg_columns)):\n  print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\")\n\nfigure(figsize=(12, 6), dpi=200)\n\nplt.bar(reg_columns, reg_xgb.feature_importances_)\nplt.title(\"Regression Feature Importantance for XGBoost\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Features\")\nplt.show()\n</pre> imp = reg_xgb.feature_importances_  for i in range(len(reg_columns)):   print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\")  figure(figsize=(12, 6), dpi=200)  plt.bar(reg_columns, reg_xgb.feature_importances_) plt.title(\"Regression Feature Importantance for XGBoost\") plt.xlabel(\"Columns\") plt.ylabel(\"Features\") plt.show() <pre>The feature longitude contributed 8.249766379594803% to the final outcome.\nThe feature latitude contributed 10.84626242518425% to the final outcome.\nThe feature housing_median_age contributed 6.627519428730011% to the final outcome.\nThe feature total_rooms contributed 2.2169318050146103% to the final outcome.\nThe feature total_bedrooms contributed 4.318756982684135% to the final outcome.\nThe feature population contributed 3.7304263561964035% to the final outcome.\nThe feature households contributed 3.546779975295067% to the final outcome.\nThe feature median_income contributed 60.46355366706848% to the final outcome.\n</pre> In\u00a0[\u00a0]: Copied! <pre>tree_loss = metrics.mean_absolute_error(reg_tree.predict(X_test_reg), Y_test_reg)\nrf_loss = metrics.mean_absolute_error(reg_rf.predict(X_test_reg), Y_test_reg)\nxgb_loss = metrics.mean_absolute_error(reg_xgb.predict(X_test_reg), Y_test_reg)\n\n\nfigure(figsize=(6, 4), dpi=200)\n\nplt.bar([\"Decision Tree\", \"Random Forest\", \"XGBoost\"], [tree_loss, rf_loss, xgb_loss])\nplt.title(\"Model Loss on Regression Task (lower is better)\")\nplt.xlabel(\"Models\")\nplt.ylabel(\"Mean Absolute Error Loss\")\nplt.show()\n</pre> tree_loss = metrics.mean_absolute_error(reg_tree.predict(X_test_reg), Y_test_reg) rf_loss = metrics.mean_absolute_error(reg_rf.predict(X_test_reg), Y_test_reg) xgb_loss = metrics.mean_absolute_error(reg_xgb.predict(X_test_reg), Y_test_reg)   figure(figsize=(6, 4), dpi=200)  plt.bar([\"Decision Tree\", \"Random Forest\", \"XGBoost\"], [tree_loss, rf_loss, xgb_loss]) plt.title(\"Model Loss on Regression Task (lower is better)\") plt.xlabel(\"Models\") plt.ylabel(\"Mean Absolute Error Loss\") plt.show()"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#eda-with-tabular-datasets","title":"EDA with Tabular Datasets\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#bring-matplotlib-visualization-to-exploratory-analysis","title":"Bring Matplotlib Visualization to Exploratory Analysis\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#definition-of-the-business-problem","title":"Definition of the Business Problem\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#information-about-attributes","title":"Information about attributes:\u00b6","text":"<ol> <li>Number of times pregnant</li> <li>Plasma glucose concentration a 2 hours in an oral glucose tolerance test</li> <li>Diastolic blood pressure (mm Hg)</li> <li>Triceps skin fold thickness (mm)</li> <li>2-Hour serum insulin (mu U/ml)</li> <li>Body mass index (weight in kg/(height in m)^2)</li> <li>Diabetes pedigree function</li> <li>Age (years)</li> <li>Class variable (0 or 1)</li> </ol> <p>We need the predictor variables and the target variable.</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#extracting-and-loading-data","title":"Extracting and Loading Data\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#exploratory-data-analysis","title":"Exploratory Data Analysis\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#data-preprocessing","title":"Data Preprocessing\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#descriptive-statistics","title":"Descriptive Statistics\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#ml-essentials","title":"ML Essentials\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#viewing-with-seaborn","title":"Viewing with Seaborn\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#widely-used-for-exploratory-analysis","title":"Widely used for Exploratory Analysis\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#preparing-the-data-for-machine-learning","title":"Preparing the Data for Machine Learning\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#normalization","title":"Normalization\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#linear-regression","title":"Linear Regression\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#loading-the-data-set","title":"Loading the data set\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#visualizing-the-data","title":"Visualizing the Data\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#training-and-testing-data","title":"Training and Testing Data\u00b6","text":"<p>Now that we've explored the data a bit, let's go ahead and split the data into training and testing sets.</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#training-the-model","title":"Training the Model\u00b6","text":"<p>Now its time to train our model on our training data!</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#predicting-test-data","title":"Predicting Test Data\u00b6","text":"<p>Now that we have fit our model, let's evaluate its performance by predicting off the test values!</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#evaluating-the-model","title":"Evaluating the model:\u00b6","text":"<p>You can use metrics such as the mean squared error (MSE) or the coefficient of determination (R-squared) to evaluate the accuracy of the linear regression model.</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#logistic-regression","title":"Logistic Regression\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#complex-modeling-advanced-regression-and-classification","title":"Complex Modeling - Advanced Regression and Classification\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#1-decision-trees","title":"1. Decision Trees\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#11-what-is-a-decision-tree","title":"1.1 What is a decision tree?\u00b6","text":"<p>Decision trees are a type of supervised learning model that are known to perform very well on labeled tabular datasets. These models work by taking a series of decisions and reaching a conclusion.</p> <p>Let us look at an example</p> <p></p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#12-how-do-we-create-these-trees","title":"1.2 How do we create these trees?\u00b6","text":"<p>Let us look at an example dataset and see how we can construct the tree above.</p> Salary Is near home? Offers cab service? Company Type Do we accept? 55000 No No Corporation No 50000 Yes Yes Startup Yes 45000 Yes Yes Mid-teir No 49000 No Yes Startup No 69000 No No Conglomerate No 75000 Yes No Corporation Yes 50000 No Yes Mid-teir Yes 55000 Yes Yes Conglomerate Yes <p>Now we look at the steps to convert this into a tree. Let the \"Do we accept?\" column be the target column.</p> <ol> <li>Find the column that has the greatest impact on the outcome of the target (under the hood we use a formula such as information-gain which tells us how much one column is affected by another column).</li> <li>Split the dataset based on this column.</li> <li>Repeat steps 1 - 2 till we have reached a good solution or our tree has become big enough.</li> </ol> <p>On the above dataset the process will look like this :</p> <ol> <li>Identify Salary as the most influential column and find that the split is around 50,000$.</li> <li>Split the dataset based on this. This gives us the following splits:</li> </ol> <p>Above or equal to 50000$.</p> Salary Is near home? Offers cab service? Company Type Do we accept? 55000 No No Corporation No 69000 No No Conglomerate No 75000 Yes No Corporation Yes 50000 No Yes Mid-teir Yes 55000 Yes Yes Conglomerate Yes <p>Below 50,000$.</p> Salary Is near home? Offers cab service? Company Type Do we accept? 45000 Yes Yes Mid-teir No 49000 No Yes Startup No <ol> <li>Note that below 50000$ is all no acceptance and label that branch as 'No'.</li> <li>Now split the next tabel based on the next most influential column which is 'Is near home'. Then we get another split.</li> </ol> <p>Is near home = 'Yes'</p> Salary Is near home? Offers cab service? Company Type Do we accept? 55000 Yes Yes Conglomerate Yes 75000 Yes No Corporation Yes <p>Is near home = 'No'</p> Salary Is near home? Offers cab service? Company Type Do we accept? 55000 No No Corporation No 69000 No No Conglomerate No 50000 No Yes Mid-teir Yes <ol> <li><p>Note that Is near home = 'Yes' has 100% acceptance and label that branch as 'Yes'.</p> </li> <li><p>We do the final split on the column 'Offers cab service' and this gives us splits that have all yes or all no in both of them.</p> </li> </ol> <p>Offers cab service = 'Yes'</p> Salary Is near home? Offers cab service? Company Type Do we accept? 50000 No Yes Mid-teir Yes <p>Offers cab service = 'No'</p> Salary Is near home? Offers cab service? Company Type Do we accept? 55000 No No Corporation No 69000 No No Conglomerate No <ol> <li>This lets us arrive at a final answer.</li> </ol>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#13-visulization","title":"1.3 Visulization\u00b6","text":"<p>We can use the in-built plot_tree method to draw a graphical representation of the tree in sklearn.</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#14-classification-on-real-world-datasets","title":"1.4 Classification on Real World Datasets\u00b6","text":"<p>To show a classification example on real world data, we will use a medical dataset and try to predict the prognosis based on the symptoms of the patient.</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#more-complex-datasets","title":"More Complex Datasets\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#random-forest-models","title":"Random Forest Models\u00b6","text":"<p>In random forest models, we train multiple decision trees on different subsets of the data and then have them vote on the correct answer. If it is a regression problem, we simply average out the results.</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#random-forest-hyper-parameters","title":"Random Forest Hyper-parameters\u00b6","text":"<p>Below are the main hyper-parameters you will need to choose when building a random forest model.</p> <ol> <li>n_estimators -&gt; The number of trees that will be used.</li> <li>max_depth -&gt; The maximum depth of each tree.</li> </ol> <p>We also put n_jobs = -1, this just tells the computer that it can use as much of the CPU as it wants to train our model very fast. If we want to use less of our CPU capacity we can set this to the maximum number of cores we want it to use.</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#gradient-boosted-trees","title":"Gradient Boosted Trees\u00b6","text":"<p>Gradient boosted trees are models that utilise a chain of decision trees to make predictions. It generally follows these steps:</p> <ol> <li>Train an initial tree on the data.</li> <li>Gauge the overall error of this tree.</li> <li>Train a new tree that aims to rectify the mistakes of the prior tree instead of learning from scratch.</li> <li>Use the new tree to modify the answers of the previous tree</li> <li>Re-evaluate after the new tree corrects the answers.</li> <li>Repeat steps 3 - 5 for N number of trees.</li> <li>The final results is formed after (N - 1) trees correct each other\u2019s mistakes.</li> </ol> <p>For a fast and powerful implementation of such models we use the XGBoost library in these examples. Other popular trees include LGBM and ADA boost trees.</p>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#31-xgboost","title":"3.1 XGBoost\u00b6","text":"<p>Xgboost is a more complex and efficient form of the normal gradient boosting algorithm. It is also optimised to run efficiently on both CPU and GPU hardware. Below are the main hyper-parameters you will need to choose when building an xgboost model.</p> <ol> <li>n_estimators -&gt; The number of trees that will be chained together.</li> <li>max_depth -&gt; The maximum depth of each tree in the chain.</li> </ol>"},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#comparison-of-classifiers","title":"Comparison of Classifiers\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#regression-using-trees","title":"Regression Using Trees\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#decision-trees-for-regression","title":"Decision Trees For Regression\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#random-forest-regression","title":"Random Forest Regression\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#xgboost-regression","title":"XGBoost Regression\u00b6","text":""},{"location":"ML%20Essentials_%20Exploring%20Data%20Analysis%20Techniques%20with%20Tabular%20Datasets/#comparison-of-regressors","title":"Comparison of Regressors\u00b6","text":""},{"location":"kaggle/","title":"Kaggle Konquest","text":"<p>Hello from ACM BPDC</p> <p>Introducing Kaggle Konquest 2024 \ud83d\udcbb - the coolest Kaggle competition hitting on March 8th! \ud83c\udf89</p> <p>The week-long competition, is sponsored by Intel with cool cash prizes and a chance to prove your ML and AI skills!</p> <p>Dive deep into datasets, and showcase analytical prowess at every turn \ud83d\udcca</p> <p>\ud83d\uddd3\ufe0f Competition Duration: 8th March 2024,(Friday)- 14th March 2024(Thursday)</p> <p>\ud83e\udd11 Prizes : AED 250, 200, 100 </p> <p> Registration Link </p> <p>Only one motto to stand by! Enter, Excel, Earn - it's that simple! \ud83d\ude80</p>"}]}